\chapter{Grant Proposal}

{\Large\bf Interpretable Summaries of Emergent Languages}

\section{Introduction}

The field of emergent language combines the concept of self-play from reinforcement learning and the language-learning ability of neural networks to simulate the evolution of language from scratch.
Work like AlphaZero \citep{silver2017mastering}, MuZero \citep{schrittwiser2020mastering}, and \citet{Baker2020Emergent} has demonstrated that human-like strategies can emerge in games like go, chess, and hide-and-seek from learning and environment dynamics alone (i.e., self-play), without additional human input.
Large language models like GPT-3 \citep{brown2020language} demonstrate that many of the nuances and complexities of language can be captured by deep neural networks.
Thus, the central goal of emergent language research is is to simulate the evolution of human language at a high level of granularity by training deep neural networks with self-play.
Simulating the evolution of human-like language from scratch would enable experiments in linguistics and NLP applications that would otherwise be impossible.

While mathematical models of the evolution of language have been used for a long time in linguistics, they require introducing many simplifying assumptions and inductive biases to the simulation.
% With deep neural networks, the complexity which they are capable of accounting for can be arbitrarily scaled meaning that the level of granularity at which language evolution could be modeled could be decreased arbitrarily.
Deep neural networks, on the other hand, can be continually scaled up to account for more and more complexities in the data they model.
Thus, using them to model the evolution of language would allow for a commensurate scaling in complexity and granularity of the simulation.
Such simulations would provide unprecedentedly  detailed experimental methods for studying the origin and evolution of human language.
\bjb{It is important to mention here that one of the other ingredients in emergent language is that the neural networks learn strategies which are derived from ``first principles'', that is, the environment.}

% \ldots to the point \cmg{wording} that the only way to gather enough data is to blindly crawling \cmg{wording} the Web \cmg{look more into this} for high-resource languages \cmg{wording}.
State-of-the-art models in NLP are extremely data-hungry---GPT-3 was trained on ${\sim}500$ billion tokens \citep{brown2020language}.
Such a requirement greatly limits the potential sources of training data to unlabeled data from Web-crawls and personal conversations in the world's highest-resource languages.
Thus, tasks which require the following would fail to fully benefit from GPT-3-scale models:
    low- and medium-resource language data,
    labelled data,
    multi-modal and embodied language,
    or data free from offensive language.
% High-resource languages only represent a fraction of the linguistic diversity in the world meaning model development can overfit to a narrow range of typological features; furthermore, any biases or harmful content in the Web crawl will be imbibed and possibly amplified by the model \cmg{These seem like weak reasons. What is the killerest of apps?}.
Emergent language simulations can scale directly with compute resources and hence could meet the needs of GPT-3-scale models.
But more importantly, emergent languages are highly \emph{controllable} since they emerge from the fully-specified first principles \cmg{awk}; for example, the structure of the emergent language could be encouraged to be similar to that of low-resource languages, improving the outcomes of transfer learning.
Furthermore, since emergent languages always emerge from a fully observable and controllable \emph{environment}, it would be possible to generate labelled and embodied data from the simulations since the grounding of the semantics would as available \cmg{accessible?} as the language itself.


% Since emergent language research started in 2016, over $100$ papers have been published on the subject, but there has yet to be an emergent language surpassing a rudimentary level.
Despite the successes of self-play and large language models, combining these techniques in emergent language presents unique and difficult problems.
In cases like AlphaZero playing chess against itself, rediscovering advanced chess strategies known to humans is a sort of a ``happy accident'' relative to the overall goal of learning to play chess.
In emergent language, the primary goal is not to solve multi-agent reinforcement learning problems, but to discover these emergent strategies (in the form of language) themselves.
While the outcome of a game of chess is easy to measure, measuring the presence of an emergent phenomenon as complex and open-ended as language is far more difficult.
% This presents an added problem because language is a more open-ended phenomenon than chess strategies.
\cmg{Mention here how this fact leads to the multiplicity of design choices.}

In contrast to large language models,
    there is no ground truth data to train the agents' neural networks; the emergent language data is entirely bootstrapped.
While models like GPT-3 can certainly copy the structures of human language, it is an open question whether or not such neural networks would acquire these structures without human language input.
\bjb{I like these problems, but I do not immediately see how they motivate the problem I want to solve.}

Combining the focus on emergent behavior and unconstrained neural networks results in a multitude of inherently difficult questions and design choices.
Foremost of which is what qualifies as \emph{emergent} and what counts as \emph{language}.
% \bjb{Would it be better for me to just state the central problem of EL (simulating the evolution of human-like language from scratch), state that this is underspecified, and blame that for why there is so little organization in the field?}
Thus, although the field's goal of simulating the evolution of human-like language from scratch is conceptually clear, the details are underspecified.
The result is a field with no concrete vision on what its goals are and how to progress towards them.
This manifests in the literature which comprises papers which are often ``one-off'' and do not clearly progress towards an overarching goal of emergent language.

In order to advance emergent language and apply it to linguistics and NLP, it is necessary for researchers to have a concrete formulation of the goals of the field and, most especially, a way to measure progress towards these goals.
This requires a way to characterize emergent languages in terms of their most salient properties.
Salience, in this case, is determined by the goals of a given research program.
For example, linguistics, generally speaking, will be most interested in measuring the ways in which an emergent language is like a human language, for example: lexical distribution, syntactic structure, and cognitive constraints on the agents.
For NLP, on the other hand, it would be more important to measure how well the emergent language is working as a substitute for real data or how resource-efficient generating the emergent language is.

While all fields in machine learning need goals and methods of measuring progress, most fields develop them organically and implicitly.
The need for an explicit characterization, as we propose, arises from the inherent variety and open-endedness of emergent language.
Only with such an explicit characterization will it be possible for researchers to easily understand the gaps in existing approaches and the next steps necessary for advancing the techniques of emergent language.
Emergent languages are comparable \cmg{word not qualified} insofar as their most important characteristics are mapped into a unified ``space'' of categorical and numerical variables.
Currently, the only way to compare emergent languages is to read the individual papers, mapping them into an \emph{ad hoc} ontology---neither efficient nor effective.

To this end, we propose developing and implementing an automatic method for mapping emergent languages \bjb{and their environments?} into a unified semantic space.
This semantic space contains the key characteristics of emergent languages, in particular those which are relevant to the applications in linguistics and NLP.\sentspace{}
For linguistics, the characteristics focus on various identifiable structures of the language; for example, this would include lexical (e.g., entropy), syntactic (e.g., context sensitivity), and semantic (e.g., compositionality) properties.
For NLP, the characteristics are more data-driven and measure the degree to which emergent language data is an effective replacement for human language data in various NLP tasks.
The primary intuition here is that the more human-like an emergent language in its lexical distribution, syntactic structure, underlying semantics, etc., the more effective it will be as a replacement for human language in these tasks.

At a high level, the metric will be implemented as a command line tool or web interface that takes a corpus of an emergent language (possibly with other metadata), and produces a collection of quantitative and categorical metrics that give a summary or a ``snapshot'' of the language.
The tools will comprise aspects of both WALS \citep{wals} and GLUE \citep{wang2018glue}.
WALS (the World Atlas of Language Structures) \citep{wals} is, in essence, a table where each row is language and each column is a linguistic feature of the language (e.g., word order, number of consonants, presence of case markings).
\bjb{Does WALS introduce/enforce a standardized way of evaluating each of these characteristics?}
GLUE \citep{wang2018glue} is a benchmark and diagnostic tool for NLP models comprising a collection of natural language understanding tasks.
The diagnostic portion of GLUE contains specially labelled data which helps developers to determine the particular areas in which their model performs well or poorly.

Both WALS and GLUE are invaluable tools to researchers because they enable one to quickly ascertain the basic facts about a language or NLP model without needing to dive into individual papers.
This is especially helpful to practitioners outside of the particular field who are looking to integrate knowledge from linguistics/NLP into their own field.
Furthermore, separate entries can easily be compared since they are analyzed according to the same framework.
Finally, being able to collect a representative sample of comparable entries highlights the yet unexplored areas \cmg{awk} and hints towards the most fruitful directions for future research.

Thus, our tool seeks to adapt work like WALS and GLUE to the issues unique to emergent language.
Naturally, this entails formulating the constituent characteristics and tasks in light of the goals of emergent language.
While linguistics and NLP have robust pre-existing methods of evaluation, emergent language does not.
As a result, a significant portion of the proposed work will be developing these methods of evaluation and not simply selecting them.

\cmg{Briefly and forcefully illustrate how the summarizer will be used; restate the impact.}

% There are two main contributions described by this proposal.
% \begin{enumerate}
%     \item Knowledge
%     \item Software
% \end{enumerate}

\bjb{Use consistent terminology: ``emergent languages'', ``emergent language environments'', or something else.}


\section{Summarizing Emergent Language}

\section{Technical Element 1: Intrinsic Metrics}
\section{Technical Element 2: Extrinsic Metrics}
