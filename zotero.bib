
@article{lewis_deal_2017,
	title = {Deal or {No} {Deal}? {End}-to-{End} {Learning} for {Negotiation} {Dialogues}},
	shorttitle = {Deal or {No} {Deal}?},
	url = {http://arxiv.org/abs/1706.05125},
	abstract = {Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).},
	urldate = {2019-08-24},
	journal = {arXiv:1706.05125 [cs]},
	author = {Lewis, Mike and Yarats, Denis and Dauphin, Yann N. and Parikh, Devi and Batra, Dhruv},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.05125},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1706.05125 PDF:/home/brendon/academic/misc/Zotero/storage/F7235K59/Lewis et al. - 2017 - Deal or No Deal End-to-End Learning for Negotiati.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/5FCFZSPE/1706.html:text/html},
}

@inproceedings{kumagai_human-like_2016,
	address = {Edinburgh, UK},
	title = {Human-like {Natural} {Language} {Generation} {Using} {Monte} {Carlo} {Tree} {Search}},
	url = {https://www.aclweb.org/anthology/W16-5502},
	doi = {10.18653/v1/W16-5502},
	urldate = {2019-08-24},
	booktitle = {Proceedings of the {INLG} 2016 {Workshop} on {Computational} {Creativity} in {Natural} {Language} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Kumagai, Kaori and Kobayashi, Ichiro and Mochihashi, Daichi and Asoh, Hideki and Nakamura, Tomoaki and Nagai, Takayuki},
	month = sep,
	year = {2016},
	pages = {11--18},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/MPB7WU3W/Kumagai et al. - 2016 - Human-like Natural Language Generation Using Monte.pdf:application/pdf},
}

@article{omidshafiei_-_2019,
	title = {α- {Rank}: {Multi}-{Agent} {Evaluation} by {Evolution}},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	shorttitle = {α- {Rank}},
	url = {https://www.nature.com/articles/s41598-019-45619-9},
	doi = {10.1038/s41598-019-45619-9},
	abstract = {We introduce α-Rank, a principled evolutionary dynamics methodology, for the evaluation and ranking of agents in large-scale multi-agent interactions, grounded in a novel dynamical game-theoretic solution concept called Markov-Conley chains (MCCs). The approach leverages continuous-time and discrete-time evolutionary dynamical systems applied to empirical games, and scales tractably in the number of agents, in the type of interactions (beyond dyadic), and the type of empirical games (symmetric and asymmetric). Current models are fundamentally limited in one or more of these dimensions, and are not guaranteed to converge to the desired game-theoretic solution concept (typically the Nash equilibrium). α-Rank automatically provides a ranking over the set of agents under evaluation and provides insights into their strengths, weaknesses, and long-term dynamics in terms of basins of attraction and sink components. This is a direct consequence of the correspondence we establish to the dynamical MCC solution concept when the underlying evolutionary model’s ranking-intensity parameter, α, is chosen to be large, which exactly forms the basis of α-Rank. In contrast to the Nash equilibrium, which is a static solution concept based solely on fixed points, MCCs are a dynamical solution concept based on the Markov chain formalism, Conley’s Fundamental Theorem of Dynamical Systems, and the core ingredients of dynamical systems: fixed points, recurrent sets, periodic orbits, and limit cycles. Our α-Rank method runs in polynomial time with respect to the total number of pure strategy profiles, whereas computing a Nash equilibrium for a general-sum game is known to be intractable. We introduce mathematical proofs that not only provide an overarching and unifying perspective of existing continuous- and discrete-time evolutionary evaluation models, but also reveal the formal underpinnings of the α-Rank methodology. We illustrate the method in canonical games and empirically validate it in several domains, including AlphaGo, AlphaZero, MuJoCo Soccer, and Poker.},
	language = {en},
	number = {1},
	urldate = {2019-08-24},
	journal = {Scientific Reports},
	author = {Omidshafiei, Shayegan and Papadimitriou, Christos and Piliouras, Georgios and Tuyls, Karl and Rowland, Mark and Lespiau, Jean-Baptiste and Czarnecki, Wojciech M. and Lanctot, Marc and Perolat, Julien and Munos, Remi},
	month = jul,
	year = {2019},
	pages = {1--29},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/CCE6SNBB/Omidshafiei et al. - 2019 - α- Rank Multi-Agent Evaluation by Evolution.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/DUYXD4L2/s41598-019-45619-9.html:text/html},
}

@article{lipowska_emergence_2018,
	title = {Emergence of linguistic conventions in multi-agent reinforcement learning},
	volume = {13},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0208095},
	abstract = {Recently, emergence of signaling conventions, among which language is a prime example, draws a considerable interdisciplinary interest ranging from game theory, to robotics to evolutionary linguistics. Such a wide spectrum of research is based on much different assumptions and methodologies, but complexity of the problem precludes formulation of a unifying and commonly accepted explanation. We examine formation of signaling conventions in a framework of a multi-agent reinforcement learning model. When the network of interactions between agents is a complete graph or a sufficiently dense random graph, a global consensus is typically reached with the emerging language being a nearly unique object-word mapping or containing some synonyms and homonyms. On finite-dimensional lattices, the model gets trapped in disordered configurations with a local consensus only. Such a trapping can be avoided by introducing a population renewal, which in the presence of superlinear reinforcement restores an ordinary surface-tension driven coarsening and considerably enhances formation of efficient signaling.},
	language = {eng},
	number = {11},
	journal = {PloS One},
	author = {Lipowska, Dorota and Lipowski, Adam},
	year = {2018},
	pmid = {30496267},
	pmcid = {PMC6264146},
	keywords = {Humans, Biological Evolution, Game Theory, Language, Learning, Linguistics, Reinforcement (Psychology)},
	pages = {e0208095},
	file = {Full Text:/home/brendon/academic/misc/Zotero/storage/NG4ILZ8R/Lipowska and Lipowski - 2018 - Emergence of linguistic conventions in multi-agent.pdf:application/pdf},
}

@article{khomtchouk_modeling_2018,
	title = {Modeling natural language emergence with integral transform theory and reinforcement learning},
	url = {http://arxiv.org/abs/1812.01431},
	abstract = {Zipf's law predicts a power-law relationship between word rank and frequency in language communication systems and has been widely reported in a variety of natural language processing applications. However, the emergence of natural language is often modeled as a function of bias between speaker and listener interests, which lacks a direct way of relating information-theoretic bias to Zipfian rank. A function of bias also serves as an unintuitive interpretation of the communicative effort exchanged between a speaker and a listener. We counter these shortcomings by proposing a novel integral transform and kernel for mapping communicative bias functions to corresponding word frequency-rank representations at any arbitrary phase transition point, resulting in a direct way to link communicative effort (modeled by speaker/listener bias) to specific vocabulary used (represented by word rank). We demonstrate the practical utility of our integral transform by showing how a change from bias to rank results in greater accuracy and performance at an image classification task for assigning word labels to images randomly subsampled from CIFAR10. We model this task as a reinforcement learning game between a speaker and listener and compare the relative impact of bias and Zipfian word rank on communicative performance (and accuracy) between the two agents.},
	urldate = {2019-09-11},
	journal = {arXiv:1812.01431 [cs]},
	author = {Khomtchouk, Bohdan and Sudhakaran, Shyam},
	month = nov,
	year = {2018},
	note = {arXiv: 1812.01431},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1812.01431 PDF:/home/brendon/academic/misc/Zotero/storage/MSE64N5T/Khomtchouk and Sudhakaran - 2018 - Modeling natural language emergence with integral .pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/YKMIUPZL/1812.html:text/html},
}

@article{degiuli_random_2019,
	title = {Random {Language} {Model}},
	volume = {122},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/1809.01201},
	doi = {10.1103/PhysRevLett.122.128301},
	abstract = {Many complex generative systems use languages to create structured objects. We consider a model of random languages, defined by weighted context-free grammars. As the distribution of grammar weights broadens, a transition is found from a random phase, in which sentences are indistinguishable from noise, to an organized phase in which nontrivial information is carried. This marks the emergence of deep structure in the language, and can be understood by a competition between energy and entropy.},
	number = {12},
	urldate = {2019-09-11},
	journal = {Physical Review Letters},
	author = {DeGiuli, E.},
	month = mar,
	year = {2019},
	note = {arXiv: 1809.01201},
	keywords = {Computer Science - Computation and Language, Computer Science - Formal Languages and Automata Theory, Condensed Matter - Disordered Systems and Neural Networks},
	pages = {128301},
	file = {arXiv\:1809.01201 PDF:/home/brendon/academic/misc/Zotero/storage/KYW7SZ7V/DeGiuli - 2019 - Random Language Model.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/74X25VNX/1809.html:text/html},
}

@article{bogin_emergence_2018,
	title = {Emergence of {Communication} in an {Interactive} {World} with {Consistent} {Speakers}},
	url = {http://arxiv.org/abs/1809.00549},
	abstract = {Training agents to communicate with one another given task-based supervision only has attracted considerable attention recently, due to the growing interest in developing models for human-agent interaction. Prior work on the topic focused on simple environments, where training using policy gradient was feasible despite the non-stationarity of the agents during training. In this paper, we present a more challenging environment for testing the emergence of communication from raw pixels, where training using policy gradient fails. We propose a new model and training algorithm, that utilizes the structure of a learned representation space to produce more consistent speakers at the initial phases of training, which stabilizes learning. We empirically show that our algorithm substantially improves performance compared to policy gradient. We also propose a new alignment-based metric for measuring context-independence in emerged communication and find our method increases context-independence compared to policy gradient and other competitive baselines.},
	urldate = {2019-09-11},
	journal = {arXiv:1809.00549 [cs]},
	author = {Bogin, Ben and Geva, Mor and Berant, Jonathan},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.00549},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1809.00549 PDF:/home/brendon/academic/misc/Zotero/storage/4G8RX232/Bogin et al. - 2018 - Emergence of Communication in an Interactive World.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/PJ63VS9K/1809.html:text/html},
}

@article{lazaridou_emergence_2018,
	title = {Emergence of {Linguistic} {Communication} from {Referential} {Games} with {Symbolic} and {Pixel} {Input}},
	url = {http://arxiv.org/abs/1804.03984},
	abstract = {The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.},
	urldate = {2019-09-11},
	journal = {arXiv:1804.03984 [cs]},
	author = {Lazaridou, Angeliki and Hermann, Karl Moritz and Tuyls, Karl and Clark, Stephen},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.03984},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv\:1804.03984 PDF:/home/brendon/academic/misc/Zotero/storage/CFK5IIK9/Lazaridou et al. - 2018 - Emergence of Linguistic Communication from Referen.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/HCEH8GEB/1804.html:text/html},
}

@article{mordatch_emergence_2017,
	title = {Emergence of {Grounded} {Compositional} {Language} in {Multi}-{Agent} {Populations}},
	url = {http://arxiv.org/abs/1703.04908},
	abstract = {By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.},
	urldate = {2019-09-11},
	journal = {arXiv:1703.04908 [cs]},
	author = {Mordatch, Igor and Abbeel, Pieter},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.04908},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1703.04908 PDF:/home/brendon/academic/misc/Zotero/storage/L52LKRKR/Mordatch and Abbeel - 2017 - Emergence of Grounded Compositional Language in Mu.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/5ZZPBFZ5/1703.html:text/html},
}

@article{havrylov_emergence_2017,
	title = {Emergence of {Language} with {Multi}-agent {Games}: {Learning} to {Communicate} with {Sequences} of {Symbols}},
	shorttitle = {Emergence of {Language} with {Multi}-agent {Games}},
	url = {http://arxiv.org/abs/1705.11192},
	abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
	urldate = {2019-09-11},
	journal = {arXiv:1705.11192 [cs]},
	author = {Havrylov, Serhii and Titov, Ivan},
	month = may,
	year = {2017},
	note = {arXiv: 1705.11192},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv\:1705.11192 PDF:/home/brendon/academic/misc/Zotero/storage/I9P9NZ6G/Havrylov and Titov - 2017 - Emergence of Language with Multi-agent Games Lear.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/MFHCUFDV/1705.html:text/html},
}

@inproceedings{poveda_neural_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Neural {Network} {Models} for {Language} {Acquisition}: {A} {Brief} {Survey}},
	isbn = {978-3-540-45487-8},
	shorttitle = {Neural {Network} {Models} for {Language} {Acquisition}},
	abstract = {Since the outbreak of connectionist modelling in the mid eighties, several problems in natural language processing have been tackled by employing neural network-based techniques. Neural network’s “biological plausibility” offers a promising framework in which the computational treatment of language may be linked to other disciplines such as cognitive science and psychology. With this brief survey, we set out to explore the landscape of artificial neural models for the acquisition of language that have been proposed in the research literature.},
	language = {en},
	booktitle = {Intelligent {Data} {Engineering} and {Automated} {Learning} – {IDEAL} 2006},
	publisher = {Springer Berlin Heidelberg},
	author = {Poveda, Jordi and Vellido, Alfredo},
	editor = {Corchado, Emilio and Yin, Hujun and Botti, Vicente and Fyfe, Colin},
	year = {2006},
	keywords = {Connectionist Model, Language Acquisition, Lexical Category, Natural Language Processing, Neural Network Model},
	pages = {1346--1357},
	file = {Poveda and Vellido - 2006 - Neural Network Models for Language Acquisition A .pdf:/home/brendon/academic/misc/Zotero/storage/YMJWIVZZ/Poveda and Vellido - 2006 - Neural Network Models for Language Acquisition A .pdf:application/pdf},
}

@article{foerster_learning_2016,
	title = {Learning to {Communicate} with {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1605.06676},
	abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
	urldate = {2019-09-11},
	journal = {arXiv:1605.06676 [cs]},
	author = {Foerster, Jakob N. and Assael, Yannis M. and de Freitas, Nando and Whiteson, Shimon},
	month = may,
	year = {2016},
	note = {arXiv: 1605.06676},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv\:1605.06676 PDF:/home/brendon/academic/misc/Zotero/storage/5EMMQRA8/Foerster et al. - 2016 - Learning to Communicate with Deep Multi-Agent Rein.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/YKCY4RIF/1605.html:text/html},
}

@article{maddison_concrete_2016,
	title = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {Discrete} {Random} {Variables}},
	shorttitle = {The {Concrete} {Distribution}},
	url = {http://arxiv.org/abs/1611.00712},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	urldate = {2019-09-11},
	journal = {arXiv:1611.00712 [cs, stat]},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.00712},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1611.00712 PDF:/home/brendon/academic/misc/Zotero/storage/8FE8KQUA/Maddison et al. - 2016 - The Concrete Distribution A Continuous Relaxation.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/5CCMAQKA/1611.html:text/html},
}

@article{jang_categorical_2016,
	title = {Categorical {Reparameterization} with {Gumbel}-{Softmax}},
	url = {http://arxiv.org/abs/1611.01144},
	abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
	urldate = {2019-09-11},
	journal = {arXiv:1611.01144 [cs, stat]},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01144},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1611.01144 PDF:/home/brendon/academic/misc/Zotero/storage/IE2LA4Q5/Jang et al. - 2016 - Categorical Reparameterization with Gumbel-Softmax.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/NC3QF2BK/1611.html:text/html},
}

@article{hamrick_analogues_2019,
	series = {{SI}: 29: {Artificial} {Intelligence} (2019)},
	title = {Analogues of mental simulation and imagination in deep learning},
	volume = {29},
	issn = {2352-1546},
	url = {http://www.sciencedirect.com/science/article/pii/S2352154618301670},
	doi = {10.1016/j.cobeha.2018.12.011},
	abstract = {Mental simulation—the capacity to imagine what will or what could be—is a salient feature of human cognition, playing a key role in a wide range of cognitive abilities. In artificial intelligence, the last few years have seen the development of methods which are analogous to mental models and mental simulation. This paper outlines recent methods in deep learning for constructing such models from data and learning to use them via reinforcement learning, and compares such approaches to human mental simulation. Model-based methods in deep learning can serve as powerful tools for building and scaling cognitive models. However, a number of challenges remain in matching the capacity of human mental simulation for efficiency, compositionality, generalization, and creativity.},
	urldate = {2019-09-23},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Hamrick, Jessica B},
	month = oct,
	year = {2019},
	keywords = {unread, mental model},
	pages = {8--16},
	file = {ScienceDirect Full Text PDF:/home/brendon/academic/misc/Zotero/storage/5G63Q5FV/Hamrick - 2019 - Analogues of mental simulation and imagination in .pdf:application/pdf;ScienceDirect Snapshot:/home/brendon/academic/misc/Zotero/storage/R5CB84CC/S2352154618301670.html:text/html},
}

@article{verma_emergence_2019,
	title = {Emergence of {Writing} {Systems} {Through} {Multi}-{Agent} {Cooperation}},
	url = {https://arxiv.org/abs/1910.00741v1},
	abstract = {Learning to communicate is considered an essential task to develop a general
AI. While recent literature in language evolution has studied emergent language
through discrete or continuous message symbols, there has been little work in
the emergence of writing systems in artificial agents. In this paper, we
present a referential game setup with two agents, where the mode of
communication is a written language system that emerges during the play. We
show that the agents can learn to coordinate successfully using this mode of
communication. Further, we study how the game rules affect the writing system
taxonomy by proposing a consistency metric.},
	language = {en},
	urldate = {2020-01-05},
	author = {Verma, Shresth and Dhar, Joydip},
	month = oct,
	year = {2019},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/GE8JR7I2/Verma and Dhar - 2019 - Emergence of Writing Systems Through Multi-Agent C.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/KISA8M4V/1910.html:text/html},
}

@misc{steinert-threlkeld_towards_2019,
	type = {Preprint},
	title = {Towards the {Emergence} of {Non}-trivial {Compositionality}},
	url = {http://philsci-archive.pitt.edu/16750/},
	abstract = {All natural languages exhibit a distinction between content words (nouns, verbs,etc.) and function words (determiners, auxiliaries, tenses, etc.). Yet surprisingly little has been said about the emergence of this universal architectural feature of human language. This paper argues that the existence of this distinction requires the presence of non-trivial compositionality and identifies assumptions that have previously been made in the literature that provably guarantee only trivial composition. It then presents a signaling game with variable contexts and shows how the distinction can emerge via reinforcement learning.},
	language = {en},
	urldate = {2020-01-05},
	journal = {Philosophy of Science},
	author = {Steinert-Threlkeld, Shane},
	month = dec,
	year = {2019},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/EXBJWKFS/Steinert-Threlkeld - 2019 - Towards the Emergence of Non-trivial Compositional.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/A65TJPMX/16750.html:text/html},
}

@article{georgalos_testing_2019,
	title = {Testing for the emergence of spontaneous order},
	issn = {1573-6938},
	url = {https://doi.org/10.1007/s10683-019-09637-8},
	doi = {10.1007/s10683-019-09637-8},
	abstract = {We report on an experimental investigation of the emergence of Spontaneous Order, the idea that societies can co-ordinate, without government intervention, on a form of society that is good for its citizens, as described by Adam Smith. Our experimental design is based on a production game with a convex input provision possibility frontier, where subjects have to choose a point on this frontier. We start with a simple society consisting of just two people, two inputs, one final good and in which the production process exhibits returns to specialisation. We then study more complex societies by increasing the size of the society (groups of 6 and 9 subjects) and the number of inputs (6 and 9 inputs respectively), as well as the combinations of inputs that each subject can provide. This form of production can be characterised as a cooperative game, where the Nash equilibrium predicts that the optimal outcome is achieved when each member of this society specialises in the provision of a single input. Based on this framework, we investigate whether Spontaneous Order can emerge, without it being imposed by the government. We find strong evidence in favour of the emergence of Spontaneous Order, with communication being an important factor. Using text classification algorithms (Multinomial Naive Bayes) we quantitatively analyse the available chat data and we provide insight into the kind of communication that fosters specialisation in the absence of external involvement. We note that, while communication has been shown to foster coordination in other contexts (for example, in public goods games, market entry games and competitive coordination games) this contribution is in the context of a production game where specialisation is crucial.},
	language = {en},
	urldate = {2020-01-05},
	journal = {Experimental Economics},
	author = {Georgalos, Konstantinos and Hey, John},
	month = dec,
	year = {2019},
	keywords = {unread, Co-ordination, Communication, Production, Specialisation, Spontaneous order},
	file = {Springer Full Text PDF:/home/brendon/academic/misc/Zotero/storage/U4HMMDJA/Georgalos and Hey - 2019 - Testing for the emergence of spontaneous order.pdf:application/pdf},
}

@article{guo_emergence_2019,
	title = {Emergence of {Numeric} {Concepts} in {Multi}-{Agent} {Autonomous} {Communication}},
	url = {https://arxiv.org/abs/1911.01098v1},
	abstract = {With the rapid development of deep learning, most of current state-of-the-art
techniques in natural langauge processing are based on deep learning models
trained with argescaled static textual corpora. However, we human beings learn
and understand in a different way. Thus, grounded language learning argues that
models need to learn and understand language by the experience and perceptions
obtained by interacting with enviroments, like how humans do. With the help of
deep reinforcement learning techniques, there are already lots of works
focusing on facilitating the emergence of communication protocols that have
compositionalities like natural languages among computational agents
population. Unlike these works, we, on the other hand, focus on the numeric
concepts which correspond to abstractions in cognition and function words in
natural language. Based on a specifically designed language game, we verify
that computational agents are capable of transmitting numeric concepts during
autonomous communication, and the emergent communication protocols can reflect
the underlying structure of meaning space. Although their encodeing method is
not compositional like natural languages from a perspective of human beings,
the emergent languages can be generalised to unseen inputs and, more
importantly, are easier for models to learn. Besides, iterated learning can
help further improving the compositionality of the emergent languages, under
the measurement of topological similarity. Furthermore, we experiment another
representation method, i.e. directly encode numerals into concatenations of
one-hot vectors, and find that the emergent languages would become
compositional like human natural languages. Thus, we argue that there are 2
important factors for the emergence of compositional languages.},
	language = {en},
	urldate = {2020-01-05},
	author = {Guo, Shangmin},
	month = nov,
	year = {2019},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/X8JI689B/Guo - 2019 - Emergence of Numeric Concepts in Multi-Agent Auton.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/4IMGZXD4/1911.html:text/html},
}

@article{ferdinand_cognitive_2019,
	title = {The cognitive roots of regularization in language},
	volume = {184},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027718303135},
	doi = {10.1016/j.cognition.2018.12.002},
	abstract = {Regularization occurs when the output a learner produces is less variable than the linguistic data they observed. In an artificial language learning experiment, we show that there exist at least two independent sources of regularization bias in cognition: a domain-general source based on cognitive load and a domain-specific source triggered by linguistic stimuli. Both of these factors modulate how frequency information is encoded and produced, but only the production-side modulations result in regularization (i.e. cause learners to eliminate variation from the observed input). We formalize the definition of regularization as the reduction of entropy and find that entropy measures are better at identifying regularization behavior than frequency-based analyses. Using our experimental data and a model of cultural transmission, we generate predictions for the amount of regularity that would develop in each experimental condition if the artificial language were transmitted over several generations of learners. Here we find that the effect of cognitive constraints can become more complex when put into the context of cultural evolution: although learning biases certainly carry information about the course of language evolution, we should not expect a one-to-one correspondence between the micro-level processes that regularize linguistic datasets and the macro-level evolution of linguistic regularity.},
	language = {en},
	urldate = {2020-01-05},
	journal = {Cognition},
	author = {Ferdinand, Vanessa and Kirby, Simon and Smith, Kenny},
	month = mar,
	year = {2019},
	keywords = {Domain generality, Domain specificity, Frequency learning, Language evolution, Regularisation},
	pages = {53--68},
	file = {ScienceDirect Full Text PDF:/home/brendon/academic/misc/Zotero/storage/S49MVSK8/Ferdinand et al. - 2019 - The cognitive roots of regularization in language.pdf:application/pdf;ScienceDirect Snapshot:/home/brendon/academic/misc/Zotero/storage/HPE6K5T2/S0010027718303135.html:text/html},
}

@inproceedings{tomlin_emergent_2019,
	title = {Emergent {Compositionality} in {Signaling} {Games}},
	abstract = {Recent work in natural language understanding (Mordatch and Abbeel, 2017; Hermann et al., 2017) has used deep learning to generate artificial languages in simulated environments, termed emergent communication. We leverage these computational environments as a testing ground for theories concerning the emergence of linguistic compositionality and compare our results with human behavior on related iterated signaling games. In particular, we provide experimental evidence suggesting that incremental pragmatic reasoning may lead to compositional referring behavior in both computational agents and in humans.},
	booktitle = {{CogSci}},
	author = {Tomlin, Nicholas and Pavlick, Ellie},
	year = {2019},
	keywords = {Deep learning, Computation, Emergent, Iteration, Natural language understanding},
}

@inproceedings{bouchacourt_miss_2019,
	title = {Miss {Tools} and {Mr} {Fruit}: {Emergent} communication in agents learning about object affordances},
	shorttitle = {Miss {Tools} and {Mr} {Fruit}},
	doi = {10.18653/v1/p19-1380},
	abstract = {Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge.},
	booktitle = {{ACL}},
	author = {Bouchacourt, Diane and Baroni, Marco},
	year = {2019},
	keywords = {Emergent, Bilateral filter},
	file = {Full Text:/home/brendon/academic/misc/Zotero/storage/8KTUZS57/Bouchacourt and Baroni - 2019 - Miss Tools and Mr Fruit Emergent communication in.pdf:application/pdf},
}

@inproceedings{todd_communication_2019,
	title = {Communication and {Language} {Emergence} {Among} {Populations} and {Clusters} of {Agents}},
	abstract = {Algorithms and other intelligent systems are increasingly able to harness the 1 power of language across a large number of domains. Alongside these rapid 2 developments has come an increased interest in the mechanisms that underly the 3 acquisition and emergence of language. Through the use of communication games, 4 researchers have been able to investigate how pairs of agents can learn to converge 5 to a pattern of linguistic communication. We combine the methods of multiple 6 researchers to confirm earlier findings. Further, we extend a classic communication 7 game to a population of more plausible communicator agents and investigate 8 the effects population size has on the ability for agents to learn to communicate. 9 Finally, we explore the effect of partitioning the population of agents into clusters 10 and examine whether and how the performance of the population changes under 11 different clustering schemes. 12},
	author = {Todd, Graham},
	year = {2019},
}

@inproceedings{sirota_evolving_2019,
	title = {Evolving recurrent neural networks for emergent communication},
	doi = {10.1145/3319619.3321957},
	abstract = {Recent research showed that deep neural networks can be trained to create shared languages to communicate and cooperate with each other. These approaches used fixed, handcrafted network architectures which were trained with reinforcement learning. We extend this approach by using neuroevolution to automate network design and find network weights of communicating agents. We show that neuroevolution is a viable approach for training agents to develop novel languages so as to communicate amongst themselves.},
	booktitle = {{GECCO} '19},
	author = {Sirota, Joshua and Bulitko, Vadim and Brown, Matthew R. G. and Hernandez, Sergio Poo},
	year = {2019},
	keywords = {Artificial neural network, Deep learning, Emergence, Reinforcement learning, Network planning and design, Neuroevolution, Recurrent neural network},
}

@article{oroojlooyjadid_review_2019,
	title = {A {Review} of {Cooperative} {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1908.03963v2},
	abstract = {Deep Reinforcement Learning has made significant progress in multi-agent
systems in recent years. In this review article, we have mostly focused on
recent papers on Multi-Agent Reinforcement Learning (MARL) than the older
papers, unless it was necessary. Several ideas and papers are proposed with
different notations, and we tried our best to unify them with a single notation
and categorize them by their relevance. In particular, we have focused on five
common approaches on modeling and solving multi-agent reinforcement learning
problems: (I) independent-learners, (II) fully observable critic, (III) value
function decomposition, (IV) consensus, (IV) learn to communicate. Moreover, we
discuss some new emerging research areas in MARL along with the relevant recent
papers. In addition, some of the recent applications of MARL in real world are
discussed. Finally, a list of available environments for MARL research are
provided and the paper is concluded with proposals on the possible research
directions.},
	language = {en},
	urldate = {2020-01-05},
	author = {OroojlooyJadid, Afshin and Hajinezhad, Davood},
	month = aug,
	year = {2019},
	keywords = {unread},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/UXVY8VLC/OroojlooyJadid and Hajinezhad - 2019 - A Review of Cooperative Multi-Agent Deep Reinforce.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/JBLRBDCR/1908.html:text/html},
}

@article{zaiem_learning_2019,
	title = {Learning to {Communicate} in {Multi}-{Agent} {Reinforcement} {Learning} : {A} {Review}},
	shorttitle = {Learning to {Communicate} in {Multi}-{Agent} {Reinforcement} {Learning}},
	url = {https://arxiv.org/abs/1911.05438v1},
	abstract = {We consider the issue of multiple agents learning to communicate through
reinforcement learning within partially observable environments, with a focus
on information asymmetry in the second part of our work. We provide a review of
the recent algorithms developed to improve the agents' policy by allowing the
sharing of information between agents and the learning of communication
strategies, with a focus on Deep Recurrent Q-Network-based models. We also
describe recent efforts to interpret the languages generated by these agents
and study their properties in an attempt to generate human-language-like
sentences. We discuss the metrics used to evaluate the generated communication
strategies and propose a novel entropy-based evaluation metric. Finally, we
address the issue of the cost of communication and introduce the idea of an
experimental setup to expose this cost in cooperative-competitive game.},
	language = {en},
	urldate = {2020-01-05},
	author = {Zaïem, Mohamed Salah and Bennequin, Etienne},
	month = nov,
	year = {2019},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/D8MRJVWT/Zaïem and Bennequin - 2019 - Learning to Communicate in Multi-Agent Reinforceme.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/LFYXYU6P/1911.html:text/html},
}

@article{choi_compositional_2018,
	title = {Compositional {Obverter} {Communication} {Learning} {From} {Raw} {Visual} {Input}},
	url = {http://arxiv.org/abs/1804.02341},
	abstract = {One of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (e.g. hand- engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols. The agents play an image description game where the image contains factors such as colors and shapes. We train the agents using the obverter technique where an agent introspects to generate messages that maximize its own understanding. Through qualitative analysis, visualization and a zero-shot test, we show that the agents can develop, out of raw image pixels, a language with compositional properties, given a proper pressure from the environment.},
	urldate = {2020-01-05},
	journal = {arXiv:1804.02341 [cs]},
	author = {Choi, Edward and Lazaridou, Angeliki and de Freitas, Nando},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.02341},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/ID8CMGDI/Choi et al. - 2018 - Compositional Obverter Communication Learning From.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/WIDLDW4R/1804.html:text/html},
}

@article{cogswell_emergence_2019,
	title = {Emergence of {Compositional} {Language} with {Deep} {Generational} {Transmission}},
	url = {http://arxiv.org/abs/1904.09067},
	abstract = {Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.},
	urldate = {2020-01-05},
	journal = {arXiv:1904.09067 [cs, stat]},
	author = {Cogswell, Michael and Lu, Jiasen and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.09067},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/ZG845PDL/Cogswell et al. - 2019 - Emergence of Compositional Language with Deep Gene.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/LD24MSCV/1904.html:text/html},
}

@inproceedings{liu_multi-task_2018,
	title = {Multi-{Task} {Adversarial} {Network} for {Disentangled} {Feature} {Learning}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Liu_Multi-Task_Adversarial_Network_CVPR_2018_paper.html},
	urldate = {2020-01-14},
	author = {Liu, Yang and Wang, Zhaowen and Jin, Hailin and Wassell, Ian},
	year = {2018},
	keywords = {unread},
	pages = {3743--3751},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/8C9ETPXJ/Liu et al. - 2018 - Multi-Task Adversarial Network for Disentangled Fe.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/LNSXGNM8/Liu_Multi-Task_Adversarial_Network_CVPR_2018_paper.html:text/html},
}

@article{kaiser_discrete_2018,
	title = {Discrete {Autoencoders} for {Sequence} {Models}},
	url = {http://arxiv.org/abs/1801.09797},
	abstract = {Recurrent models for sequences have been recently successful at many tasks, especially for language modeling and machine translation. Nevertheless, it remains challenging to extract good representations from these models. For instance, even though language has a clear hierarchical structure going from characters through words to sentences, it is not apparent in current language models. We propose to improve the representation in sequence models by augmenting current approaches with an autoencoder that is forced to compress the sequence through an intermediate discrete latent space. In order to propagate gradients though this discrete representation we introduce an improved semantic hashing technique. We show that this technique performs well on a newly proposed quantitative efficiency measure. We also analyze latent codes produced by the model showing how they correspond to words and phrases. Finally, we present an application of the autoencoder-augmented model to generating diverse translations.},
	urldate = {2020-01-21},
	journal = {arXiv:1801.09797 [cs, stat]},
	author = {Kaiser, Łukasz and Bengio, Samy},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.09797},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, unread},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/R4HM4INB/Kaiser and Bengio - 2018 - Discrete Autoencoders for Sequence Models.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/3LT5W7DS/1801.html:text/html},
}

@article{baroni_linguistic_2020,
	title = {Linguistic generalization and compositionality in modern artificial neural networks},
	volume = {375},
	issn = {0962-8436, 1471-2970},
	url = {http://arxiv.org/abs/1904.00157},
	doi = {10.1098/rstb.2019.0307},
	abstract = {In the last decade, deep artificial neural networks have achieved astounding performance in many natural language processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: Are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar-dependent generalizations, but also that they do not rely on systematic compositional rules. I argue that the intriguing behaviour of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language.},
	number = {1791},
	urldate = {2020-01-21},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Baroni, Marco},
	month = feb,
	year = {2020},
	note = {arXiv: 1904.00157},
	keywords = {Computer Science - Computation and Language},
	pages = {20190307},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/QYUJLX7T/Baroni - 2020 - Linguistic generalization and compositionality in .pdf:application/pdf;arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/HGEKVKEN/Baroni - 2020 - Linguistic generalization and compositionality in .pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/NLY45MCM/1904.html:text/html;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/IBPDMC6B/1904.html:text/html},
}

@article{griffiths_language_2007,
	title = {Language {Evolution} by {Iterated} {Learning} {With} {Bayesian} {Agents}},
	volume = {31},
	copyright = {2007 Cognitive Science Society, Inc.},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1080/15326900701326576},
	doi = {10.1080/15326900701326576},
	abstract = {Languages are transmitted from person to person and generation to generation via a process of iterated learning: people learn a language from other people who once learned that language themselves. We analyze the consequences of iterated learning for learning algorithms based on the principles of Bayesian inference, assuming that learners compute a posterior distribution over languages by combining a prior (representing their inductive biases) with the evidence provided by linguistic data. We show that when learners sample languages from this posterior distribution, iterated learning converges to a distribution over languages that is determined entirely by the prior. Under these conditions, iterated learning is a form of Gibbs sampling, a widely-used Markov chain Monte Carlo algorithm. The consequences of iterated learning are more complicated when learners choose the language with maximum posterior probability, being affected by both the prior of the learners and the amount of information transmitted between generations. We show that in this case, iterated learning corresponds to another statistical inference algorithm, a variant of the expectation-maximization (EM) algorithm. These results clarify the role of iterated learning in explanations of linguistic universals and provide a formal connection between constraints on language acquisition and the languages that come to be spoken, suggesting that information transmitted via iterated learning will ultimately come to mirror the minds of the learners.},
	language = {en},
	number = {3},
	urldate = {2020-04-21},
	journal = {Cognitive Science},
	author = {Griffiths, Thomas L. and Kalish, Michael L.},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1080/15326900701326576},
	keywords = {unread, Language evolution, Bayesian models, Cultural transmission, Iterated learning},
	pages = {441--480},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/GUQJ4SGV/Griffiths and Kalish - 2007 - Language Evolution by Iterated Learning With Bayes.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/T42K5M9V/15326900701326576.html:text/html},
}

@article{smith_iterated_2003,
	title = {Iterated {Learning}: {A} {Framework} for the {Emergence} of {Language}},
	volume = {9},
	issn = {1064-5462},
	shorttitle = {Iterated {Learning}},
	url = {https://doi.org/10.1162/106454603322694825},
	doi = {10.1162/106454603322694825},
	abstract = {Language is culturally transmitted. Iterated learning, the process by which the output of one individual's learning becomes the input to other individuals' learning, provides a framework for investigating the cultural evolution of linguistic structure. We present two models, based upon the iterated learning framework, which show that the poverty of the stimulus available to language learners leads to the emergence of linguistic structure. Compositionality is language's adaptation to stimulus poverty.},
	number = {4},
	urldate = {2020-04-21},
	journal = {Artificial Life},
	author = {Smith, Kenny and Kirby, Simon and Brighton, Henry},
	month = oct,
	year = {2003},
	note = {Publisher: MIT Press},
	keywords = {unread},
	pages = {371--386},
	file = {Snapshot:/home/brendon/academic/misc/Zotero/storage/5NUX4AJG/106454603322694825.html:text/html},
}

@misc{noauthor_3_nodate,
	title = {(3) ({PDF}) {Animacy} {Distinctions} {Arise} from {Iterated} {Learning}},
	url = {https://www.researchgate.net/publication/329653718_Animacy_Distinctions_Arise_from_Iterated_Learning},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2020-04-21},
	journal = {ResearchGate},
	note = {Library Catalog: www.researchgate.net},
	keywords = {unread},
	file = {Snapshot:/home/brendon/academic/misc/Zotero/storage/HUZ8KP4U/329653718_Animacy_Distinctions_Arise_from_Iterated_Learning.html:text/html},
}

@article{chaabouni_compositionality_2020,
	title = {Compositionality and {Generalization} in {Emergent} {Languages}},
	url = {http://arxiv.org/abs/2004.09124},
	abstract = {Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as {\textbackslash}emph\{compositionality\}. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results. First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.},
	urldate = {2020-05-02},
	journal = {arXiv:2004.09124 [cs]},
	author = {Chaabouni, Rahma and Kharitonov, Eugene and Bouchacourt, Diane and Dupoux, Emmanuel and Baroni, Marco},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.09124},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/7ENB4G63/Chaabouni et al. - 2020 - Compositionality and Generalization in Emergent La.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/MKPNAB54/2004.html:text/html},
}

@misc{seita_unsupervised_nodate,
	title = {Unsupervised {Meta}-{Learning}: {Learning} to {Learn} without {Supervision}},
	shorttitle = {Unsupervised {Meta}-{Learning}},
	url = {http://bair.berkeley.edu/blog/2020/05/01/umrl/},
	abstract = {The BAIR Blog},
	urldate = {2020-05-02},
	journal = {The Berkeley Artificial Intelligence Research Blog},
	author = {Seita, Daniel},
	note = {Library Catalog: bair.berkeley.edu},
	keywords = {unread},
	file = {Snapshot:/home/brendon/academic/misc/Zotero/storage/5F379USG/umrl.html:text/html},
}

@inproceedings{kirby_syntax_1998,
	title = {Syntax without {Natural} {Selection}: {How} compositionality emerges from vocabulary in a population of learners},
	shorttitle = {Syntax without {Natural} {Selection}},
	abstract = {this paper I put forward a new approach to understanding the origins of some of the key ingredients in a syntactic system. I show, using a computational model, that compositional syntax is an inevitable outcome of the dynamics of observationally learned communication systems. In a simulated population of individuals, language develops from a simple idiosyncratic vocabulary with little expressive power, to a compositional system with high expressivity, nouns and verbs, and word order expressing meaning distinctions.},
	booktitle = {In},
	publisher = {University Press},
	author = {Kirby, Simon},
	year = {1998},
	keywords = {unread},
	pages = {303--323},
	file = {Citeseer - Full Text PDF:/home/brendon/academic/misc/Zotero/storage/5U5HNMAM/Kirby - 1998 - Syntax without Natural Selection How compositiona.pdf:application/pdf;Citeseer - Snapshot:/home/brendon/academic/misc/Zotero/storage/A8Z9AU7Z/summary.html:text/html},
}

@article{li_ease--teaching_2019,
	title = {Ease-of-{Teaching} and {Language} {Structure} from {Emergent} {Communication}},
	url = {http://arxiv.org/abs/1906.02403},
	abstract = {Artificial agents have been shown to learn to communicate when needed to complete a cooperative task. Some level of language structure (e.g., compositionality) has been found in the learned communication protocols. This observed structure is often the result of specific environmental pressures during training. By introducing new agents periodically to replace old ones, sequentially and within a population, we explore such a new pressure -- ease of teaching -- and show its impact on the structure of the resulting language.},
	urldate = {2020-06-01},
	journal = {arXiv:1906.02403 [cs]},
	author = {Li, Fushan and Bowling, Michael},
	month = oct,
	year = {2019},
	note = {arXiv: 1906.02403},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multiagent Systems, unread},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/ZN8A8NFE/Li and Bowling - 2019 - Ease-of-Teaching and Language Structure from Emerg.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/I2KAZFPM/1906.html:text/html},
}

@article{resnick_capacity_2020,
	title = {Capacity, {Bandwidth}, and {Compositionality} in {Emergent} {Language} {Learning}},
	url = {http://arxiv.org/abs/1910.11424},
	abstract = {Many recent works have discussed the propensity, or lack thereof, for emergent languages to exhibit properties of natural languages. A favorite in the literature is learning compositionality. We note that most of those works have focused on communicative bandwidth as being of primary importance. While important, it is not the only contributing factor. In this paper, we investigate the learning biases that affect the efficacy and compositionality of emergent languages. Our foremost contribution is to explore how capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.},
	urldate = {2020-06-01},
	journal = {arXiv:1910.11424 [cs, stat]},
	author = {Resnick, Cinjon and Gupta, Abhinav and Foerster, Jakob and Dai, Andrew M. and Cho, Kyunghyun},
	month = apr,
	year = {2020},
	note = {arXiv: 1910.11424},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Multiagent Systems, unread},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/9E8XZANE/Resnick et al. - 2020 - Capacity, Bandwidth, and Compositionality in Emerg.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/FDXFB7EA/1910.html:text/html},
}

@article{chen_isolating_2019,
	title = {Isolating {Sources} of {Disentanglement} in {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/1802.04942},
	abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our \${\textbackslash}beta\$-TCVAE (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art \${\textbackslash}beta\$-VAE objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
	urldate = {2020-06-01},
	journal = {arXiv:1802.04942 [cs, stat]},
	author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
	month = apr,
	year = {2019},
	note = {arXiv: 1802.04942},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, unread},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/IU3MC7LF/Chen et al. - 2019 - Isolating Sources of Disentanglement in Variationa.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/S8L76SQC/1802.html:text/html},
}

@incollection{chaabouni_anti-efficient_2019,
	title = {Anti-efficient encoding in emergent communication},
	url = {http://papers.nips.cc/paper/8859-anti-efficient-encoding-in-emergent-communication.pdf},
	urldate = {2020-06-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	keywords = {unread},
	pages = {6293--6303},
	file = {NIPS Full Text PDF:/home/brendon/academic/misc/Zotero/storage/ZBER4D3V/Chaabouni et al. - 2019 - Anti-efficient encoding in emergent communication.pdf:application/pdf;NIPS Snapshot:/home/brendon/academic/misc/Zotero/storage/UKPNHUBZ/8859-anti-efficient-encoding-in-emergent-communication.html:text/html},
}

@inproceedings{tomlin_incremental_2018,
	title = {Incremental {Pragmatics} and {Emergent} {Communication}},
	abstract = {Recent work has demonstrated the ability of reinforcement learning agents to develop rich communication protocols in certain cooperative environments. Several key results have been achieved by constraining the task such that grounded communication is the only optimal strategy. We explore conditions under which groundedness may arise even when it is not strictly required by the task design. In particular, we suggest that incremental pragmatic reasoning could play a role in emergent communication, with evidence from the Task \& Talk reference game.},
	author = {Tomlin, Nicholas and Pavlick, Ellie},
	year = {2018},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/88G59HDQ/Tomlin and Pavlick - 2018 - Incremental Pragmatics and Emergent Communication.pdf:application/pdf},
}

@article{eysenbach_diversity_2018,
	title = {Diversity is {All} {You} {Need}: {Learning} {Skills} without a {Reward} {Function}},
	shorttitle = {Diversity is {All} {You} {Need}},
	url = {http://arxiv.org/abs/1802.06070},
	abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose DIAYN ('Diversity is All You Need'), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. We show how pretrained skills can provide a good parameter initialization for downstream tasks, and can be composed hierarchically to solve complex, sparse reward tasks. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning.},
	urldate = {2020-07-20},
	journal = {arXiv:1802.06070 [cs]},
	author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.06070},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/8A53W3VQ/Eysenbach et al. - 2018 - Diversity is All You Need Learning Skills without.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/7CDL6S7Y/1802.html:text/html},
}

@article{kharitonov_entropy_2020,
	title = {Entropy {Minimization} {In} {Emergent} {Languages}},
	url = {http://arxiv.org/abs/1905.13687},
	abstract = {There is growing interest in studying the languages that emerge when neural agents are jointly trained to solve tasks requiring communication through a discrete channel. We investigate here the information-theoretic complexity of such languages, focusing on the basic two-agent, one-exchange setup. We find that, under common training procedures, the emergent languages are subject to an entropy minimization pressure that has also been detected in human language, whereby the mutual information between the communicating agent's inputs and the messages is minimized, within the range afforded by the need for successful communication. That is, emergent languages are (nearly) as simple as the task they are developed for allow them to be. This pressure is amplified as we increase communication channel discreteness. Further, we observe that stronger discrete-channel-driven entropy minimization leads to representations with increased robustness to overfitting and adversarial attacks. We conclude by discussing the implications of our findings for the study of natural and artificial communication systems.},
	urldate = {2020-08-25},
	journal = {arXiv:1905.13687 [cs]},
	author = {Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco},
	month = jun,
	year = {2020},
	note = {arXiv: 1905.13687},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/P9XXZHBC/Kharitonov et al. - 2020 - Entropy Minimization In Emergent Languages.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/MGDCFJI2/1905.html:text/html},
}

@article{lowe_interaction_2020,
	title = {On the interaction between supervision and self-play in emergent communication},
	url = {http://arxiv.org/abs/2002.01093},
	abstract = {A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.},
	urldate = {2020-08-27},
	journal = {arXiv:2002.01093 [cs, stat]},
	author = {Lowe, Ryan and Gupta, Abhinav and Foerster, Jakob and Kiela, Douwe and Pineau, Joelle},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.01093},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/L9RX423V/Lowe et al. - 2020 - On the interaction between supervision and self-pl.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/8MVQ6MEC/2002.html:text/html},
}

@article{ren_compositional_2020,
	title = {Compositional {Languages} {Emerge} in a {Neural} {Iterated} {Learning} {Model}},
	abstract = {The principle of compositionality, which enables natural language to represent complex concepts via a structured combination of simpler ones, allows us to convey an open-ended set of messages using a limited vocabulary. If compositionality is indeed a natural property of language, we may expect it to appear in communication protocols that are created by neural agents via grounded language learning. Inspired by the iterated learning framework, which simulates the process of language evolution, we propose an effective neural iterated learning algorithm that, when applied to interacting neural agents, facilitates the emergence of a more structured type of language. Indeed, these languages provide specific advantages to neural agents during training, which translates as a larger posterior probability, which is then incrementally amplified via the iterated learning procedure. Our experiments confirm our analysis, and also demonstrate that the emerged languages largely improve the generalization of the neural agent communication.},
	journal = {ICLR},
	author = {Ren, Yi and Guo, Shangmin and Labeau, Matthieu and Cohen, Shay B. and Kirby, S.},
	year = {2020},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/4ZFQYCIF/Ren et al. - 2020 - Compositional Languages Emerge in a Neural Iterate.pdf:application/pdf},
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} {Meaning}, {Form}, and {Understanding} in the {Age} of {Data}},
	shorttitle = {Climbing towards {NLU}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.463},
	doi = {10.18653/v1/2020.acl-main.463},
	abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we ﬁnd that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the ﬁeld towards better science around natural language understanding.},
	language = {en},
	urldate = {2020-08-28},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	year = {2020},
	pages = {5185--5198},
	file = {Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf:/home/brendon/academic/misc/Zotero/storage/EQBFIDT4/Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf:application/pdf},
}

@article{henderson_unstoppable_2020,
	title = {The {Unstoppable} {Rise} of {Computational} {Linguistics} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2005.06420},
	abstract = {In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures. We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model. This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.},
	language = {en},
	urldate = {2020-08-28},
	journal = {arXiv:2005.06420 [cs]},
	author = {Henderson, James},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.06420},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Henderson - 2020 - The Unstoppable Rise of Computational Linguistics .pdf:/home/brendon/academic/misc/Zotero/storage/3ZR4TW5H/Henderson - 2020 - The Unstoppable Rise of Computational Linguistics .pdf:application/pdf},
}

@article{eskenazi_report_2020,
	title = {Report from the {NSF} {Future} {Directions} {Workshop}, {Toward} {User}-{Oriented} {Agents}: {Research} {Directions} and {Challenges}},
	shorttitle = {Report from the {NSF} {Future} {Directions} {Workshop}, {Toward} {User}-{Oriented} {Agents}},
	url = {http://arxiv.org/abs/2006.06026},
	abstract = {This USER Workshop was convened with the goal of defining future research directions for the burgeoning intelligent agent research community and to communicate them to the National Science Foundation. It took place in Pittsburgh Pennsylvania on October 24 and 25, 2019 and was sponsored by National Science Foundation Grant Number IIS-1934222. Any opinions, findings and conclusions or future directions expressed in this document are those of the authors and do not necessarily reflect the views of the National Science Foundation. The 27 participants presented their individual research interests and their personal research goals. In the breakout sessions that followed, the participants defined the main research areas within the domain of intelligent agents and they discussed the major future directions that the research in each area of this domain should take},
	urldate = {2020-08-28},
	journal = {arXiv:2006.06026 [cs]},
	author = {Eskenazi, Maxine and Zhao, Tiancheng},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.06026},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/EPD3QNAR/Eskenazi and Zhao - 2020 - Report from the NSF Future Directions Workshop, To.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/ZZFQHUEG/2006.html:text/html},
}

@article{lazaridou_emergent_2020,
	title = {Emergent {Multi}-{Agent} {Communication} in the {Deep} {Learning} {Era}},
	url = {http://arxiv.org/abs/2006.02419},
	abstract = {The ability to cooperate through language is a defining feature of humans. As the perceptual, motory and planning capabilities of deep artificial networks increase, researchers are studying whether they also can develop a shared language to interact. From a scientific perspective, understanding the conditions under which language evolves in communities of deep agents and its emergent features can shed light on human language evolution. From an applied perspective, endowing deep networks with the ability to solve problems interactively by communicating with each other and with us should make them more flexible and useful in everyday life. This article surveys representative recent language emergence studies from both of these two angles.},
	urldate = {2020-09-04},
	journal = {arXiv:2006.02419 [cs]},
	author = {Lazaridou, Angeliki and Baroni, Marco},
	month = jul,
	year = {2020},
	note = {arXiv: 2006.02419},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, important},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/K9SCPV2N/Lazaridou and Baroni - 2020 - Emergent Multi-Agent Communication in the Deep Lea.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/PCJJFP2W/2006.html:text/html},
}

@article{baroni_rat_2020,
	title = {Rat big, cat eaten! {Ideas} for a useful deep-agent protolanguage},
	abstract = {Deep-agent communities developing their own language-like communication protocol are a hot (or at least warm) topic in AI. Such agents could be very useful in machine-machine and human-machine interaction scenarios long before they have evolved a protocol as complex as human language. Here, I propose a small set of priorities we should focus on, if we want to get as fast as possible to a stage where deep agents speak a useful protolanguage.},
	journal = {ArXiv},
	author = {Baroni, Marco},
	year = {2020},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/2APPZG85/Baroni - 2020 - Rat big, cat eaten! Ideas for a useful deep-agent .pdf:application/pdf},
}

@inproceedings{jaques_social_2019,
	title = {Social {Influence} as {Intrinsic} {Motivation} for {Multi}-{Agent} {Deep} {Reinforcement} {Learning}},
	url = {http://proceedings.mlr.press/v97/jaques19a.html},
	abstract = {We propose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents’ act...},
	language = {en},
	urldate = {2020-11-20},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jaques, Natasha and Lazaridou, Angeliki and Hughes, Edward and Gulcehre, Caglar and Ortega, Pedro and Strouse, Dj and Leibo, Joel Z. and Freitas, Nando De},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {3040--3049},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/Z59TUDEV/Jaques et al. - 2019 - Social Influence as Intrinsic Motivation for Multi.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/595V83RU/jaques19a.html:text/html},
}

@article{kang_incorporating_2020,
	title = {Incorporating {Pragmatic} {Reasoning} {Communication} into {Emergent} {Language}},
	url = {http://arxiv.org/abs/2006.04109},
	abstract = {Emergentism and pragmatics are two research fields that study the dynamics of linguistic communication along substantially different timescales and intelligence levels. From the perspective of multi-agent reinforcement learning, they correspond to stochastic games with reinforcement training and stage games with opponent awareness. Given that their combination has been explored in linguistics, we propose computational models that combine short-term mutual reasoning-based pragmatics with long-term language emergentism. We explore this for agent communication referential games as well as in Starcraft II, assessing the relative merits of different kinds of mutual reasoning pragmatics models both empirically and theoretically. Our results shed light on their importance for making inroads towards getting more natural, accurate, robust, fine-grained, and succinct utterances.},
	urldate = {2021-01-12},
	journal = {arXiv:2006.04109 [cs]},
	author = {Kang, Yipeng and Wang, Tonghan and de Melo, Gerard},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.04109},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/KH5525KI/Kang et al. - 2020 - Incorporating Pragmatic Reasoning Communication in.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/F8DHZMZ3/2006.html:text/html},
}

@article{kajic_learning_2020,
	title = {Learning to cooperate: {Emergent} communication in multi-agent navigation},
	shorttitle = {Learning to cooperate},
	url = {http://arxiv.org/abs/2004.01097},
	abstract = {Emergent communication in artificial agents has been studied to understand language evolution, as well as to develop artificial systems that learn to communicate with humans. We show that agents performing a cooperative navigation task in various gridworld environments learn an interpretable communication protocol that enables them to efficiently, and in many cases, optimally, solve the task. An analysis of the agents' policies reveals that emergent signals spatially cluster the state space, with signals referring to specific locations and spatial directions such as "left", "up", or "upper left room". Using populations of agents, we show that the emergent protocol has basic compositional structure, thus exhibiting a core property of natural language.},
	urldate = {2021-01-12},
	journal = {arXiv:2004.01097 [cs, stat]},
	author = {Kajić, Ivana and Aygün, Eser and Precup, Doina},
	month = jun,
	year = {2020},
	note = {arXiv: 2004.01097},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Multiagent Systems, ur},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/9GB6ZH73/Kajić et al. - 2020 - Learning to cooperate Emergent communication in m.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/FXDDUWAF/2004.html:text/html},
}

@article{korbak_measuring_2020,
	title = {Measuring non-trivial compositionality in emergent communication},
	url = {http://arxiv.org/abs/2010.15058},
	abstract = {Compositionality is an important explanatory target in emergent communication and language evolution. The vast majority of computational models of communication account for the emergence of only a very basic form of compositionality: trivial compositionality. A compositional protocol is trivially compositional if the meaning of a complex signal (e.g. blue circle) boils down to the intersection of meanings of its constituents (e.g. the intersection of the set of blue objects and the set of circles). A protocol is non-trivially compositional (NTC) if the meaning of a complex signal (e.g. biggest apple) is a more complex function of the meanings of their constituents. In this paper, we review several metrics of compositionality used in emergent communication and experimentally show that most of them fail to detect NTC - i.e. they treat non-trivial compositionality as a failure of compositionality. The one exception is tree reconstruction error, a metric motivated by formal accounts of compositionality. These results emphasise important limitations of emergent communication research that could hamper progress on modelling the emergence of NTC.},
	urldate = {2021-01-30},
	journal = {arXiv:2010.15058 [cs]},
	author = {Korbak, Tomasz and Zubek, Julian and Rączaszek-Leonardi, Joanna},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.15058},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, important},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/YLCTJYWL/Korbak et al. - 2020 - Measuring non-trivial compositionality in emergent.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/K2V27NW5/2010.html:text/html},
}

@techreport{hahn_modeling_2020,
	title = {Modeling word and morpheme order in natural language as an efficient tradeoff of memory and surprisal},
	url = {https://psyarxiv.com/nu4qz/},
	abstract = {Memory limitations are known to constrain language comprehension and production, and have been argued to account for crosslinguistic word order regularities. However, a systematic assessment of the role of memory limitations in language structure has proven elusive, in part because it is hard to extract precise large-scale quantitative generalizations about language from existing mechanistic models of memory use in sentence processing. We provide an architecture-independent information-theoretic formalization of memory limitations which enables a simple calculation of the memory efficiency of languages. Our notion of memory efficiency is based on the idea of a memory–surprisal tradeoff : a certain level of average surprisal per word can only be achieved at the cost of storing some amount of information about past context. Based on this notion of memory usage, we advance the Efficient Tradeoff Hypothesis: the order of elements in natural language is under pressure to enable favorable memory-surprisal tradeoffs. We derive that languages enable more efficient tradeoffs when they exhibit information locality: when predictive information about an element is concentrated in its recent past. We provide empirical evidence from three test domains in support of the Efficient Tradeoff Hypothesis: a reanalysis of a miniature artificial language learning experiment, a large-scale study of word order in corpora of 54 languages, and an analysis of morpheme order in two agglutinative languages. These results suggest that principles of order in natural language can be explained via highly generic cognitively motivated principles and lend support to efficiency-based models of the structure of human language.},
	urldate = {2021-05-17},
	institution = {PsyArXiv},
	author = {Hahn, Michael and Degen, Judith and Futrell, Richard},
	month = jun,
	year = {2020},
	doi = {10.31234/osf.io/nu4qz},
	note = {type: article},
	keywords = {Linguistics, Psycholinguistics and Neurolinguistics, Social and Behavioral Sciences, ur},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/E5VMHWTZ/Hahn et al. - 2020 - Modeling word and morpheme order in natural langua.pdf:application/pdf},
}

@article{newman-griffis_translational_2021,
	title = {Translational {NLP}: {A} {New} {Paradigm} and {General} {Principles} for {Natural} {Language} {Processing} {Research}},
	shorttitle = {Translational {NLP}},
	url = {http://arxiv.org/abs/2104.07874},
	abstract = {Natural language processing (NLP) research combines the study of universal principles, through basic science, with applied science targeting specific use cases and settings. However, the process of exchange between basic NLP and applications is often assumed to emerge naturally, resulting in many innovations going unapplied and many important questions left unstudied. We describe a new paradigm of Translational NLP, which aims to structure and facilitate the processes by which basic and applied NLP research inform one another. Translational NLP thus presents a third research paradigm, focused on understanding the challenges posed by application needs and how these challenges can drive innovation in basic science and technology design. We show that many significant advances in NLP research have emerged from the intersection of basic principles with application needs, and present a conceptual framework outlining the stakeholders and key questions in translational research. Our framework provides a roadmap for developing Translational NLP as a dedicated research area, and identifies general translational principles to facilitate exchange between basic and applied research.},
	urldate = {2021-05-17},
	journal = {arXiv:2104.07874 [cs]},
	author = {Newman-Griffis, Denis and Lehman, Jill Fain and Rosé, Carolyn and Hochheiser, Harry},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.07874},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, important},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/XRP3BW7E/Newman-Griffis et al. - 2021 - Translational NLP A New Paradigm and General Prin.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/GXY8ZYHX/2104.html:text/html},
}

@article{hovy_importance_2021,
	title = {The {Importance} of {Modeling} {Social} {Factors} of {Language}: {Theory} and {Practice}},
	abstract = {Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language’s social factors. We show that current NLP systems systematically break down when faced with interpreting the social factors of language. This limits applications to a subset of information-related tasks and prevents NLP from reaching human-level performance. At the same time, systems that incorporate even a minimum of social factors already show remarkable improvements. We formalize a taxonomy of seven social factors based on linguistic theory and exemplify current failures and emerging successes for each of them. We suggest that the NLP community address social factors to get closer to the goal of humanlike language understanding.},
	language = {en},
	author = {Hovy, Dirk and Yang, Diyi},
	month = may,
	year = {2021},
	keywords = {important},
	pages = {15},
	file = {Hovy and Yang - The Importance of Modeling Social Factors of Langu.pdf:/home/brendon/academic/misc/Zotero/storage/VGHHEW6H/Hovy and Yang - The Importance of Modeling Social Factors of Langu.pdf:application/pdf},
}

@article{bullard_quasi-equivalence_2021,
	title = {Quasi-{Equivalence} {Discovery} for {Zero}-{Shot} {Emergent} {Communication}},
	url = {http://arxiv.org/abs/2103.08067},
	abstract = {Effective communication is an important skill for enabling information exchange in multi-agent settings and emergent communication is now a vibrant field of research, with common settings involving discrete cheap-talk channels. Since, by definition, these settings involve arbitrary encoding of information, typically they do not allow for the learned protocols to generalize beyond training partners. In contrast, in this work, we present a novel problem setting and the Quasi-Equivalence Discovery (QED) algorithm that allows for zero-shot coordination (ZSC), i.e., discovering protocols that can generalize to independently trained agents. Real world problem settings often contain costly communication channels, e.g., robots have to physically move their limbs, and a non-uniform distribution over intents. We show that these two factors lead to unique optimal ZSC policies in referential games, where agents use the energy cost of the messages to communicate intent. Other-Play was recently introduced for learning optimal ZSC policies, but requires prior access to the symmetries of the problem. Instead, QED can iteratively discovers the symmetries in this setting and converges to the optimal ZSC policy.},
	urldate = {2021-05-17},
	journal = {arXiv:2103.08067 [cs]},
	author = {Bullard, Kalesha and Kiela, Douwe and Pineau, Joelle and Foerster, Jakob},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.08067},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/HNGVCIAT/Bullard et al. - 2021 - Quasi-Equivalence Discovery for Zero-Shot Emergent.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/FZ3XTSDC/2103.html:text/html},
}

@article{denamganai_emergent_2020,
	title = {On ({Emergent}) {Systematic} {Generalisation} and {Compositionality} in {Visual} {Referential} {Games} with {Straight}-{Through} {Gumbel}-{Softmax} {Estimator}},
	url = {http://arxiv.org/abs/2012.10776},
	abstract = {The drivers of compositionality in artificial languages that emerge when two (or more) agents play a non-visual referential game has been previously investigated using approaches based on the REINFORCE algorithm and the (Neural) Iterated Learning Model. Following the more recent introduction of the {\textbackslash}textit\{Straight-Through Gumbel-Softmax\} (ST-GS) approach, this paper investigates to what extent the drivers of compositionality identified so far in the field apply in the ST-GS context and to what extent do they translate into (emergent) systematic generalisation abilities, when playing a visual referential game. Compositionality and the generalisation abilities of the emergent languages are assessed using topographic similarity and zero-shot compositional tests. Firstly, we provide evidence that the test-train split strategy significantly impacts the zero-shot compositional tests when dealing with visual stimuli, whilst it does not when dealing with symbolic ones. Secondly, empirical evidence shows that using the ST-GS approach with small batch sizes and an overcomplete communication channel improves compositionality in the emerging languages. Nevertheless, while shown robust with symbolic stimuli, the effect of the batch size is not so clear-cut when dealing with visual stimuli. Our results also show that not all overcomplete communication channels are created equal. Indeed, while increasing the maximum sentence length is found to be beneficial to further both compositionality and generalisation abilities, increasing the vocabulary size is found detrimental. Finally, a lack of correlation between the language compositionality at training-time and the agents' generalisation abilities is observed in the context of discriminative referential games with visual stimuli. This is similar to previous observations in the field using the generative variant with symbolic stimuli.},
	urldate = {2021-05-17},
	journal = {arXiv:2012.10776 [cs]},
	author = {Denamganaï, Kevin and Walker, James Alfred},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.10776},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, ur},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/X2S4GDIW/Denamganaï and Walker - 2020 - On (Emergent) Systematic Generalisation and Compos.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/VEKK3TET/2012.html:text/html},
}

@inproceedings{gupta_compositionality_2020,
	address = {Online},
	title = {Compositionality and {Capacity} in {Emergent} {Languages}},
	url = {https://www.aclweb.org/anthology/2020.repl4nlp-1.5},
	doi = {10.18653/v1/2020.repl4nlp-1.5},
	abstract = {Recent works have discussed the extent to which emergent languages can exhibit properties of natural languages particularly learning compositionality. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication in addition to the communicative bandwidth. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.},
	urldate = {2021-05-17},
	booktitle = {Proceedings of the 5th {Workshop} on {Representation} {Learning} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Gupta, Abhinav and Resnick, Cinjon and Foerster, Jakob and Dai, Andrew and Cho, Kyunghyun},
	month = jul,
	year = {2020},
	pages = {34--38},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/H89MN4I7/Gupta et al. - 2020 - Compositionality and Capacity in Emergent Language.pdf:application/pdf},
}

@article{manning_emergent_2020,
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	volume = {117},
	copyright = {© 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/48/30046},
	doi = {10.1073/pnas.1907367117},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.},
	language = {en},
	number = {48},
	urldate = {2021-05-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	month = dec,
	year = {2020},
	pmid = {32493748},
	note = {Publisher: National Academy of Sciences
Section: Colloquium on the Science of Deep Learning},
	keywords = {artificial neural netwok, learning, self-supervision, syntax},
	pages = {30046--30054},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/HC7P33XY/Manning et al. - 2020 - Emergent linguistic structure in artificial neural.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/J388C6VH/30046.html:text/html},
}

@article{lazaridou_multi-agent_2020,
	title = {Multi-agent {Communication} meets {Natural} {Language}: {Synergies} between {Functional} and {Structural} {Language} {Learning}},
	shorttitle = {Multi-agent {Communication} meets {Natural} {Language}},
	url = {http://arxiv.org/abs/2005.07064},
	abstract = {We present a method for combining multi-agent communication and traditional data-driven approaches to natural language learning, with an end goal of teaching agents to communicate with humans in natural language. Our starting point is a language model that has been trained on generic, not task-specific language data. We then place this model in a multi-agent self-play environment that generates task-specific rewards used to adapt or modulate the model, turning it into a task-conditional language model. We introduce a new way for combining the two types of learning based on the idea of reranking language model samples, and show that this method outperforms others in communicating with humans in a visual referential communication task. Finally, we present a taxonomy of different types of language drift that can occur alongside a set of measures to detect them.},
	urldate = {2021-05-17},
	journal = {arXiv:2005.07064 [cs]},
	author = {Lazaridou, Angeliki and Potapenko, Anna and Tieleman, Olivier},
	month = may,
	year = {2020},
	note = {arXiv: 2005.07064},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/VQD5I7YS/Lazaridou et al. - 2020 - Multi-agent Communication meets Natural Language .pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/Q8IKUHAJ/2005.html:text/html},
}

@inproceedings{chowdhury_escell_2020,
	title = {Escell: {Emergent} {Symbolic} {Cellular} {Language}},
	shorttitle = {Escell},
	doi = {10.1109/ISBI45749.2020.9098343},
	abstract = {We present ESCELL, a method for developing an emergent symbolic language of communication between multiple agents reasoning about cells. We show how agents are able to cooperate and communicate successfully in the form of symbols similar to human language to accomplish a task in the form of a referential game (Lewis` signaling game). In one form of the game, a sender and a receiver observe a set of cells from 5 different cell phenotypes. The sender is told one cell is a target and is allowed to send one symbol to the receiver from a fixed arbitrary vocabulary size. The receiver relies on the information in the symbol to identify the target cell. We train the sender and receiver networks to develop an innate emergent language between themselves to accomplish this task. We observe that the networks are able to successfully identify cells from 5 different phenotypes with an accuracy of 93.2\%. We also introduce a new form of the signaling game where the sender is shown one image instead of all the images that the receiver sees. The networks successfully develop an emergent language to get an identification accuracy of 77.8\%.},
	booktitle = {2020 {IEEE} 17th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI})},
	author = {Chowdhury, Aritra and Kubricht, James R. and Sood, Anup and Tu, Peter and Santamaria-Pang, Alberto},
	month = apr,
	year = {2020},
	note = {ISSN: 1945-8452},
	keywords = {Machine learning, ur, cell classification, Computer architecture, emergent languages, Games, multi-agent communication, Neural networks, Proteins, Receivers, referential games, symbolic deep learning, Vocabulary},
	pages = {1604--1607},
	file = {Submitted Version:/home/brendon/academic/misc/Zotero/storage/8SS3W6KN/Chowdhury et al. - 2020 - Escell Emergent Symbolic Cellular Language.pdf:application/pdf},
}

@article{keresztury_compositional_2020,
	title = {Compositional properties of emergent languages in deep learning},
	url = {http://arxiv.org/abs/2001.08618},
	abstract = {Recent findings in multi-agent deep learning systems point towards the emergence of compositional languages. These claims are often made without exact analysis or testing of the language. In this work, we analyze the emergent language resulting from two different cooperative multi-agent game with more exact measures for compositionality. Our findings suggest that solutions found by deep learning models are often lacking the ability to reason on an abstract level therefore failing to generalize the learned knowledge to out of the training distribution examples. Strategies for testing compositional capacities and emergence of human-level concepts are discussed.},
	urldate = {2021-05-17},
	journal = {arXiv:2001.08618 [cs, stat]},
	author = {Keresztury, Bence and Bruni, Elia},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.08618},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/ZPVXNAT2/Keresztury and Bruni - 2020 - Compositional properties of emergent languages in .pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/5SL7AZDT/2001.html:text/html},
}

@article{kharitonov_emergent_2020,
	title = {Emergent {Language} {Generalization} and {Acquisition} {Speed} are not tied to {Compositionality}},
	url = {http://arxiv.org/abs/2004.03420},
	abstract = {Studies of discrete languages emerging when neural agents communicate to solve a joint task often look for evidence of compositional structure. This stems for the expectation that such a structure would allow languages to be acquired faster by the agents and enable them to generalize better. We argue that these beneficial properties are only loosely connected to compositionality. In two experiments, we demonstrate that, depending on the task, non-compositional languages might show equal, or better, generalization performance and acquisition speed than compositional ones. Further research in the area should be clearer about what benefits are expected from compositionality, and how the latter would lead to them.},
	urldate = {2021-05-17},
	journal = {arXiv:2004.03420 [cs]},
	author = {Kharitonov, Eugene and Baroni, Marco},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.03420},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/W7WRYGSD/Kharitonov and Baroni - 2020 - Emergent Language Generalization and Acquisition S.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/NWBVKSBK/2004.html:text/html},
}

@article{hazra_zero-shot_2021,
	title = {Zero-{Shot} {Generalization} using {Intrinsically} {Motivated} {Compositional} {Emergent} {Protocols}},
	url = {http://arxiv.org/abs/2105.05069},
	abstract = {Human language has been described as a system that makes {\textbackslash}textit\{use of finite means to express an unlimited array of thoughts\}. Of particular interest is the aspect of compositionality, whereby, the meaning of a compound language expression can be deduced from the meaning of its constituent parts. If artificial agents can develop compositional communication protocols akin to human language, they can be made to seamlessly generalize to unseen combinations. Studies have recognized the role of curiosity in enabling linguistic development in children. In this paper, we seek to use this intrinsic feedback in inducing a systematic and unambiguous protolanguage. We demonstrate how compositionality can enable agents to not only interact with unseen objects but also transfer skills from one task to another in a zero-shot setting: {\textbackslash}textit\{Can an agent, trained to `pull' and `push twice', `pull twice'?\}.},
	urldate = {2021-05-17},
	journal = {arXiv:2105.05069 [cs]},
	author = {Hazra, Rishi and Dixit, Sonu and Sen, Sayambhu},
	month = may,
	year = {2021},
	note = {arXiv: 2105.05069},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/SCBHPS6R/Hazra et al. - 2021 - Zero-Shot Generalization using Intrinsically Motiv.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/6RRKHK22/2105.html:text/html},
}

@article{brandizzi_rlupus_nodate,
	title = {{RLupus}: {Cooperation} through {Emergent} {Communication}  in {The} {Werewolf} {Social} {Deduction} {Game}},
	abstract = {Multi-agent systems have been studied intensively for their ability to develop complex behaviors from a simple set of rules. One such behavior is cooperation achievable through communication which can be either hand-crafted or emergent. The environmental setting is crucial to determine what kind of cooperation is needed and how communication should be exchanged by working agents. Social games are a good choice to study the emergence of sophisticated communication patterns for their ability to cherry-pick certain aspects of interaction which are then easier to contextualize.},
	language = {en},
	author = {Brandizzi, Nicolo' and Iocchi, Luca and Grossi, Davide},
	keywords = {Computer Science - Multiagent Systems},
	pages = {8},
	file = {Brandizzi et al. - 2021 - RLupus Cooperation through emergent communication.pdf:/home/brendon/academic/misc/Zotero/storage/QEMAJRAR/Brandizzi et al. - 2021 - RLupus Cooperation through emergent communication.pdf:application/pdf;Brandizzi et al. - RLupus Cooperation through Emergent Communication.pdf:/home/brendon/academic/misc/Zotero/storage/CAYC79RV/Brandizzi et al. - RLupus Cooperation through Emergent Communication.pdf:application/pdf},
}

@article{hazra_infinite_2021,
	title = {Infinite use of finite means: {Zero}-{Shot} {Generalization} using {Compositional} {Emergent} {Protocols}},
	shorttitle = {Infinite use of finite means},
	url = {http://arxiv.org/abs/2012.05011},
	abstract = {Human language has been described as a system that makes {\textbackslash}textit\{use of finite means to express an unlimited array of thoughts\}. Of particular interest is the aspect of compositionality, whereby, the meaning of a compound language expression can be deduced from the meaning of its constituent parts. If artificial agents can develop compositional communication protocols akin to human language, they can be made to seamlessly generalize to unseen combinations. However, the real question is, how do we induce compositionality in emergent communication? Studies have recognized the role of curiosity in enabling linguistic development in children. It is this same intrinsic urge that drives us to master complex tasks with decreasing amounts of explicit reward. In this paper, we seek to use this intrinsic feedback in inducing a systematic and unambiguous protolanguage in artificial agents. We show how these rewards can be leveraged in training agents to induce compositionality in absence of any external feedback. Additionally, we introduce gComm, an environment for investigating grounded language acquisition in 2D-grid environments. Using this, we demonstrate how compositionality can enable agents to not only interact with unseen objects but also transfer skills from one task to another in a zero-shot setting: {\textbackslash}textit\{Can an agent, trained to `pull' and `push twice', `pull twice'?\}.},
	urldate = {2021-05-22},
	journal = {arXiv:2012.05011 [cs]},
	author = {Hazra, Rishi and Dixit, Sonu and Sen, Sayambhu},
	month = may,
	year = {2021},
	note = {arXiv: 2012.05011},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/TJM7KSE7/Hazra et al. - 2021 - Infinite use of finite means Zero-Shot Generaliza.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/TAHZX3MZ/2012.html:text/html},
}

@inproceedings{bisk_experience_2020,
	address = {Online},
	title = {Experience {Grounds} {Language}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.703},
	doi = {10.18653/v1/2020.emnlp-main.703},
	abstract = {Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.},
	urldate = {2021-05-25},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Bisk, Yonatan and Holtzman, Ari and Thomason, Jesse and Andreas, Jacob and Bengio, Yoshua and Chai, Joyce and Lapata, Mirella and Lazaridou, Angeliki and May, Jonathan and Nisnevich, Aleksandr and Pinto, Nicolas and Turian, Joseph},
	month = nov,
	year = {2020},
	keywords = {important},
	pages = {8718--8735},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/3DYNQRF2/Bisk et al. - 2020 - Experience Grounds Language.pdf:application/pdf},
}

@article{hockett_origin_1960,
	title = {The {Origin} of {Speech}},
	language = {en},
	journal = {SCIENTIFIC AMERICAN},
	author = {Hockett, Charlcs F},
	year = {1960},
	pages = {11},
	file = {Hockett - 1960 - The Origin of Speech.pdf:/home/brendon/academic/misc/Zotero/storage/RRS8LE7T/Hockett - 1960 - The Origin of Speech.pdf:application/pdf},
}

@article{sai_survey_2020,
	title = {A {Survey} of {Evaluation} {Metrics} {Used} for {NLG} {Systems}},
	url = {http://arxiv.org/abs/2008.12009},
	abstract = {The success of Deep Learning has created a surge in interest in a wide a range of Natural Language Generation (NLG) tasks. Deep Learning has not only pushed the state of the art in several existing NLG tasks but has also facilitated researchers to explore various newer NLG tasks such as image captioning. Such rapid progress in NLG has necessitated the development of accurate automatic evaluation metrics that would allow us to track the progress in the field of NLG. However, unlike classification tasks, automatically evaluating NLG systems in itself is a huge challenge. Several works have shown that early heuristic-based metrics such as BLEU, ROUGE are inadequate for capturing the nuances in the different NLG tasks. The expanding number of NLG models and the shortcomings of the current metrics has led to a rapid surge in the number of evaluation metrics proposed since 2014. Moreover, various evaluation metrics have shifted from using pre-determined heuristic-based formulae to trained transformer models. This rapid change in a relatively short time has led to the need for a survey of the existing NLG metrics to help existing and new researchers to quickly come up to speed with the developments that have happened in NLG evaluation in the last few years. Through this survey, we first wish to highlight the challenges and difficulties in automatically evaluating NLG systems. Then, we provide a coherent taxonomy of the evaluation metrics to organize the existing metrics and to better understand the developments in the field. We also describe the different metrics in detail and highlight their key contributions. Later, we discuss the main shortcomings identified in the existing metrics and describe the methodology used to evaluate evaluation metrics. Finally, we discuss our suggestions and recommendations on the next steps forward to improve the automatic evaluation metrics.},
	urldate = {2021-06-15},
	journal = {arXiv:2008.12009 [cs]},
	author = {Sai, Ananya B. and Mohankumar, Akash Kumar and Khapra, Mitesh M.},
	month = oct,
	year = {2020},
	note = {arXiv: 2008.12009},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/CBHVKX5C/Sai et al. - 2020 - A Survey of Evaluation Metrics Used for NLG System.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/UJ3GUUNH/2008.html:text/html},
}

@article{chaabouni_communicating_2021,
	title = {Communicating artificial neural networks develop efficient color-naming systems},
	volume = {118},
	copyright = {Copyright © 2021 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/118/12/e2016569118},
	doi = {10.1073/pnas.2016569118},
	abstract = {{\textless}p{\textgreater}Words categorize the semantic fields they refer to in ways that maximize communication accuracy while minimizing complexity. Focusing on the well-studied color domain, we show that artificial neural networks trained with deep-learning techniques to play a discrimination game develop communication systems whose distribution on the accuracy/complexity plane closely matches that of human languages. The observed variation among emergent color-naming systems is explained by different degrees of discriminative need, of the sort that might also characterize different human communities. Like human languages, emergent systems show a preference for relatively low-complexity solutions, even at the cost of imperfect communication. We demonstrate next that the nature of the emergent systems crucially depends on communication being discrete (as is human word usage). When continuous message passing is allowed, emergent systems become more complex and eventually less efficient. Our study suggests that efficient semantic categorization is a general property of discrete communication systems, not limited to human language. It suggests moreover that it is exactly the discrete nature of such systems that, acting as a bottleneck, pushes them toward low complexity and optimal efficiency.{\textless}/p{\textgreater}},
	language = {en},
	number = {12},
	urldate = {2021-06-25},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Chaabouni, Rahma and Kharitonov, Eugene and Dupoux, Emmanuel and Baroni, Marco},
	month = mar,
	year = {2021},
	pmid = {33723064},
	note = {Publisher: National Academy of Sciences
Section: Social Sciences},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/D2PUL4HR/Chaabouni et al. - 2021 - Communicating artificial neural networks develop e.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/A999BUEC/e2016569118.html:text/html},
}

@article{pasemann_emergence_nodate,
	title = {The {Emergence} of {Communication} by {Evolving} {Dynamical} {Systems}},
	url = {https://core.ac.uk/reader/147981920},
	urldate = {2021-10-05},
	author = {Pasemann, Frank},
	keywords = {ur},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/VK7HAX4E/Pasemann - The Emergence of Communication by Evolving Dynamic.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/C6KS4WFD/147981920.html:text/html},
}

@inproceedings{harding_graesser_emergent_2019,
	address = {Hong Kong, China},
	title = {Emergent {Linguistic} {Phenomena} in {Multi}-{Agent} {Communication} {Games}},
	url = {https://aclanthology.org/D19-1384},
	doi = {10.18653/v1/D19-1384},
	abstract = {We describe a multi-agent communication framework for examining high-level linguistic phenomena at the community-level. We demonstrate that complex linguistic behavior observed in natural language can be reproduced in this simple setting: i) the outcome of contact between communities is a function of inter- and intra-group connectivity; ii) linguistic contact either converges to the majority protocol, or in balanced cases leads to novel creole languages of lower complexity; and iii) a linguistic continuum emerges where neighboring languages are more mutually intelligible than farther removed languages. We conclude that at least some of the intricate properties of language evolution need not depend on complex evolved linguistic capabilities, but can emerge from simple social exchanges between perceptually-enabled agents playing communication games.},
	urldate = {2021-10-05},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Harding Graesser, Laura and Cho, Kyunghyun and Kiela, Douwe},
	month = nov,
	year = {2019},
	pages = {3700--3710},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/PQ6YT5U5/Harding Graesser et al. - 2019 - Emergent Linguistic Phenomena in Multi-Agent Commu.pdf:application/pdf},
}

@inproceedings{eccles_biases_2019,
	title = {Biases for {Emergent} {Communication} in {Multi}-agent {Reinforcement} {Learning}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/fe5e7cb609bdbe6d62449d61849c38b0-Abstract.html},
	urldate = {2021-10-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Eccles, Tom and Bachrach, Yoram and Lever, Guy and Lazaridou, Angeliki and Graepel, Thore},
	year = {2019},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/YSDCFUU5/Eccles et al. - 2019 - Biases for Emergent Communication in Multi-agent R.pdf:application/pdf},
}

@inproceedings{kottur_natural_2017,
	address = {Copenhagen, Denmark},
	title = {Natural {Language} {Does} {Not} {Emerge} `{Naturally}' in {Multi}-{Agent} {Dialog}},
	url = {https://aclanthology.org/D17-1321},
	doi = {10.18653/v1/D17-1321},
	abstract = {A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task \& Talk reference game between two agents as a testbed, we present a sequence of `negative' results culminating in a `positive' one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge `naturally',despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.},
	urldate = {2021-10-12},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kottur, Satwik and Moura, José and Lee, Stefan and Batra, Dhruv},
	month = sep,
	year = {2017},
	keywords = {emergent language, important},
	pages = {2962--2967},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/2EWZNQU9/Kottur et al. - 2017 - Natural Language Does Not Emerge `Naturally' in Mu.pdf:application/pdf},
}

@article{broome_engineering_1985,
	title = {{ENGINEERING} {THE} {PHILOSOPHY} {OF} {SCIENCE}},
	volume = {16},
	issn = {0026-1068},
	url = {http://www.jstor.org/stable/24437062},
	number = {1},
	urldate = {2021-10-19},
	journal = {Metaphilosophy},
	author = {BROOME, TAFT H.},
	year = {1985},
	note = {Publisher: Wiley},
	pages = {47--56},
	file = {JSTOR Full Text PDF:/home/brendon/academic/misc/Zotero/storage/4CRN5UMS/BROOME - 1985 - ENGINEERING THE PHILOSOPHY OF SCIENCE.pdf:application/pdf},
}

@article{poser_structural_1998,
	title = {On {Structural} {Differences} {Between} {Science} and {Engineering}},
	volume = {4},
	issn = {1091-8264},
	url = {http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=sptq_1998_0004_0002_0128_0135&svc_id=info:www.pdcnet.org/collection},
	doi = {10.5840/techne1998426},
	language = {en},
	number = {2},
	urldate = {2021-10-19},
	journal = {Society for Philosophy and Technology Quarterly Electronic Journal},
	author = {Poser, Hans and {Society for Philosophy and Technology}},
	year = {1998},
	pages = {128--135},
	file = {Poser and Society for Philosophy and Technology - 1998 - On Structural Differences Between Science and Engi.pdf:/home/brendon/academic/misc/Zotero/storage/G65B5CVJ/Poser and Society for Philosophy and Technology - 1998 - On Structural Differences Between Science and Engi.pdf:application/pdf},
}

@incollection{franssen_philosophy_2018,
	edition = {Fall 2018},
	title = {Philosophy of {Technology}},
	url = {https://plato.stanford.edu/archives/fall2018/entriesechnology/},
	abstract = {If philosophy is the attempt “to understand how things in thebroadest possible sense of the term hang together in the broadestpossible sense of the term”, as Sellars (1962) put it,philosophy should not ignore technology. It is largely by technologythat contemporary society hangs together. It is hugely important notonly as an economic force but also as a cultural force. Indeed duringthe last two centuries, when it gradually emerged as a discipline,philosophy of technology has mostly been concerned with the meaning oftechnology for, and its impact on, society and culture, rather thanwith technology itself. Mitcham (1994) calls this type of philosophyof technology “humanities philosophy of technology”because it accepts “the primacy of the humanities overtechnologies” and is continuous with the overall perspective ofthe humanities (and some of the social sciences). Only recently abranch of the philosophy of technology has developed that is concernedwith technology itself and that aims to understand both the practiceof designing and creating artifacts (in a wide sense, includingartificial processes and systems) and the nature of the things socreated. This latter branch of the philosophy of technology seekscontinuity with the philosophy of science and with several otherfields in the analytic tradition in modern philosophy, such as thephilosophy of action and decision-making, rather than with thehumanities and social science., The entry starts with a brief historical overview, then continues witha presentation of the themes on which modern analytic philosophy oftechnology focuses. This is followed by a discussion of the societaland ethical aspects of technology, in which some of the concerns ofhumanities philosophy of technology are addressed. This twofoldpresentation takes into consideration the development of technology asthe outcome of a process originating within and guided by the practiceof engineering, by standards on which only limited societal control isexercised, as well as the consequences for society of theimplementation of the technology so created, which result fromprocesses upon which only limited control can be exercised.},
	urldate = {2021-10-19},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Franssen, Maarten and Lokhorst, Gert-Jan and van de Poel, Ibo},
	editor = {Zalta, Edward N.},
	year = {2018},
}

@misc{noauthor_agent-based_nodate,
	title = {Agent-{Based} {Computational} {Economics} ({Tesfatsion})},
	url = {http://www2.econ.iastate.edu/tesfatsi/ace.htm},
	urldate = {2021-10-22},
	file = {Agent-Based Computational Economics (Tesfatsion):/home/brendon/academic/misc/Zotero/storage/SU47P5NK/ace.html:text/html},
}

@misc{noauthor_philosophy_nodate,
	title = {Philosophy of {Economics}},
	url = {http://www-personal.umd.umich.edu/~delittle/Encyclopedia%20entries/Philosophy%20of%20Economics.htm},
	urldate = {2021-11-05},
	file = {Philosophy of Economics:/home/brendon/academic/misc/Zotero/storage/YQ64LJD9/Philosophy of Economics.html:text/html},
}

@article{werner_evolution_1991,
	title = {Evolution of {Communication} in {Artificial} {Organisms}},
	author = {Werner, Gregory M. and Dyer, Michael G.},
	year = {1991},
	file = {_.pdf:/home/brendon/academic/misc/Zotero/storage/QDF23I3E/_.pdf:application/pdf},
}

@article{noukhovitch_emergent_2021,
	title = {Emergent {Communication} under {Competition}},
	url = {http://arxiv.org/abs/2101.10276},
	abstract = {The literature in modern machine learning has only negative results for learning to communicate between competitive agents using standard RL. We introduce a modified sender-receiver game to study the spectrum of partially-competitive scenarios and show communication can indeed emerge in a competitive setting. We empirically demonstrate three key takeaways for future research. First, we show that communication is proportional to cooperation, and it can occur for partially competitive scenarios using standard learning algorithms. Second, we highlight the difference between communication and manipulation and extend previous metrics of communication to the competitive case. Third, we investigate the negotiation game where previous work failed to learn communication between independent agents (Cao et al., 2018). We show that, in this setting, both agents must benefit from communication for it to emerge; and, with a slight modification to the game, we demonstrate successful communication between competitive agents. We hope this work overturns misconceptions and inspires more research in competitive emergent communication.},
	urldate = {2021-12-17},
	journal = {arXiv:2101.10276 [cs]},
	author = {Noukhovitch, Michael and LaCroix, Travis and Lazaridou, Angeliki and Courville, Aaron},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.10276},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/HNLGJFDJ/Noukhovitch et al. - 2021 - Emergent Communication under Competition.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/RK78BK26/2101.html:text/html},
}

@article{mihai_emergence_2021,
	title = {The emergence of visual semantics through communication games},
	url = {http://arxiv.org/abs/2101.10253},
	abstract = {The emergence of communication systems between agents which learn to play referential signalling games with realistic images has attracted a lot of attention recently. The majority of work has focused on using fixed, pretrained image feature extraction networks which potentially bias the information the agents learn to communicate. In this work, we consider a signalling game setting in which a `sender' agent must communicate the information about an image to a `receiver' who must select the correct image from many distractors. We investigate the effect of the feature extractor's weights and of the task being solved on the visual semantics learned by the models. We first demonstrate to what extent the use of pretrained feature extraction networks inductively bias the visual semantics conveyed by emergent communication channel and quantify the visual semantics that are induced. We then go on to explore ways in which inductive biases can be introduced to encourage the emergence of semantically meaningful communication without the need for any form of supervised pretraining of the visual feature extractor. We impose various augmentations to the input images and additional tasks in the game with the aim to induce visual representations which capture conceptual properties of images. Through our experiments, we demonstrate that communication systems which capture visual semantics can be learned in a completely self-supervised manner by playing the right types of game. Our work bridges a gap between emergent communication research and self-supervised feature learning.},
	urldate = {2021-12-17},
	journal = {arXiv:2101.10253 [cs]},
	author = {Mihai, Daniela and Hare, Jonathon},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.10253},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/4HJ3AJTR/Mihai and Hare - 2021 - The emergence of visual semantics through communic.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/HDSN5CLB/2101.html:text/html},
}

@article{hagiwara_multiagent_2021,
	title = {Multiagent {Multimodal} {Categorization} for {Symbol} {Emergence}: {Emergent} {Communication} via {Interpersonal} {Cross}-modal {Inference}},
	shorttitle = {Multiagent {Multimodal} {Categorization} for {Symbol} {Emergence}},
	url = {http://arxiv.org/abs/2109.07194},
	abstract = {This paper describes a computational model of multiagent multimodal categorization that realizes emergent communication. We clarify whether the computational model can reproduce the following functions in a symbol emergence system, comprising two agents with different sensory modalities playing a naming game. (1) Function for forming a shared lexical system that comprises perceptual categories and corresponding signs, formed by agents through individual learning and semiotic communication between agents. (2) Function to improve the categorization accuracy in an agent via semiotic communication with another agent, even when some sensory modalities of each agent are missing. (3) Function that an agent infers unobserved sensory information based on a sign sampled from another agent in the same manner as cross-modal inference. We propose an interpersonal multimodal Dirichlet mixture (Inter-MDM), which is derived by dividing an integrative probabilistic generative model, which is obtained by integrating two Dirichlet mixtures (DMs). The Markov chain Monte Carlo algorithm realizes emergent communication. The experimental results demonstrated that Inter-MDM enables agents to form multimodal categories and appropriately share signs between agents. It is shown that emergent communication improves categorization accuracy, even when some sensory modalities are missing. Inter-MDM enables an agent to predict unobserved information based on a shared sign.},
	urldate = {2021-12-17},
	journal = {arXiv:2109.07194 [cs]},
	author = {Hagiwara, Yoshinobu and Furukawa, Kazuma and Taniguchi, Akira and Taniguchi, Tadahiro},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.07194},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/3IITYDRM/Hagiwara et al. - 2021 - Multiagent Multimodal Categorization for Symbol Em.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/C3MVNBAK/2109.html:text/html},
}

@article{li_learning_2021,
	title = {Learning {Emergent} {Discrete} {Message} {Communication} for {Cooperative} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2102.12550},
	abstract = {Communication is a important factor that enables agents work cooperatively in multi-agent reinforcement learning (MARL). Most previous work uses continuous message communication whose high representational capacity comes at the expense of interpretability. Allowing agents to learn their own discrete message communication protocol emerged from a variety of domains can increase the interpretability for human designers and other agents.This paper proposes a method to generate discrete messages analogous to human languages, and achieve communication by a broadcast-and-listen mechanism based on self-attention. We show that discrete message communication has performance comparable to continuous message communication but with much a much smaller vocabulary size.Furthermore, we propose an approach that allows humans to interactively send discrete messages to agents.},
	urldate = {2021-12-17},
	journal = {arXiv:2102.12550 [cs]},
	author = {Li, Sheng and Zhou, Yutai and Allen, Ross and Kochenderfer, Mykel J.},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.12550},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/5RFR7EIK/Li et al. - 2021 - Learning Emergent Discrete Message Communication f.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/6584ZU4M/2102.html:text/html},
}

@article{grupen_curriculum-driven_2021,
	title = {Curriculum-{Driven} {Multi}-{Agent} {Learning} and the {Role} of {Implicit} {Communication} in {Teamwork}},
	url = {http://arxiv.org/abs/2106.11156},
	abstract = {We propose a curriculum-driven learning strategy for solving difficult multi-agent coordination tasks. Our method is inspired by a study of animal communication, which shows that two straightforward design features (mutual reward and decentralization) support a vast spectrum of communication protocols in nature. We highlight the importance of similarly interpreting emergent communication as a spectrum. We introduce a toroidal, continuous-space pursuit-evasion environment and show that naive decentralized learning does not perform well. We then propose a novel curriculum-driven strategy for multi-agent learning. Experiments with pursuit-evasion show that our approach enables decentralized pursuers to learn to coordinate and capture a superior evader, significantly outperforming sophisticated analytical policies. We argue through additional quantitative analysis -- including influence-based measures such as Instantaneous Coordination -- that emergent implicit communication plays a large role in enabling superior levels of coordination.},
	urldate = {2021-12-17},
	journal = {arXiv:2106.11156 [cs]},
	author = {Grupen, Niko A. and Lee, Daniel D. and Selman, Bart},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.11156},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/WQ2HXMNI/Grupen et al. - 2021 - Curriculum-Driven Multi-Agent Learning and the Rol.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/R7C8ZRPL/2106.html:text/html},
}

@article{mu_emergent_2021,
	title = {Emergent {Communication} of {Generalizations}},
	url = {http://arxiv.org/abs/2106.02668},
	abstract = {To build agents that can collaborate effectively with others, recent research has trained artificial agents to communicate with each other in Lewis-style referential games. However, this often leads to successful but uninterpretable communication. We argue that this is due to the game objective: communicating about a single object in a shared visual context is prone to overfitting and does not encourage language useful beyond concrete reference. In contrast, human language conveys a rich variety of abstract ideas. To promote such skills, we propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. We find that these games greatly improve systematicity and interpretability of the learned languages, according to several metrics in the literature. Finally, we propose a method for identifying logical operations embedded in the emergent languages by learning an approximate compositional reconstruction of the language.},
	urldate = {2021-12-17},
	journal = {arXiv:2106.02668 [cs]},
	author = {Mu, Jesse and Goodman, Noah},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.02668},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/NH7F56XH/Mu and Goodman - 2021 - Emergent Communication of Generalizations.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/GJG6EZLN/2106.html:text/html},
}

@article{tucker_emergent_nodate,
	title = {Emergent {Discrete} {Communication} in {Semantic} {Spaces}},
	abstract = {Neural agents trained in reinforcement learning settings can learn to communicate among themselves via discrete tokens, accomplishing as a team what agents would be unable to do alone. However, the current standard of using one-hot vectors as discrete communication tokens prevents agents from acquiring more desirable aspects of communication such as zero-shot understanding. Inspired by word embedding techniques from natural language processing, we propose neural agent architectures that enables them to communicate via discrete tokens derived from a learned, continuous space. We show in a decision theoretic framework that our technique optimizes communication over a wide range of scenarios, whereas one-hot tokens are only optimal under restrictive assumptions. In self-play experiments, we validate that our trained agents learn to cluster tokens in semantically-meaningful ways, allowing them communicate in noisy environments where other techniques fail. Lastly, we demonstrate both that agents using our method can effectively respond to novel human communication and that humans can understand unlabeled emergent agent communication, outperforming the use of one-hot communication.},
	language = {en},
	author = {Tucker, Mycal and Li, Huao and Agrawal, Siddharth and Hughes, Dana and Sycara, Katia and Lewis, Michael and Shah, Julie},
	pages = {23},
	file = {Tucker et al. - Emergent Discrete Communication in Semantic Spaces.pdf:/home/brendon/academic/misc/Zotero/storage/9IDQ8VXP/Tucker et al. - Emergent Discrete Communication in Semantic Spaces.pdf:application/pdf},
}

@inproceedings{gupta_meta_2021,
	title = {Meta {Learning} for {Multi}-agent {Communication}},
	url = {https://openreview.net/forum?id=mo_rSxN38VO},
	abstract = {We propose an adaptive population-based method to train agents in a cooperative multi-agent reinforcement learning setup. We show that our method induces useful diversity into a population of...},
	language = {en},
	urldate = {2021-12-17},
	author = {Gupta, Abhinav and Lazaridou, Angeliki and Lanctot, Marc},
	month = mar,
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/LIWMTPBU/Gupta et al. - 2021 - Meta Learning for Multi-agent Communication.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/W66BGQ29/forum.html:text/html},
}

@article{shvydkoy_multiflocks_2021,
	title = {Multiflocks: {Emergent} {Dynamics} in {Systems} with {Multiscale} {Collective} {Behavior}},
	volume = {19},
	issn = {1540-3459},
	shorttitle = {Multiflocks},
	url = {https://epubs.siam.org/doi/abs/10.1137/20M1324454},
	doi = {10.1137/20M1324454},
	abstract = {We study the multiscale description of large-time collective behavior of agents driven by alignment. The resulting multiflock dynamics arises naturally with realistic initial configurations consisting of multiple spatial scaling, which in turn peak at different time scales. We derive a “master-equation” which describes a complex multiflock congregations governed by two ingredients: (i) a fast inner-flock communication; and (ii) a slow(-er) interflock communication. The latter is driven by macroscopic observables which feature the up-scaling of the problem. We extend the current monoflock theory, proving a series of results which describe rates of multiflocking with natural dependencies on communication strengths. Both agent-based, kinetic, and hydrodynamic descriptions are considered, with particular emphasis placed on the discrete and macroscopic descriptions.},
	number = {2},
	urldate = {2021-12-17},
	journal = {Multiscale Modeling \& Simulation},
	author = {Shvydkoy, Roman and Tadmor, Eitan},
	month = jan,
	year = {2021},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {35Q35, 76N10, 92D25, alignment, Cucker--Smale, hydrodynamics, large-time behavior, multiflock, multiscale},
	pages = {1115--1141},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/EQJKT7MU/Shvydkoy and Tadmor - 2021 - Multiflocks Emergent Dynamics in Systems with Mul.pdf:application/pdf},
}

@article{guo_expressivity_2021,
	title = {Expressivity of {Emergent} {Language} is a {Trade}-off between {Contextual} {Complexity} and {Unpredictability}},
	url = {http://arxiv.org/abs/2106.03982},
	abstract = {Researchers are now using deep learning models to explore the emergence of language in various language games, where simulated agents interact and develop an emergent language to solve a task. Although it is quite intuitive that different types of language games posing different communicative challenges might require emergent languages which encode different levels of information, there is no existing work exploring the expressivity of the emergent languages. In this work, we propose a definition of partial order between expressivity based on the generalisation performance across different language games. We also validate the hypothesis that expressivity of emergent languages is a trade-off between the complexity and unpredictability of the context those languages are used in. Our second novel contribution is introducing contrastive loss into the implementation of referential games. We show that using our contrastive loss alleviates the collapse of message types seen using standard referential loss functions.},
	urldate = {2021-12-17},
	journal = {arXiv:2106.03982 [cs]},
	author = {Guo, Shangmin and Ren, Yi and Mathewson, Kory and Kirby, Simon and Albrecht, Stefano V. and Smith, Kenny},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.03982},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/GPDBX727/Guo et al. - 2021 - Expressivity of Emergent Language is a Trade-off b.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/X39IEU3R/2106.html:text/html},
}

@article{perkins_texrel_2021,
	title = {{TexRel}: a {Green} {Family} of {Datasets} for {Emergent} {Communications} on {Relations}},
	shorttitle = {{TexRel}},
	url = {http://arxiv.org/abs/2105.12804},
	abstract = {We propose a new dataset TexRel as a playground for the study of emergent communications, in particular for relations. By comparison with other relations datasets, TexRel provides rapid training and experimentation, whilst being sufficiently large to avoid overfitting in the context of emergent communications. By comparison with using symbolic inputs, TexRel provides a more realistic alternative whilst remaining efficient and fast to learn. We compare the performance of TexRel with a related relations dataset Shapeworld. We provide baseline performance results on TexRel for sender architectures, receiver architectures and end-to-end architectures. We examine the effect of multitask learning in the context of shapes, colors and relations on accuracy, topological similarity and clustering precision. We investigate whether increasing the size of the latent meaning space improves metrics of compositionality. We carry out a case-study on using TexRel to reproduce the results of an experiment in a recent paper that used symbolic inputs, but using our own non-symbolic inputs, from TexRel, instead.},
	urldate = {2021-12-17},
	journal = {arXiv:2105.12804 [cs]},
	author = {Perkins, Hugh},
	month = may,
	year = {2021},
	note = {arXiv: 2105.12804},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/F83TCTC8/Perkins - 2021 - TexRel a Green Family of Datasets for Emergent Co.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/CUYWP5J5/2105.html:text/html},
}

@article{vani_iterated_2021,
	title = {Iterated learning for emergent systematicity in {VQA}},
	url = {http://arxiv.org/abs/2105.01119},
	abstract = {Although neural module networks have an architectural bias towards compositionality, they require gold standard layouts to generalize systematically in practice. When instead learning layouts and modules jointly, compositionality does not arise automatically and an explicit pressure is necessary for the emergence of layouts exhibiting the right structure. We propose to address this problem using iterated learning, a cognitive science theory of the emergence of compositional languages in nature that has primarily been applied to simple referential games in machine learning. Considering the layouts of module networks as samples from an emergent language, we use iterated learning to encourage the development of structure within this language. We show that the resulting layouts support systematic generalization in neural agents solving the more complex task of visual question-answering. Our regularized iterated learning method can outperform baselines without iterated learning on SHAPES-SyGeT (SHAPES Systematic Generalization Test), a new split of the SHAPES dataset we introduce to evaluate systematic generalization, and on CLOSURE, an extension of CLEVR also designed to test systematic generalization. We demonstrate superior performance in recovering ground-truth compositional program structure with limited supervision on both SHAPES-SyGeT and CLEVR.},
	urldate = {2021-12-17},
	journal = {arXiv:2105.01119 [cs]},
	author = {Vani, Ankit and Schwarzer, Max and Lu, Yuchen and Dhekane, Eeshan and Courville, Aaron},
	month = may,
	year = {2021},
	note = {arXiv: 2105.01119},
	keywords = {Computer Science - Machine Learning, I.2.6},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/ZSCXJX7H/Vani et al. - 2021 - Iterated learning for emergent systematicity in VQ.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/IQDX5JUD/2105.html:text/html},
}

@article{cope_learning_2021,
	title = {Learning to {Communicate} with {Strangers} via {Channel} {Randomisation} {Methods}},
	url = {http://arxiv.org/abs/2104.09557},
	abstract = {We introduce two methods for improving the performance of agents meeting for the first time to accomplish a communicative task. The methods are: (1) `message mutation' during the generation of the communication protocol; and (2) random permutations of the communication channel. These proposals are tested using a simple two-player game involving a `teacher' who generates a communication protocol and sends a message, and a `student' who interprets the message. After training multiple agents via self-play we analyse the performance of these agents when they are matched with a stranger, i.e. their zero-shot communication performance. We find that both message mutation and channel permutation positively influence performance, and we discuss their effects.},
	urldate = {2021-12-17},
	journal = {arXiv:2104.09557 [cs]},
	author = {Cope, Dylan and Schoots, Nandi},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.09557},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/FM7MKN4G/Cope and Schoots - 2021 - Learning to Communicate with Strangers via Channel.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/R7RK5GLL/2104.html:text/html},
}

@inproceedings{vithanage_effect_2021,
	address = {Cham},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Effect of {Dialogue} {Structure} and {Memory} on {Language} {Emergence} in a {Multi}-task {Game}},
	isbn = {978-3-030-88113-9},
	doi = {10.1007/978-3-030-88113-9_17},
	abstract = {In language emergence, neural agents engage in finite-length conversations using a finite set of symbols to reach a given goal. In such systems, two key factors can determine the dialogue structure; the size of the symbol set and the conversation length. During training, agents invent and assign meanings to the symbols without any external supervision. Existing studies do not investigate how these models behave when they train under multiple tasks requiring different levels of coordination and information exchange. Moreover, only a handful of work discusses the relationship between the dialogue structure and the performance. In this paper, we formulate a game environment where neural agents simultaneously learn on heterogeneous tasks. Using our setup, we investigate how the dialogue structure and the agent’s capability of processing memory affect the agent performance across multiple tasks. We observed that memory capacity non-linearly affects the task performances, where the nature of the task influences this non-linearity. In contrast, the performance gain obtained by varying the dialogue structure is mostly task-independent. We further observed that agents prefer smaller symbol sets with longer conversation lengths than the converse.},
	language = {en},
	booktitle = {Advances in {Computational} {Collective} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Vithanage, Kasun and Wijesinghe, Rukshan and Xavier, Alex and Tissera, Dumindu and Jayasena, Sanath and Fernando, Subha},
	editor = {Wojtkiewicz, Krystian and Treur, Jan and Pimenidis, Elias and Maleszka, Marcin},
	year = {2021},
	pages = {212--224},
	file = {Springer Full Text PDF:/home/brendon/academic/misc/Zotero/storage/S84U96XA/Vithanage et al. - 2021 - Effect of Dialogue Structure and Memory on Languag.pdf:application/pdf},
}

@inproceedings{rodriguez_luna_internal_2020,
	address = {Online},
	title = {Internal and external pressures on language emergence: least effort, object constancy and frequency},
	shorttitle = {Internal and external pressures on language emergence},
	url = {https://www.aclweb.org/anthology/2020.findings-emnlp.397},
	doi = {10.18653/v1/2020.findings-emnlp.397},
	abstract = {In previous work, artiﬁcial agents were shown to achieve almost perfect accuracy in referential games where they have to communicate to identify images. Nevertheless, the resulting communication protocols rarely display salient features of natural languages, such as compositionality. In this paper, we propose some realistic sources of pressure on communication that avert this outcome. More specifically, we formalise the principle of least effort through an auxiliary objective. Moreover, we explore several game variants, inspired by the principle of object constancy, in which we alter the frequency, position, and luminosity of the objects in the images. We perform an extensive analysis on their effect through compositionality metrics, diagnostic classiﬁers, and zero-shot evaluation. Our ﬁndings reveal that the proposed sources of pressure result in emerging languages with less redundancy, more focus on high-level conceptual information, and better abilities of generalisation. Overall, our contributions reduce the gap between emergent and natural languages.},
	language = {en},
	urldate = {2021-12-17},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Rodríguez Luna, Diana and Ponti, Edoardo Maria and Hupkes, Dieuwke and Bruni, Elia},
	year = {2020},
	pages = {4428--4437},
	file = {Rodríguez Luna et al. - 2020 - Internal and external pressures on language emerge.pdf:/home/brendon/academic/misc/Zotero/storage/KU5BLZXK/Rodríguez Luna et al. - 2020 - Internal and external pressures on language emerge.pdf:application/pdf},
}

@article{verma_towards_2021,
	title = {Towards {Sample} {Efficient} {Learners} in {Population} based {Referential} {Games} through {Action} {Advising}},
	abstract = {The ability of agents to learn to communicate through interaction has been studied through emergent communication tasks. Previous works in this domain have studied the linguistic properties of the emergent languages like compositionality, generalization, and as well as the environmental pressures that shape them. However, most of these experiments require a considerable amount of shared training time between agents to communicate successfully. Our work highlights the problem of sample inefficiency of agents in population-based referential games and proposes an Action Advising framework to counter it.},
	language = {en},
	author = {Verma, Shresth},
	year = {2021},
	pages = {3},
	file = {Verma - 2021 - Towards Sample Efficient Learners in Population ba.pdf:/home/brendon/academic/misc/Zotero/storage/ZIJBZ4ZS/Verma - 2021 - Towards Sample Efficient Learners in Population ba.pdf:application/pdf},
}

@article{perkins_compositionality_2021,
	title = {Compositionality {Through} {Language} {Transmission}, using {Artificial} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2101.11739},
	abstract = {We propose an architecture and process for using the Iterated Learning Model ("ILM") for artificial neural networks. We show that ILM does not lead to the same clear compositionality as observed using DCGs, but does lead to a modest improvement in compositionality, as measured by holdout accuracy and topologic similarity. We show that ILM can lead to an anti-correlation between holdout accuracy and topologic rho. We demonstrate that ILM can increase compositionality when using non-symbolic high-dimensional images as input.},
	urldate = {2021-12-17},
	journal = {arXiv:2101.11739 [cs]},
	author = {Perkins, Hugh},
	month = apr,
	year = {2021},
	note = {arXiv: 2101.11739},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/4TF4N994/Perkins - 2021 - Compositionality Through Language Transmission, us.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/E2HIQI5W/2101.html:text/html},
}

@article{hazra_gcomm_2021,
	title = {{gComm}: {An} environment for investigating generalization in {Grounded} {Language} {Acquisition}},
	shorttitle = {{gComm}},
	url = {http://arxiv.org/abs/2105.03943},
	abstract = {gComm is a step towards developing a robust platform to foster research in grounded language acquisition in a more challenging and realistic setting. It comprises a 2-d grid environment with a set of agents (a stationary speaker and a mobile listener connected via a communication channel) exposed to a continuous array of tasks in a partially observable setting. The key to solving these tasks lies in agents developing linguistic abilities and utilizing them for efficiently exploring the environment. The speaker and listener have access to information provided in different modalities, i.e. the speaker's input is a natural language instruction that contains the target and task specifications and the listener's input is its grid-view. Each must rely on the other to complete the assigned task, however, the only way they can achieve the same, is to develop and use some form of communication. gComm provides several tools for studying different forms of communication and assessing their generalization.},
	urldate = {2021-12-17},
	journal = {arXiv:2105.03943 [cs]},
	author = {Hazra, Rishi and Dixit, Sonu},
	month = may,
	year = {2021},
	note = {arXiv: 2105.03943},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/IYWZH4YD/Hazra and Dixit - 2021 - gComm An environment for investigating generaliza.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/TQV8GHBW/2105.html:text/html},
}

@article{ohmer_why_2021,
	title = {Why and how to study the impact of perception on language emergence in artificial agents},
	volume = {43},
	url = {https://escholarship.org/uc/item/6p82v6st},
	abstract = {The study of emergent languages in deep multi-agent simulations has become an important research field. While targeting different objectives, most studies focus on analyzing properties of the emergent language—often in relation to the agents’ inputs—ignoring the influence of the agents’ perceptual processes. In this work, we use communication games to investigate how differences in perception affect emergent language. Using a conventional setup, we train two deep reinforcement learning agents, a sender and a receiver, on a reference game. However, we systematically manipulate the agents’ perception by enforcing similar representations for objects with specific shared features. We find that perceptual biases of both sender and receiver influence which object features the agents’ messages are grounded in. When uniformly enforcing the similarity of all features that are relevant for the reference game, agents perform better and the emergent protocol better captures conceptual input properties.},
	language = {en},
	number = {43},
	urldate = {2021-12-17},
	journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	author = {Ohmer, Xenia and Marino, Michael and Franke, Michael and König, Peter},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/CKVJHEAY/Ohmer et al. - 2021 - Why and how to study the impact of perception on l.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/9IFXLS9K/6p82v6st.html:text/html},
}

@misc{noauthor_toward_nodate,
	title = {Toward the {Emergence} of {Quantifiers} - {ProQuest}},
	url = {https://www.proquest.com/docview/2566057125?pq-origsite=gscholar&fromopenview=true},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
	language = {en},
	urldate = {2021-12-17},
	keywords = {important},
	file = {Snapshot:/home/brendon/academic/misc/Zotero/storage/PEYJ5FL5/2566057125.html:text/html;Toward the Emergence of Quantifiers - ProQuest.pdf:/home/brendon/academic/misc/Zotero/storage/2USTIUYZ/Toward the Emergence of Quantifiers - ProQuest.pdf:application/pdf},
}

@article{kempe_amount_2021,
	title = {Amount of {Learning} and {Signal} {Stability} {Modulate} {Emergence} of {Structure} and {Iconicity} in {Novel} {Signaling} {Systems}},
	volume = {45},
	issn = {1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13057},
	doi = {10.1111/cogs.13057},
	abstract = {Iterated language learning experiments that explore the emergence of linguistic structure in the laboratory vary considerably in methodological implementation, limiting the generalizability of findings. Most studies also restrict themselves to exploring the emergence of combinatorial and compositional structure in isolation. Here, we use a novel signal space comprising binary auditory and visual sequences and manipulate the amount of learning and temporal stability of these signals. Participants had to learn signals for meanings differing in size, shape, and brightness; their productions in the test phase were transmitted to the next participant. Across transmission chains of 10 generations each, Experiment 1 varied how much learning of auditory signals took place, and Experiment 2 varied temporal stability of visual signals. We found that combinatorial structure emerged only for auditory signals, and iconicity emerged when the amount of learning was reduced, as an opportunity for rote-memorization hampers the exploration of the iconic affordances of the signal space. In addition, compositionality followed an inverted u-shaped trajectory raising across several generations before declining again toward the end of the transmission chains. This suggests that detection of systematic form-meaning linkages requires stable combinatorial units that can guide learners toward the structural properties of signals, but these combinatorial units had not yet emerged in these unfamiliar systems. Our findings underscore the importance of systematically manipulating training conditions and signal characteristics in iterated language learning experiments to study the interactions between the emergence of iconicity, combinatorial and compositional structure in novel signaling systems.},
	language = {en},
	number = {11},
	urldate = {2021-12-17},
	journal = {Cognitive Science},
	author = {Kempe, Vera and Gauvrit, Nicolas and Panayotov, Nikolay and Cunningham, Sheila and Tamariz, Monica},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13057},
	keywords = {Combinatoriality, Compositionality, Iconicity, Iterated language learning},
	pages = {e13057},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/M82VT5BD/Kempe et al. - 2021 - Amount of Learning and Signal Stability Modulate E.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/T5W7FVQ9/cogs.html:text/html},
}

@article{cheng_toward_nodate,
	title = {Toward {Modeling} the {Emergence} of {Symbolic} {Communication}},
	abstract = {We quantitatively study the emergence of symbolic communication in humans with a communication game that attempts to recapitulate an essential step in the development of human language: the emergence of shared abstract symbols in order to accomplish complex tasks. A teacher must communicate an abstract notion, a formula in ﬁrst order logic rendered to them in natural language, to a student. Subjects do so through a narrow channel that deprives them of common shared symbols: they cannot see or speak to one another and must only communicate via the motions of cars in a computer game. We observe that subjects spontaneously develop a shared symbolic vocabulary of car motions for task-speciﬁc concepts, such as “square” and “forall”, as well as for task-agnostic concepts such as “I’m confused”. Today, no agent can take part in this task, even though recognizing intentional motions and using those motions effectively are core competencies that we expect of social robots. As we scale up this task, we hope to shed light on how symbols are learned by humans and open up a rich new world of tasks for robots.},
	language = {en},
	author = {Cheng, Emily and Kuo, Yen-Ling and Cases, Ignacio and Katz, Boris and Barbu, Andrei},
	pages = {5},
	file = {Cheng et al. - Toward Modeling the Emergence of Symbolic Communic.pdf:/home/brendon/academic/misc/Zotero/storage/L7YF5GMJ/Cheng et al. - Toward Modeling the Emergence of Symbolic Communic.pdf:application/pdf},
}

@article{kucinski_catalytic_2021,
	title = {Catalytic {Role} {Of} {Noise} {And} {Necessity} {Of} {Inductive} {Biases} {In} {The} {Emergence} {Of} {Compositional} {Communication}},
	url = {http://arxiv.org/abs/2111.06464},
	abstract = {Communication is compositional if complex signals can be represented as a combination of simpler subparts. In this paper, we theoretically show that inductive biases on both the training framework and the data are needed to develop a compositional communication. Moreover, we prove that compositionality spontaneously arises in the signaling games, where agents communicate over a noisy channel. We experimentally confirm that a range of noise levels, which depends on the model and the data, indeed promotes compositionality. Finally, we provide a comprehensive study of this dependence and report results in terms of recently studied compositionality metrics: topographical similarity, conflict count, and context independence.},
	urldate = {2021-12-17},
	journal = {arXiv:2111.06464 [cs]},
	author = {Kuciński, Łukasz and Korbak, Tomasz and Kołodziej, Paweł and Miłoś, Piotr},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.06464},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/S29Q5L9A/Kuciński et al. - 2021 - Catalytic Role Of Noise And Necessity Of Inductive.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/IKGWF4Q2/2111.html:text/html},
}

@misc{barrett_language_2021,
	type = {Preprint},
	title = {Language {Games} and the {Emergence} of {Discourse}},
	url = {http://philsci-archive.pitt.edu/19417/},
	abstract = {Ludwig Wittgenstein (1958) used the notion of a language game to illustrate how language is interwoven with action. Here we consider how successful linguistic discourse of the sort he described might emerge in the context of a self-assembling evolutionary game. More specifically, we consider how discourse and coordinated action might self-assemble in the context of two generalized signaling games. The first game shows how prospective language users might learn to initiate meaningful discourse. The second shows how more subtle varieties of discourse might co-emerge with a meaningful language.},
	language = {en},
	urldate = {2021-12-17},
	author = {Barrett, Jeffrey A. and VanDrunen, Jacob},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/W9DQN8FA/Barrett and VanDrunen - 2021 - Language Games and the Emergence of Discourse.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/J3TQHBXH/19417.html:text/html},
}

@inproceedings{maruyama_learning_2021,
	title = {Learning, {Development}, and {Emergence} of {Compositionality} in {Natural} {Language} {Processing}},
	doi = {10.1109/ICDL49984.2021.9515636},
	abstract = {There are two paradigms in language processing, as characterised by symbolic compositional and statistical distributional modelling, which may be regarded as based upon the principles of compositionality (or symbolic recursion) and of contextuality (or the distributional hypothesis), respectively. Starting with philosophy of language as in Frege and Wittgenstein, we elucidate the nature of language and language processing from interdisciplinary perspectives across different fields of science. At the same time, we shed new light on conceptual issues in language processing on the basis of recent advances in Transformer-based models such as BERT and GPT-3. We link linguistic cognition with mathematical cognition through these discussions, explicating symbol grounding/emergence problems shared by both of them. We also discuss whether animal cognition can develop recursive compositional information processing.},
	booktitle = {2021 {IEEE} {International} {Conference} on {Development} and {Learning} ({ICDL})},
	author = {Maruyama, Yoshihiro},
	month = aug,
	year = {2021},
	keywords = {Linguistics, Animals, Information processing, Bit error rate, Cognition, compositionality, Conferences, contextuality, natural language processing, Natural language processing, philosophy of language, statistical distributional model of language, symbolic compositional model of language},
	pages = {1--7},
	file = {IEEE Xplore Full Text PDF:/home/brendon/academic/misc/Zotero/storage/XBHFRBG3/Maruyama - 2021 - Learning, Development, and Emergence of Compositio.pdf:application/pdf},
}

@article{lieck_discretisation_nodate,
	title = {Discretisation and {Continuity}: {Simulating} the {Emergence} of {Symbols} in {Communication} {Games}},
	abstract = {Signalling systems of various species (humans and non-human animals) as well as our world both exhibit discrete and continuous properties. However, continuous meanings are not always expressed using continuous forms but instead frequently categorised into discrete symbols. While discrete symbols are ubiquitous in communication, the emergence of discretisation from a continuous form space is not well understood. We investigate the emergence of discrete symbols by simulating the learning process of two agents that acquire a shared signalling system. The task is formalised as a reinforcement learning problem in continuous form and meaning space. We identify two central causes for the emergence of discretisation: 1) suboptimal signalling conventions and 2) a topological mismatch between form and meaning space. A long version of this paper has been accepted for publication in Cognition (International Journal of Cognitive Science).},
	language = {en},
	author = {Lieck, Robert and Wall, Leona and Rohrmeier, Martin},
	pages = {6},
	file = {Lieck et al. - Discretisation and Continuity Simulating the Emer.pdf:/home/brendon/academic/misc/Zotero/storage/72UBYQMY/Lieck et al. - Discretisation and Continuity Simulating the Emer.pdf:application/pdf},
}

@article{lee_emergent_2018,
	title = {Emergent {Translation} in {Multi}-{Agent} {Communication}},
	abstract = {This work proposes a communication game where two agents, native speakers of their own respective languages, jointly learn to solve a visual referential task and finds that the ability to understand and translate a foreign language emerges as a means to achieve shared goals. While most machine translation systems to date are trained on large parallel corpora, humans learn language in a different way: by being grounded in an environment and interacting with other humans. In this work, we propose a communication game where two agents, native speakers of their own respective languages, jointly learn to solve a visual referential task. We find that the ability to understand and translate a foreign language emerges as a means to achieve shared goals. The emergent translation is interactive and multimodal, and crucially does not require parallel corpora, but only monolingual, independent text and corresponding images. Our proposed translation model achieves this by grounding the source and target languages into a shared visual modality, and outperforms several baselines on both word-level and sentence-level translation tasks. Furthermore, we show that agents in a multilingual community learn to translate better and faster than in a bilingual communication setting.},
	journal = {ICLR},
	author = {Lee, Jason and Cho, Kyunghyun and Weston, J. and Kiela, Douwe},
	year = {2018},
	keywords = {ur},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/P8BDK6WQ/Lee et al. - 2018 - Emergent Translation in Multi-Agent Communication.pdf:application/pdf},
}

@article{ikram_hexajungle_nodate,
	title = {{HexaJungle}: a {MARL} {Simulator} to {Study} the {Emergence} of {Language}},
	abstract = {Multi-agent reinforcement learning in mixed-motive settings allows for the study of complex dynamics of agent interactions. Embodied agents in partially observable environments with the ability to communicate can share information, agree on strategies, or even lie to each other. In order to study this, we propose a simple environment where we can impose varying levels of cooperation, communication and competition as prerequisites to reach an optimal outcome. Welcome to the jungle.},
	language = {en},
	author = {Ikram, Kiran and Mondragón, Esther and Alonso, Eduardo and Ortiz, Michael Garcia},
	pages = {3},
	file = {Ikram et al. - HexaJungle a MARL Simulator to Study the Emergenc.pdf:/home/brendon/academic/misc/Zotero/storage/7U2MGDX2/Ikram et al. - HexaJungle a MARL Simulator to Study the Emergenc.pdf:application/pdf},
}

@article{cao_emergent_2018,
	title = {Emergent {Communication} through {Negotiation}},
	abstract = {This paper study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction, introduces two communication protocols -- one grounded in the semantics of the game, and one which is a priori ungrounded and is a form of cheap talk. Multi-agent reinforcement learning offers a way to study how communication could emerge in communities of agents needing to solve specific problems. In this paper, we study the emergence of communication in the negotiation environment, a semi-cooperative model of agent interaction. We introduce two communication protocols -- one grounded in the semantics of the game, and one which is {\textbackslash}textit\{a priori\} ungrounded and is a form of cheap talk. We show that self-interested agents can use the pre-grounded communication channel to negotiate fairly, but are unable to effectively use the ungrounded channel. However, prosocial agents do learn to use cheap talk to find an optimal negotiating strategy, suggesting that cooperation is necessary for language to emerge. We also study communication behaviour in a setting where one agent interacts with agents in a community with different levels of prosociality and show how agent identifiability can aid negotiation.},
	journal = {ICLR},
	author = {Cao, Kris and Lazaridou, Angeliki and Lanctot, Marc and Leibo, Joel Z. and Tuyls, K. and Clark, S.},
	year = {2018},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/HBP2AC6B/Cao et al. - 2018 - Emergent Communication through Negotiation.pdf:application/pdf},
}

@article{dietterich_editorial_1990,
	title = {Editorial {Exploratory} research in machine learning},
	volume = {5},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00115892},
	doi = {10.1007/BF00115892},
	language = {en},
	number = {1},
	urldate = {2021-12-23},
	journal = {Machine Learning},
	author = {Dietterich, Thomas G.},
	month = mar,
	year = {1990},
	pages = {5--9},
	file = {Dietterich - 1990 - Editorial Exploratory research in machine learning.pdf:/home/brendon/academic/misc/Zotero/storage/YNYF42H5/Dietterich - 1990 - Editorial Exploratory research in machine learning.pdf:application/pdf;Full Text:/home/brendon/academic/misc/Zotero/storage/3LTSVGNJ/Dietterich - 2004 - Editorial Exploratory research in machine learning.pdf:application/pdf},
}

@article{andreas_measuring_2019,
	title = {Measuring {Compositionality} in {Representation} {Learning}},
	url = {http://arxiv.org/abs/1902.07181},
	abstract = {Many machine learning algorithms represent input data with vector embeddings or discrete codes. When inputs exhibit compositional structure (e.g. objects built from parts or procedures from subroutines), it is natural to ask whether this compositional structure is reflected in the the inputs' learned representations. While the assessment of compositionality in languages has received significant attention in linguistics and adjacent fields, the machine learning literature lacks general-purpose tools for producing graded measurements of compositional structure in more general (e.g. vector-valued) representation spaces. We describe a procedure for evaluating compositionality by measuring how well the true representation-producing model can be approximated by a model that explicitly composes a collection of inferred representational primitives. We use the procedure to provide formal and empirical characterizations of compositional structure in a variety of settings, exploring the relationship between compositionality and learning dynamics, human judgments, representational similarity, and generalization.},
	urldate = {2021-12-23},
	journal = {arXiv:1902.07181 [cs, stat]},
	author = {Andreas, Jacob},
	month = apr,
	year = {2019},
	note = {arXiv: 1902.07181},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/2KHXETYI/Andreas - 2019 - Measuring Compositionality in Representation Learn.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/AIY9DBY2/1902.html:text/html},
}

@article{liu_benchmarking_2019,
	title = {Benchmarking {Natural} {Language} {Understanding} {Services} for building {Conversational} {Agents}},
	url = {http://arxiv.org/abs/1903.05566},
	abstract = {We have recently seen the emergence of several publicly available Natural Language Understanding (NLU) toolkits, which map user utterances to structured, but more abstract, Dialogue Act (DA) or Intent specifications, while making this process accessible to the lay developer. In this paper, we present the first wide coverage evaluation and comparison of some of the most popular NLU services, on a large, multi-domain (21 domains) dataset of 25K user utterances that we have collected and annotated with Intent and Entity Type specifications and which will be released as part of this submission. The results show that on Intent classification Watson significantly outperforms the other platforms, namely, Dialogflow, LUIS and Rasa; though these also perform well. Interestingly, on Entity Type recognition, Watson performs significantly worse due to its low Precision. Again, Dialogflow, LUIS and Rasa perform well on this task.},
	urldate = {2021-12-23},
	journal = {arXiv:1903.05566 [cs]},
	author = {Liu, Xingkun and Eshghi, Arash and Swietojanski, Pawel and Rieser, Verena},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05566},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/PL7TYLLL/Liu et al. - 2019 - Benchmarking Natural Language Understanding Servic.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/739TCNPA/1903.html:text/html},
}

@incollection{hepburn_scientific_2021,
	edition = {Summer 2021},
	title = {Scientific {Method}},
	url = {https://plato.stanford.edu/archives/sum2021/entries/scientific-method/},
	abstract = {Science is an enormously successful human enterprise. The study ofscientific method is the attempt to discern the activities by whichthat success is achieved. Among the activities often identified ascharacteristic of science are systematic observation andexperimentation, inductive and deductive reasoning, and the formationand testing of hypotheses and theories. How these are carried out indetail can vary greatly, but characteristics like these have beenlooked to as a way of demarcating scientific activity fromnon-science, where only enterprises which employ some canonical formof scientific method or methods should be considered science (see alsothe entry on science and pseudo-science). Others have questioned whether there is anything like a fixed toolkitof methods which is common across science and only science. Somereject privileging one view of method as part of rejecting broaderviews about the nature of science, such as naturalism (Dupré2004); some reject any restriction in principle (pluralism)., Scientific method should be distinguished from the aims and productsof science, such as knowledge, predictions, or control. Methods arethe means by which those goals are achieved. Scientific method shouldalso be distinguished from meta-methodology, which includes the valuesand justifications behind a particular characterization of scientificmethod (i.e., a methodology) — values such as objectivity,reproducibility, simplicity, or past successes. Methodological rulesare proposed to govern method and it is a meta-methodological questionwhether methods obeying those rules satisfy given values. Finally,method is distinct, to some degree, from the detailed and contextualpractices through which methods are implemented. The latter mightrange over: specific laboratory techniques; mathematical formalisms orother specialized languages used in descriptions and reasoning;technological or other material means; ways of communicating andsharing results, whether with other scientists or with the public atlarge; or the conventions, habits, enforced customs, and institutionalcontrols over how and what science is carried out., While it is important to recognize these distinctions, theirboundaries are fuzzy. Hence, accounts of method cannot be entirelydivorced from their methodological and meta-methodological motivationsor justifications, Moreover, each aspect plays a crucial role inidentifying methods. Disputes about method have therefore played outat the detail, rule, and meta-rule levels. Changes in beliefs aboutthe certainty or fallibility of scientific knowledge, for instance(which is a meta-methodological consideration of what we can hope formethods to deliver), have meant different emphases on deductive andinductive reasoning, or on the relative importance attached toreasoning over observation (i.e., differences over particularmethods.) Beliefs about the role of science in society will affect theplace one gives to values in scientific method., The issue which has shaped debates over scientific method the most inthe last half century is the question of how pluralist do we need tobe about method? Unificationists continue to hold out for one methodessential to science; nihilism is a form of radical pluralism, whichconsiders the effectiveness of any methodological prescription to beso context sensitive as to render it not explanatory on its own. Somemiddle degree of pluralism regarding the methods embodied inscientific practice seems appropriate. But the details of scientificpractice vary with time and place, from institution to institution,across scientists and their subjects of investigation. How significantare the variations for understanding science and its success? How muchcan method be abstracted from practice? This entry describes some ofthe attempts to characterize scientific method or methods, as well asarguments for a more context-sensitive approach to methods embedded inactual scientific practices.},
	urldate = {2022-01-03},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Hepburn, Brian and Andersen, Hanne},
	editor = {Zalta, Edward N.},
	year = {2021},
	keywords = {al-Kindi, Albert the Great [= Albertus magnus], Aquinas, Saint Thomas, Arabic and Islamic Philosophy, disciplines in: natural philosophy and natural science, Arabic and Islamic Philosophy, historical and methodological topics in: Greek sources, Arabic and Islamic Philosophy, historical and methodological topics in: influence of Arabic and Islamic Philosophy on the Latin West, Aristotle, Bacon, Francis, Bacon, Roger, Berkeley, George, biology: experiment in, Boyle, Robert, Cambridge Platonists, confirmation, Descartes, René, Enlightenment, epistemology, epistemology: Bayesian, epistemology: social, Feyerabend, Paul, Galileo Galilei, Grosseteste, Robert, Hempel, Carl, Hume, David, Hume, David: Newtonianism and Anti-Newtonianism, induction: problem of, Kant, Immanuel, Kuhn, Thomas, Leibniz, Gottfried Wilhelm, Locke, John, Mill, John Stuart, More, Henry, Neurath, Otto, Newton, Isaac, Newton, Isaac: philosophy, Ockham [Occam], William, operationalism, Peirce, Charles Sanders, Plato, Popper, Karl, rationality: historicist theories of, Reichenbach, Hans, reproducibility, scientific, Schlick, Moritz, science: and pseudo-science, science: theory and observation in, science: unity of, scientific discovery, scientific knowledge: social dimensions of, simulations in science, skepticism: medieval, space and time: absolute and relational space and motion, post-Newtonian theories, Vienna Circle, Whewell, William, Zabarella, Giacomo},
	file = {SEP - Snapshot:/home/brendon/academic/misc/Zotero/storage/8YV9PY7X/scientific-method.html:text/html},
}

@inproceedings{kharitonov_egg_2019,
	address = {Hong Kong, China},
	title = {{EGG}: a toolkit for research on {Emergence} of {lanGuage} in {Games}},
	shorttitle = {{EGG}},
	url = {https://aclanthology.org/D19-3010},
	doi = {10.18653/v1/D19-3010},
	abstract = {There is renewed interest in simulating language emergence among deep neural agents that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the evolution of human language. However, optimizing deep architectures connected by a discrete communication channel (such as that in which language emerges) is technically challenging. We introduce EGG, a toolkit that greatly simplifies the implementation of emergent-language communication games. EGG's modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area.},
	urldate = {2022-01-04},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP}): {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Kharitonov, Eugene and Chaabouni, Rahma and Bouchacourt, Diane and Baroni, Marco},
	month = nov,
	year = {2019},
	pages = {55--60},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/3369HZPN/Kharitonov et al. - 2019 - EGG a toolkit for research on Emergence of lanGua.pdf:application/pdf},
}

@article{denamganai_referentialgym_2020,
	title = {{ReferentialGym}: {A} {Nomenclature} and {Framework} for {Language} {Emergence} \& {Grounding} in ({Visual}) {Referential} {Games}},
	shorttitle = {{ReferentialGym}},
	abstract = {A nomenclature is proposed to understand the main initiatives in studying language emergence and grounding, accounting for the variations in assumptions and constraints, and a PyTorch based deep learning framework is introduced, entitled ReferentialGym, which is dedicated to furthering the exploration of language emerged and grounding. Natural languages are powerful tools wielded by human beings to communicate information and co-operate towards common goals. Their values lie in some main properties like compositionality, hierarchy and recurrent syntax, which computational linguists have been researching the emergence of in artificial languages induced by language games. Only relatively recently, the AI community has started to investigate language emergence and grounding working towards better human-machine interfaces. For instance, interactive/conversational AI assistants that are able to relate their vision to the ongoing conversation. 
This paper provides two contributions to this research field. Firstly, a nomenclature is proposed to understand the main initiatives in studying language emergence and grounding, accounting for the variations in assumptions and constraints. Secondly, a PyTorch based deep learning framework is introduced, entitled ReferentialGym, which is dedicated to furthering the exploration of language emergence and grounding. By providing baseline implementations of major algorithms and metrics, in addition to many different features and approaches, ReferentialGym attempts to ease the entry barrier to the field and provide the community with common implementations.},
	journal = {ArXiv},
	author = {Denamganai, Kevin and Walker, James Alfred},
	year = {2020},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/R5PSW2YS/Denamganai and Walker - 2020 - ReferentialGym A Nomenclature and Framework for L.pdf:application/pdf},
}

@article{slowik_towards_2020,
	title = {Towards {Graph} {Representation} {Learning} in {Emergent} {Communication}},
	abstract = {This paper uses graph convolutional networks to support the evolution of language and cooperation in multi-agent systems and proposes a graph referential game with varying degrees of complexity, providing strong baseline models that exhibit desirable properties in terms of language emergence and cooperation. Recent findings in neuroscience suggest that the human brain represents information in a geometric structure (for instance, through conceptual spaces). In order to communicate, we flatten the complex representation of entities and their attributes into a single word or a sentence. In this paper we use graph convolutional networks to support the evolution of language and cooperation in multi-agent systems. Motivated by an image-based referential game, we propose a graph referential game with varying degrees of complexity, and we provide strong baseline models that exhibit desirable properties in terms of language emergence and cooperation. We show that the emerged communication protocol is robust, that the agents uncover the true factors of variation in the game, and that they learn to generalize beyond the samples encountered during training.},
	journal = {ArXiv},
	author = {Słowik, Agnieszka and Gupta, Abhinav and Hamilton, William L. and Jamnik, M. and Holden, S.},
	year = {2020},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/46S7ZS5Y/Słowik et al. - 2020 - Towards Graph Representation Learning in Emergent .pdf:application/pdf},
}

@misc{gupta_analyzing_2020,
	title = {Analyzing structural priors in multi-agent communication},
	url = {https://www.semanticscholar.org/paper/Analyzing-structural-priors-in-multi-agent-Gupta-Slowik/91a6a59b36846f7f27441cb23651e458e12e1be1},
	abstract = {Human language and thought are characterized by the ability to systematically generate a potentially infinite number of complex structures (e.g., sentences) from a finite set of familiar components (e.g., words). Recent works in emergent communication have discussed the propensity of artificial agents to develop a systematically compositional language through playing co-operative referential games. The degree of structure in the input data was found to affect the compositionality of the emerged communication protocols. Thus, we explore various structural priors in multi-agent communication and propose a novel graph referential game. We compare the effect of structural inductive bias (bag-of-words, sequences and graphs) on the emergence of compositional understanding of the input concepts measured by topographic similarity and generalization to unseen combinations of familiar properties. We empirically show that graph neural networks induce a better compositional language prior and a stronger generalization to out-of-domain data. We further perform ablation studies that show the robustness of the emerged protocol in graph referential games.},
	language = {en},
	urldate = {2022-01-04},
	author = {Gupta, A. and Slowik, A. and Hamilton, William L. and Jamnik, M. and Holden, S. and Pal, C.},
	year = {2020},
	file = {Snapshot:/home/brendon/academic/misc/Zotero/storage/ZSLYEYJ9/91a6a59b36846f7f27441cb23651e458e12e1be1.html:text/html},
}

@article{gupta_analyzing_nodate,
	title = {Analyzing structural priors in multi-agent communication},
	abstract = {Human language and thought are characterized by the ability to systematically generate a potentially infinite number of complex structures (e.g., sentences) from a finite set of familiar components (e.g., words). Recent works in emergent communication have discussed the propensity of artificial agents to develop a systematically compositional language through playing co-operative referential games. The degree of structure in the input data was found to affect the compositionality of the emerged communication protocols. Thus, we explore various structural priors in multi-agent communication and propose a novel graph referential game. We compare the effect of structural inductive bias (bag-of-words, sequences and graphs) on the emergence of compositional understanding of the input concepts measured by topographic similarity and generalization to unseen combinations of familiar properties. We empirically show that graph neural networks induce a better compositional language prior and a stronger generalization to out-of-domain data. We further perform ablation studies that show the robustness of the emerged protocol in graph referential games.},
	language = {en},
	author = {Gupta, Abhinav and Słowik, Agnieszka and Hamilton, William L and Jamnik, Mateja and Holden, Sean B and Pal, Christopher},
	pages = {7},
	file = {Gupta et al. - Analyzing structural priors in multi-agent communi.pdf:/home/brendon/academic/misc/Zotero/storage/8V5XJ2GP/Gupta et al. - Analyzing structural priors in multi-agent communi.pdf:application/pdf},
}

@article{slowik_exploring_2020,
	title = {Exploring {Structural} {Inductive} {Biases} in {Emergent} {Communication}},
	url = {https://www.semanticscholar.org/paper/Exploring-Structural-Inductive-Biases-in-Emergent-S%C5%82owik-Gupta/29d1adb458d5b5a0fc837d37af01a6673efd531c},
	abstract = {This work explores various structural priors in multi-agent communication and proposes a novel graph referential game that empirically shows that graph neural networks induce a better compositional language prior and a stronger generalization to out-of-domain data. Human language and thought are characterized by the ability to systematically generate a potentially infinite number of complex structures (e.g., sentences) from a finite set of familiar components (e.g., words). Recent works in emergent communication have discussed the propensity of artificial agents to develop a systematically compositional language through playing co-operative referential games. The degree of structure in the input data was found to affect the compositionality of the emerged communication protocols. Thus, we explore various structural priors in multi-agent communication and propose a novel graph referential game. We compare the effect of structural inductive bias (bag-of-words, sequences and graphs) on the emergence of compositional understanding of the input concepts measured by topographic similarity and generalization to unseen combinations of familiar properties. We empirically show that graph neural networks induce a better compositional language prior and a stronger generalization to out-of-domain data. We further perform ablation studies that show the robustness of the emerged protocol in graph referential games.},
	language = {en},
	urldate = {2022-01-04},
	journal = {undefined},
	author = {Słowik, Agnieszka and Gupta, Abhinav and Hamilton, William L. and Jamnik, M. and Holden, S. and Pal, C.},
	year = {2020},
	file = {Snapshot:/home/brendon/academic/misc/Zotero/storage/TX8CD8CF/29d1adb458d5b5a0fc837d37af01a6673efd531c.html:text/html},
}

@article{slowik_structural_2021,
	title = {Structural {Inductive} {Biases} in {Emergent} {Communication}},
	url = {http://arxiv.org/abs/2002.01335},
	abstract = {In order to communicate, humans ﬂatten a complex representation of ideas and their attributes into a single word or a sentence. We investigate the impact of representation learning in artiﬁcial agents by developing graph referential games. We empirically show that agents parametrized by graph neural networks develop a more compositional language compared to bag-of-words and sequence models, which allows them to systematically generalize to new combinations of familiar features.},
	language = {en},
	urldate = {2022-01-04},
	journal = {arXiv:2002.01335 [cs, stat]},
	author = {Słowik, Agnieszka and Gupta, Abhinav and Hamilton, William L. and Jamnik, Mateja and Holden, Sean B. and Pal, Christopher},
	month = jul,
	year = {2021},
	note = {arXiv: 2002.01335},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Multiagent Systems},
	file = {Słowik et al. - 2021 - Structural Inductive Biases in Emergent Communicat.pdf:/home/brendon/academic/misc/Zotero/storage/8KE7VPJS/Słowik et al. - 2021 - Structural Inductive Biases in Emergent Communicat.pdf:application/pdf},
}

@article{dessi_focus_2019,
	title = {Focus on {What}'s {Informative} and {Ignore} {What}'s not: {Communication} {Strategies} in a {Referential} {Game}},
	shorttitle = {Focus on {What}'s {Informative} and {Ignore} {What}'s not},
	abstract = {This work analyzes the object-symbol mapping in a simple referential game with two neural network-based agents to see that, when the environment is uniformly distributed, the agents rely on a random subset of features to describe the objects, but when one feature is non-uniformly distributed, they make a better use of the remaining features. Research in multi-agent cooperation has shown that artificial agents are able to learn to play a simple referential game while developing a shared lexicon. This lexicon is not easy to analyze, as it does not show many properties of a natural language. In a simple referential game with two neural network-based agents, we analyze the object-symbol mapping trying to understand what kind of strategy was used to develop the emergent language. We see that, when the environment is uniformly distributed, the agents rely on a random subset of features to describe the objects. When we modify the objects making one feature non-uniformly distributed,the agents realize it is less informative and start to ignore it, and, surprisingly, they make a better use of the remaining features. This interesting result suggests that more natural, less uniformly distributed environments might aid in spurring the emergence of better-behaved languages.},
	journal = {ArXiv},
	author = {Dessì, Roberto and Bouchacourt, Diane and Crepaldi, D. and Baroni, Marco},
	year = {2019},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/J26KZX2J/Dessì et al. - 2019 - Focus on What's Informative and Ignore What's not.pdf:application/pdf},
}

@article{korbak_developmentally_2019,
	title = {Developmentally motivated emergence of compositional communication via template transfer},
	abstract = {A training regime implementing template transfer, the idea of carrying over learned biases across contexts, is proposed, which supports a conjecture that compositional communication is scaffolded on simpler communication protocols. This paper explores a novel approach to achieving emergent compositional communication in multi-agent systems. We propose a training regime implementing template transfer, the idea of carrying over learned biases across contexts. In our method, a sender-receiver pair is first trained with disentangled loss functions and then the receiver is transferred to train a new sender with a standard loss. Unlike other methods (e.g. the obverter algorithm), our approach does not require imposing inductive biases on the architecture of the agents. We experimentally show the emergence of compositional communication using topographical similarity, zero-shot generalization and context independence as evaluation metrics. The presented approach is connected to an important line of work in semiotics and developmental psycholinguistics: it supports a conjecture that compositional communication is scaffolded on simpler communication protocols.},
	journal = {ArXiv},
	author = {Korbak, Tomasz and Zubek, Julian and Kucinski, L. and Milos, Piotr and Rączaszek-Leonardi, J.},
	year = {2019},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/CVS7RR8F/Korbak et al. - 2019 - Developmentally motivated emergence of composition.pdf:application/pdf},
}

@article{dessi_interpretable_2021,
	title = {Interpretable agent communication from scratch(with a generic visual processor emerging on the side)},
	abstract = {This work trains two deep nets from scratch to perform realistic referent identification through unsupervised emergent communication, and shows that the largely interpretable emergent protocol allows the nets to successfully communicate even about object types they did not see at training time. As deep networks begin to be deployed as autonomous agents, the issue of how they can communicate with each other becomes important. Here, we train two deep nets from scratch to perform large-scale referent identification through unsupervised emergent communication. We show that the partially interpretable emergent protocol allows the nets to successfully communicate even about object classes they did not see at training time. The visual representations induced as a by-product of our training regime, moreover, when re-used as generic visual features, show comparable quality to a recent self-supervised learning model. Our results provide concrete evidence of the viability of (interpretable) emergent deep net communication in a more realistic scenario than previously considered, as well as establishing an intriguing link between this field and self-supervised visual learning. 1},
	journal = {ArXiv},
	author = {Dessì, Roberto and Kharitonov, E. and Baroni, Marco},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/SF9QJIBY/Dessì et al. - 2021 - Interpretable agent communication from scratch(wit.pdf:application/pdf},
}

@article{guo_inductive_2020,
	title = {Inductive {Bias} and {Language} {Expressivity} in {Emergent} {Communication}},
	abstract = {With empirical experiments on a handcrafted symbolic dataset, it is shown that languages emerged from different games have different compositionality and further different expressivity. Referential games and reconstruction games are the most common game types for studying emergent languages. We investigate how the type of the language game affects the emergent language in terms of: i) language compositionality and ii) transfer of an emergent language to a task different from its origin, which we refer to as language expressivity. With empirical experiments on a handcrafted symbolic dataset, we show that languages emerged from different games have different compositionality and further different expressivity.},
	journal = {ArXiv},
	author = {Guo, Shangmin and Ren, Yi and Slowik, A. and Mathewson, K.},
	year = {2020},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/MU4MUX5Q/Guo et al. - 2020 - Inductive Bias and Language Expressivity in Emerge.pdf:application/pdf},
}

@article{bunge_technology_1966,
	title = {Technology as {Applied} {Science}},
	volume = {7},
	issn = {0040-165X},
	url = {http://www.jstor.org/stable/3101932},
	doi = {10.2307/3101932},
	number = {3},
	urldate = {2022-01-11},
	journal = {Technology and Culture},
	author = {Bunge, Mario},
	year = {1966},
	note = {Publisher: [The Johns Hopkins University Press, Society for the History of Technology]},
	pages = {329--347},
	file = {Bunge - 1966 - Technology as Applied Science.pdf:/home/brendon/academic/misc/Zotero/storage/683DVWZN/Bunge - 1966 - Technology as Applied Science.pdf:application/pdf},
}

@inproceedings{bouchacourt_how_2018,
	address = {Brussels, Belgium},
	title = {How agents see things: {On} visual representations in an emergent language game},
	shorttitle = {How agents see things},
	url = {https://aclanthology.org/D18-1119},
	doi = {10.18653/v1/D18-1119},
	abstract = {There is growing interest in the language developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents' symbol usage, rather than on their representation of visual input. In this paper, we consider the referential games of Lazaridou et al. (2017), and investigate the representations the agents develop during their evolving interaction. We find that the agents establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we care about developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use.},
	urldate = {2022-01-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bouchacourt, Diane and Baroni, Marco},
	month = oct,
	year = {2018},
	pages = {981--985},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/4UGFI5J8/Bouchacourt and Baroni - 2018 - How agents see things On visual representations i.pdf:application/pdf},
}

@inproceedings{evtimova_emergent_2018,
	title = {Emergent {Communication} in a {Multi}-{Modal}, {Multi}-{Step} {Referential} {Game}},
	url = {https://openreview.net/forum?id=rJGZq6g0-},
	abstract = {Inspired by previous work on emergent communication in referential games, we propose a novel multi-modal, multi-step referential game, where the sender and receiver have access to distinct...},
	language = {en},
	urldate = {2022-01-14},
	author = {Evtimova, Katrina and Drozdov, Andrew and Kiela, Douwe and Cho, Kyunghyun},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/WYJTRWJF/Evtimova et al. - 2018 - Emergent Communication in a Multi-Modal, Multi-Ste.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/QJJ34YTK/forum.html:text/html},
}

@inproceedings{havrylov_emergence_2017-1,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Emergence of language with multi-agent games: learning to communicate with sequences of symbols},
	isbn = {978-1-5108-6096-4},
	shorttitle = {Emergence of language with multi-agent games},
	abstract = {Learning to communicate through interaction, rather than relying on explicit supervision, is often considered a prerequisite for developing a general AI. We study a setting where two agents engage in playing a referential game and, from scratch, develop a communication protocol necessary to succeed in this game. Unlike previous work, we require that messages they exchange, both at train and test time, are in the form of a language (i.e. sequences of discrete symbols). We compare a reinforcement learning approach and one using a differentiable relaxation (straight-through Gumbel-softmax estimator (Jang et al., 2017)) and observe that the latter is much faster to converge and it results in more effective protocols. Interestingly, we also observe that the protocol we induce by optimizing the communication success exhibits a degree of compositionality and variability (i.e. the same information can be phrased in different ways), both properties characteristic of natural languages. As the ultimate goal is to ensure that communication is accomplished in natural language, we also perform experiments where we inject prior information about natural language into our model and study properties of the resulting protocol.},
	urldate = {2022-01-14},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Havrylov, Serhii and Titov, Ivan},
	month = dec,
	year = {2017},
	pages = {2146--2156},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/5PUCEG4J/Havrylov and Titov - 2017 - Emergence of language with multi-agent games lear.pdf:application/pdf},
}

@inproceedings{kharitonov_emergent_2020-1,
	address = {Online},
	title = {Emergent {Language} {Generalization} and {Acquisition} {Speed} are not tied to {Compositionality}},
	url = {https://aclanthology.org/2020.blackboxnlp-1.2},
	doi = {10.18653/v1/2020.blackboxnlp-1.2},
	abstract = {Studies of discrete languages emerging when neural agents communicate to solve a joint task often look for evidence of compositional structure. This stems for the expectation that such a structure would allow languages to be acquired faster by the agents and enable them to generalize better. We argue that these beneficial properties are only loosely connected to compositionality. In two experiments, we demonstrate that, depending on the task, non-compositional languages might show equal, or better, generalization performance and acquisition speed than compositional ones. Further research in the area should be clearer about what benefits are expected from compositionality, and how the latter would lead to them.},
	urldate = {2022-01-14},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Kharitonov, Eugene and Baroni, Marco},
	month = nov,
	year = {2020},
	pages = {11--15},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/3WYVEUSM/Kharitonov and Baroni - 2020 - Emergent Language Generalization and Acquisition S.pdf:application/pdf},
}

@article{lazaridou_multi-agent_2016,
	title = {Multi-{Agent} {Cooperation} and the {Emergence} of ({Natural}) {Language}},
	url = {https://openreview.net/forum?id=Hk8N3Sclg},
	abstract = {The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are in- terested in developing interactive...},
	language = {en},
	urldate = {2022-01-14},
	journal = {International Conference on Learning Representations},
	author = {Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
	month = nov,
	year = {2016},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/MZMLEKVH/Lazaridou et al. - 2016 - Multi-Agent Cooperation and the Emergence of (Natu.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/2QMKTHT5/forum.html:text/html},
}

@inproceedings{li_emergent_2020,
	address = {Barcelona, Spain (Online)},
	title = {Emergent {Communication} {Pretraining} for {Few}-{Shot} {Machine} {Translation}},
	url = {https://aclanthology.org/2020.coling-main.416},
	doi = {10.18653/v1/2020.coling-main.416},
	abstract = {While state-of-the-art models that rely upon massively multilingual pretrained encoders achieve sample efficiency in downstream applications, they still require abundant amounts of unlabelled text. Nevertheless, most of the world's languages lack such resources. Hence, we investigate a more radical form of unsupervised knowledge transfer in the absence of linguistic data. In particular, for the first time we pretrain neural networks via emergent communication from referential games. Our key assumption is that grounding communication on images—as a crude approximation of real-world environments—inductively biases the model towards learning natural languages. On the one hand, we show that this substantially benefits machine translation in few-shot settings. On the other hand, this also provides an extrinsic evaluation protocol to probe the properties of emergent languages ex vitro. Intuitively, the closer they are to natural languages, the higher the gains from pretraining on them should be. For instance, in this work we measure the influence of communication success and maximum sequence length on downstream performances. Finally, we introduce a customised adapter layer and annealing strategies for the regulariser of maximum-a-posteriori inference during fine-tuning. These turn out to be crucial to facilitate knowledge transfer and prevent catastrophic forgetting. Compared to a recurrent baseline, our method yields gains of 59.0\% 147.6\% in BLEU score with only 500 NMT training instances and 65.1\% 196.7\% with 1,000 NMT training instances across four language pairs. These proof-of-concept results reveal the potential of emergent communication pretraining for both natural language processing tasks in resource-poor settings and extrinsic evaluation of artificial languages.},
	urldate = {2022-01-14},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Li, Yaoyiran and Ponti, Edoardo Maria and Vulić, Ivan and Korhonen, Anna},
	month = dec,
	year = {2020},
	keywords = {important},
	pages = {4716--4731},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/S4UETCGS/Li et al. - 2020 - Emergent Communication Pretraining for Few-Shot Ma.pdf:application/pdf},
}

@inproceedings{lowe_interaction_2019,
	title = {On the interaction between supervision and self-play in emergent communication},
	url = {https://openreview.net/forum?id=rJxGLlBtwH},
	abstract = {A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too...},
	language = {en},
	urldate = {2022-01-14},
	author = {Lowe*, Ryan and Gupta*, Abhinav and Foerster, Jakob and Kiela, Douwe and Pineau, Joelle},
	month = sep,
	year = {2019},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/Z6E7RQH6/Lowe et al. - 2019 - On the interaction between supervision and self-pl.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/9SJXBMTN/forum.html:text/html},
}

@incollection{winther_structure_2021,
	edition = {Spring 2021},
	title = {The {Structure} of {Scientific} {Theories}},
	url = {https://plato.stanford.edu/archives/spr2021/entries/structure-scientific-theories/},
	abstract = {Scientific inquiry has led to immense explanatory and technologicalsuccesses, partly as a result of the pervasiveness of scientifictheories. Relativity theory, evolutionary theory, and plate tectonicswere, and continue to be, wildly successful families of theorieswithin physics, biology, and geology. Other powerful theory clustersinhabit comparatively recent disciplines such as cognitive science,climate science, molecular biology, microeconomics, and GeographicInformation Science (GIS). Effective scientific theories magnifyunderstanding, help supply legitimate explanations, and assist informulating predictions. Moving from their knowledge-producingrepresentational functions to their interventional roles (Hacking1983), theories are integral to building technologies used withinconsumer, industrial, and scientific milieus., This entry explores the structure of scientific theories from theperspective of the Syntactic, Semantic, and Pragmatic Views. Each ofthese answers questions such as the following in unique ways. What isthe best characterization of the composition and function ofscientific theory? How is theory linked with world? Whichphilosophical tools can and should be employed in describing andreconstructing scientific theory? Is an understanding of practice andapplication necessary for a comprehension of the core structure of ascientific theory? Finally, and most generally, how are these threeviews ultimately related?},
	urldate = {2022-01-18},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Winther, Rasmus Grønfeldt},
	editor = {Zalta, Edward N.},
	year = {2021},
	keywords = {confirmation, Feyerabend, Paul, Kuhn, Thomas, rationality: historicist theories of, science: theory and observation in, simulations in science, Vienna Circle, Carnap, Rudolf, cognitive science, Darwinism, empiricism: logical, feminist philosophy, interventions: epistemology and philosophy of science, genetics: population, incommensurability: of scientific theories, model theory, models in science, paradox: Skolem’s, physics: structuralism in, pragmatism, reduction, scientific, scientific explanation, scientific realism, scientific representation, statistical physics: philosophy of statistical mechanics, structural realism, style: in mathematics, theoretical terms in science, underdetermination, of scientific theories},
	file = {SEP - Snapshot:/home/brendon/academic/misc/Zotero/storage/72GUZ4JF/structure-scientific-theories.html:text/html},
}

@article{hauser_mystery_2014,
	title = {The mystery of language evolution},
	volume = {5},
	issn = {1664-1078},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4019876/},
	doi = {10.3389/fpsyg.2014.00401},
	abstract = {Understanding the evolution of language requires evidence regarding origins and processes that led to change. In the last 40 years, there has been an explosion of research on this problem as well as a sense that considerable progress has been made. We argue instead that the richness of ideas is accompanied by a poverty of evidence, with essentially no explanation of how and why our linguistic computations and representations evolved. We show that, to date, (1) studies of nonhuman animals provide virtually no relevant parallels to human linguistic communication, and none to the underlying biological capacity; (2) the fossil and archaeological evidence does not inform our understanding of the computations and representations of our earliest ancestors, leaving details of origins and selective pressure unresolved; (3) our understanding of the genetics of language is so impoverished that there is little hope of connecting genes to linguistic processes any time soon; (4) all modeling attempts have made unfounded assumptions, and have provided no empirical tests, thus leaving any insights into language's origins unverifiable. Based on the current state of evidence, we submit that the most fundamental questions about the origins and evolution of our linguistic capacity remain as mysterious as ever, with considerable uncertainty about the discovery of either relevant or conclusive evidence that can adjudicate among the many open hypotheses. We conclude by presenting some suggestions about possible paths forward.},
	urldate = {2022-02-08},
	journal = {Frontiers in Psychology},
	author = {Hauser, Marc D. and Yang, Charles and Berwick, Robert C. and Tattersall, Ian and Ryan, Michael J. and Watumull, Jeffrey and Chomsky, Noam and Lewontin, Richard C.},
	month = may,
	year = {2014},
	pmid = {24847300},
	pmcid = {PMC4019876},
	pages = {401},
	file = {PubMed Central Full Text PDF:/home/brendon/academic/misc/Zotero/storage/5ZJE6TG8/Hauser et al. - 2014 - The mystery of language evolution.pdf:application/pdf},
}

@article{mihai_learning_2021,
	title = {Learning to {Draw}: {Emergent} {Communication} through {Sketching}},
	shorttitle = {Learning to {Draw}},
	abstract = {This work explores a visual communication channel between agents that are allowed to draw with simple strokes and demonstrates that agents can not only successfully learn to communicate by drawing, but with appropriate inductive biases, can do so in a fashion that humans can interpret. Evidence that visual communication preceded written language and provided a basis for it goes back to prehistory, in forms such as cave and rock paintings depicting traces of our distant ancestors. Emergent communication research has sought to explore how agents can learn to communicate in order to collaboratively solve tasks. Existing research has focused on language, with a learned communication channel transmitting sequences of discrete tokens between the agents. In this work, we explore a visual communication channel between agents that are allowed to draw with simple strokes. Our agents are parameterised by deep neural networks, and the drawing procedure is differentiable, allowing for end-to-end training. In the framework of a referential communication game, we demonstrate that agents can not only successfully learn to communicate by drawing, but with appropriate inductive biases, can do so in a fashion that humans can interpret. We hope to encourage future research to consider visual communication as a more flexible and directly interpretable alternative of training collaborative agents.},
	journal = {ArXiv},
	author = {Mihai, Daniela and Hare, Jonathon S.},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/I9QBP2FN/Mihai and Hare - 2021 - Learning to Draw Emergent Communication through S.pdf:application/pdf},
}

@article{karten_enforcers_2022,
	title = {The {Enforcers}: {Consistent} {Sparse}-{Discrete} {Methods} for {Constraining} {Informative} {Emergent} {Communication}},
	shorttitle = {The {Enforcers}},
	abstract = {This work successfully constrain training using a learned gate to regulate when to communicate while using discrete prototypes that reflect what to communicate for cooperative tasks with partial observability, and shows that the method satisfies constraints while yielding the same performance as comparable, unconstrained methods. Communication enables agents to cooperate to achieve their goals. Learning when to communicate, i.e. sparse communication, is particularly important where bandwidth is limited, in situations where agents interact with humans, in partially observable scenarios where agents must convey information unavailable to others, and in noncooperative scenarios where agents may hide information to gain a competitive advantage. Recent work in learning sparse communication, however, suffers from high variance training where, the price of decreasing communication is a decrease in reward, particularly in cooperative tasks. Sparse communications are necessary to match agent communication to limited human bandwidth. Humans additionally communicate via discrete linguistic tokens, previously shown to decrease task performance when compared to continuous communication vectors. This research addresses the above issues by limiting the loss in reward of decreasing communication and eliminating the penalty for discretization. In this work, we successfully constrain training using a learned gate to regulate when to communicate while using discrete prototypes that reflect what to communicate for cooperative tasks with partial observability. We provide two types of "Enforcers" for hard and soft budget constraints and present results of communication under different budgets. We show that our method satisfies constraints while yielding the same performance as comparable, unconstrained methods. 1},
	journal = {ArXiv},
	author = {Karten, Seth and Agrawal, Siddharth and Tucker, Mycal and Hughes, Dana and Lewis, Michael and Shah, J. and Sycara, K.},
	year = {2022},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/8NGKAVBS/Karten et al. - 2022 - The Enforcers Consistent Sparse-Discrete Methods .pdf:application/pdf},
}

@inproceedings{grupen_multi-agent_2021,
	title = {Multi-{Agent} {Curricula} and {Emergent} {Implicit} {Signaling}},
	abstract = {This work proposes a curriculum-driven strategy that combines: (i) velocity-based environment shaping, tailored to the skill level of the multi-agent team; and (ii) a behavioral curriculum that helps agents learn successful single-agent behaviors as a precursor to learning multi- agent behaviors. Emergent communication has made strides towards learning communication from scratch, but has focused primarily on protocols that resemble human language. In nature, multi-agent cooperation gives rise to a wide range of communication that varies in structure and complexity. In this work, we recognize the full spectrum of communication that exists in nature and propose studying lower-level communication. Specifically, we study emergent implicit signaling in the context of decentralized multi-agent learning in difficult, sparse reward environments. However, learning to coordinate in such environments is challenging. We propose a curriculum-driven strategy that combines: (i) velocity-based environment shaping, tailored to the skill level of the multi-agent team; and (ii) a behavioral curriculum that helps agents learn successful single-agent behaviors as a precursor to learning multi-agent behaviors. Pursuitevasion experiments show that our approach learns effective coordination, significantly outperforming sophisticated analytical and learned policies. Our method completes the pursuit-evasion task even when pursuers move at half of the evader’s speed, whereas the highest-performing baseline fails at 80\% of the evader’s speed. Moreover, we examine the use of implicit signals in coordination through position-based social influence. We show that pursuers trained with our strategy exchange more than twice as much information (in bits) than baseline methods, indicating that our method has learned, and relies heavily on, the exchange of implicit signals.},
	author = {Grupen, Niko A. and Lee, Daniel D. and Selman, B.},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/QL48R8K7/Grupen et al. - 2021 - Multi-Agent Curricula and Emergent Implicit Signal.pdf:application/pdf},
}

@article{ohmer_mutual_2021,
	title = {Mutual influence between language and perception in multi-agent communication games},
	abstract = {This paper explores how interactions at language interfaces with many other cognitive domains can be studied with deep learning methods, focusing on the relation between language emergence and visual perception, and points out ways to modulate and improve visual representation learning and emergent communication in artificial agents. Language interfaces with many other cognitive domains. This paper explores how interactions at these interfaces can be studied with deep learning methods, focusing on the relation between language emergence and visual perception. To model the emergence of language, a sender and a receiver agent are trained on a reference game. The agents are implemented as deep neural networks, with dedicated vision and language modules. Motivated by the mutual influence between language and perception in cognition, we apply systematic manipulations to the agents’ (i) visual representations, to analyze the effects on emergent communication, and (ii) communication protocols, to analyze the effects on visual representations. Our analyses show that perceptual biases shape semantic categorization and communicative content. Conversely, if the communication protocol partitions object space along certain attributes, agents learn to represent visual information about these attributes more accurately. Finally, an evolutionary analysis suggests that visual representations may have evolved in part to facilitate the communication of environmentally relevant distinctions. Aside from accounting for co-adaptation effects between language and perception, our results point out ways to modulate and improve visual representation learning and emergent communication in artificial agents.},
	journal = {ArXiv},
	author = {Ohmer, Xenia and Marino, Michael and Franke, M. and König, P.},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/HSFNSA86/Ohmer et al. - 2021 - Mutual influence between language and perception i.pdf:application/pdf},
}

@article{portelance_emergence_2021,
	title = {The {Emergence} of the {Shape} {Bias} {Results} from {Communicative} {Efficiency}},
	doi = {10.18653/v1/2021.conll-1.48},
	abstract = {It is proposed that communicative efficiency explains both how the shape bias emerged and why it persists across generations, and it is shown that pressure brought on by communicative need is also necessary for it to persist across generations. By the age of two, children tend to assume that new word categories are based on objects’ shape, rather than their color or texture; this assumption is called the shape bias. They are thought to learn this bias by observing that their caregiver’s language is biased towards shape based categories. This presents a chicken and egg problem: if the shape bias must be present in the language in order for children to learn it, how did it arise in language in the first place? In this paper, we propose that communicative efficiency explains both how the shape bias emerged and why it persists across generations. We model this process with neural emergent language agents that learn to communicate about raw pixelated images. First, we show that the shape bias emerges as a result of efficient communication strategies employed by agents. Second, we show that pressure brought on by communicative need is also necessary for it to persist across generations; simply having a shape bias in an agent’s input language is insufficient. These results suggest that, over and above the operation of other learning strategies, the shape bias in human learners may emerge and be sustained by communicative pressures.},
	journal = {CONLL},
	author = {Portelance, Eva and Frank, Michael C. and Jurafsky, Dan and Sordoni, Alessandro and Laroche, R.},
	year = {2021},
	keywords = {important},
	file = {Full Text:/home/brendon/academic/misc/Zotero/storage/BKVPM6U3/Portelance et al. - 2021 - The Emergence of the Shape Bias Results from Commu.pdf:application/pdf},
}

@article{eloff_towards_2021,
	title = {Towards {Learning} to {Speak} and {Hear} {Through} {Multi}-{Agent} {Communication} over a {Continuous} {Acoustic} {Channel}},
	abstract = {This work proposes an environment and training methodology to serve as a means to carry out an initial exploration of emergent language between agents with a continuous communication channel trained through reinforcement learning, and shows that basic compositionality emerges in the learned language representations. While multi-agent reinforcement learning has been used as an effective means to study emergent communication between agents, existing work has focused almost exclusively on communication with discrete symbols. Human communication often takes place (and emerged) over a continuous acoustic channel; human infants acquire language in large part through continuous signalling with their caregivers. We therefore ask: Are we able to observe emergent language between agents with a continuous communication channel trained through reinforcement learning? And if so, what is the impact of channel characteristics on the emerging language? We propose an environment and training methodology to serve as a means to carry out an initial exploration of these questions. We use a simple messaging environment where a “speaker” agent needs to convey a concept to a “listener”. The Speaker is equipped with a vocoder that maps symbols to a continuous waveform, this is passed over a lossy continuous channel, and the Listener needs to map the continuous signal to the concept. Using deep Q-learning, we show that basic compositionality emerges in the learned language representations. We find that noise is essential in the communication channel when conveying unseen concept combinations. And we show that we can ground the emergent communication by introducing a caregiver predisposed to “hearing” or “speaking” English. Finally, we describe how our platform serves as a starting point for future work that uses a combination of deep reinforcement learning and multi-agent systems to study our questions of continuous signalling in language learning and emergence.},
	journal = {ArXiv},
	author = {Eloff, Kevin and Pretorius, Arnu and Räsänen, O. and Engelbrecht, H. and Kamper, H.},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/TX6UQCL9/Eloff et al. - 2021 - Towards Learning to Speak and Hear Through Multi-A.pdf:application/pdf},
}

@article{taylor_socially_2021,
	title = {Socially {Supervised} {Representation} {Learning}: the {Role} of {Subjectivity} in {Learning} {Efficient} {Representations}},
	shorttitle = {Socially {Supervised} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2109.09390},
	abstract = {Despite its rise as a prominent solution to the data inefficiency of today's machine learning models, self-supervised learning has yet to be studied from a purely multi-agent perspective. In this work, we propose that aligning internal subjective representations, which naturally arise in a multi-agent setup where agents receive partial observations of the same underlying environmental state, can lead to more data-efficient representations. We propose that multi-agent environments, where agents do not have access to the observations of others but can communicate within a limited range, guarantees a common context that can be leveraged in individual representation learning. The reason is that subjective observations necessarily refer to the same subset of the underlying environmental states and that communication about these states can freely offer a supervised signal. To highlight the importance of communication, we refer to our setting as {\textbackslash}textit\{socially supervised representation learning\}. We present a minimal architecture comprised of a population of autoencoders, where we define loss functions, capturing different aspects of effective communication, and examine their effect on the learned representations. We show that our proposed architecture allows the emergence of aligned representations. The subjectivity introduced by presenting agents with distinct perspectives of the environment state contributes to learning abstract representations that outperform those learned by a single autoencoder and a population of autoencoders, presented with identical perspectives of the environment state. Altogether, our results demonstrate how communication from subjective perspectives can lead to the acquisition of more abstract representations in multi-agent systems, opening promising perspectives for future research at the intersection of representation learning and emergent communication.},
	urldate = {2022-03-17},
	journal = {arXiv:2109.09390 [cs]},
	author = {Taylor, Julius and Nisioti, Eleni and Moulin-Frier, Clément},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.09390
version: 1},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/WQXC7VYB/Taylor et al. - 2021 - Socially Supervised Representation Learning the R.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/XYHDQCAE/2109.html:text/html},
}

@inproceedings{patel_interpretation_2021,
	title = {Interpretation of {Emergent} {Communication} in {Heterogeneous} {Collaborative} {Embodied} {Agents}},
	doi = {10.1109/ICCV48922.2021.01565},
	abstract = {Communication between embodied AI agents has received increasing attention in recent years. Despite its use, it is still unclear whether the learned communication is interpretable and grounded in perception. To study the grounding of emergent forms of communication, we first introduce the collaborative multi-object navigation task ‘CoMON.' In this task, an ‘oracle agent' has detailed environment information in the form of a map. It communicates with a ‘navigator agent' that perceives the environment visually and is tasked to find a sequence of goals. To succeed at the task, effective communication is essential. CoMON hence serves as a basis to study different communication mechanisms between heterogeneous agents, that is, agents with different capabilities and roles. We study two common communication mechanisms and analyze their communication patterns through an egocentric and spatial lens. We show that the emergent communication can be grounded to the agent observations and the spatial structure of the 3D environment.},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Patel, Shivansh and Wani, Saim and Jain, Unnat and Schwing, Alexander and Lazebnik, Svetlana and Savva, Manolis and Chang, Angel X.},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {Collaboration, Computer vision, Explainable AI, Grounding, Navigation, Systematics, Task analysis, Three-dimensional displays, Vision for robotics and autonomous vehicles, Visual reasoning and logical representation},
	pages = {15993--15943},
	file = {IEEE Xplore Abstract Record:/home/brendon/academic/misc/Zotero/storage/B8FG88PR/9711485.html:text/html;IEEE Xplore Full Text PDF:/home/brendon/academic/misc/Zotero/storage/TCVQXR8N/Patel et al. - 2021 - Interpretation of Emergent Communication in Hetero.pdf:application/pdf},
}

@inproceedings{perkins_neural_2021,
	title = {Neural networks can understand compositional functions that humans do not, in the context of emergent communication},
	abstract = {It is shown that it is possible to craft transformations that, applied to compositional grammars, result in Grammars that neural networks can learn easily, but humans do not, which could explain the disconnect between current metrics of compositionality, that are arguably human-centric, and the ability of neural networks to generalize to unseen examples. We show that it is possible to craft transformations that, applied to compositional grammars, result in grammars that neural networks can learn easily, but humans do not. This could explain the disconnect between current metrics of compositionality, that are arguably human-centric, and the ability of neural networks to generalize to unseen examples. We propose to use the transformations as a benchmark, ICY, which could be used to measure aspects of the compositional inductive bias of networks, and to search for networks with similar compositional inductive biases to humans. As an example of this approach, we propose a hierarchical model, HU-RNN, which shows an inductive bias towards position-independent, word-like groups of tokens.},
	author = {Perkins, Hugh},
	year = {2021},
	keywords = {important},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/4VEKMF93/Perkins - 2021 - Neural networks can understand compositional funct.pdf:application/pdf},
}

@inproceedings{qiu_emergent_2021,
	title = {Emergent {Graphical} {Conventions} in a {Visual} {Communication} {Game}},
	abstract = {This work takes the very first step to model and simulate such an evolution process through which symbolic sign systems emerge in the trade-off between iconicity and symbolicity via two neural agents playing a visual communication game. Humans communicate with graphical sketches apart from symbolic languages [8]. While recent studies of emergent communication primarily focus on symbolic languages [24], their settings overlook the graphical sketches existing in human communication; they do not account for the evolution process through which symbolic sign systems emerge in the trade-off between iconicity and symbolicity. In this work, we take the very first step to model and simulate such an evolution process via two neural agents playing a visual communication game; the sender communicates with the receiver by sketching on a canvas. We devise a novel reinforcement learning method such that agents are evolved jointly towards successful communication and abstract graphical conventions. To inspect the emerged conventions, we carefully define three key properties – iconicity, symbolicity, and semanticity – and design evaluation methods accordingly. Our experimental results under different controls are consistent with the observation in studies of human graphical conventions [9, 17]. Of note, we find that evolved sketches can preserve the continuum of semantics [30] under proper environmental pressures. More interestingly, co-evolved agents can switch between conventionalized and iconic communication based on their familiarity with referents. We hope the present research can pave the path for studying emergent communication with the unexplored modality of sketches.},
	author = {Qiu, Shuwen and Xie, Sirui and Fan, Lifeng and Gao, Tao and Zhu, Song-Chun and Zhu, Yixin},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/QAKX4PFK/Qiu et al. - 2021 - Emergent Graphical Conventions in a Visual Communi.pdf:application/pdf},
}

@article{li_learning_2021-1,
	title = {Learning {Emergent} {Discrete} {Message} {Communication} for {Cooperative} {Reinforcement} {Learning}},
	abstract = {It is shown that discrete message communication has performance comparable to continuous message communication but with much a much smaller vocabulary size and an approach that allows humans to interactively send discrete messages to agents is proposed. Communication is a important factor that enables agents work cooperatively in multi-agent reinforcement learning (MARL). Most previous work uses continuous message communication whose high representational capacity comes at the expense of interpretability. Allowing agents to learn their own discrete message communication protocol emerged from a variety of domains can increase the interpretability for human designers and other agents.This paper proposes a method to generate discrete messages analogous to human languages, and achieve communication by a broadcast-andlisten mechanism based on self-attention. We show that discrete message communication has performance comparable to continuous message communication but with much a much smaller vocabulary size.Furthermore, we propose an approach that allows humans to interactively send discrete messages to agents.},
	journal = {ArXiv},
	author = {Li, Sheng and Zhou, Yutai and Allen, R. and Kochenderfer, Mykel J.},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/NVRNMAC4/Li et al. - 2021 - Learning Emergent Discrete Message Communication f.pdf:application/pdf},
}

@inproceedings{ueda_relationship_2021,
	title = {On the {Relationship} between {Zipf}'s {Law} of {Abbreviation} and {Interfering} {Noise} in {Emergent} {Languages}},
	doi = {10.18653/v1/2021.acl-srw.6},
	abstract = {Noise on a speaker is one of the factors for ZLA or at least causes emergent languages to approach ZLA, while noise on a listener and a channel is not, and it is assumed that it could be simulated by modeling the effect of noises on the agents’ environment. This paper studies whether emergent languages in a signaling game follow Zipf’s law of abbreviation (ZLA), especially when the communication ability of agents is limited because of interfering noises. ZLA is a wellknown tendency in human languages where the more frequently a word is used, the shorter it will be. Surprisingly, previous work demonstrated that emergent languages do not obey ZLA at all when neural agents play a signaling game. It also reported that a ZLA-like tendency appeared by adding an explicit penalty on word lengths, which can be considered some external factors in reality such as articulatory effort. We hypothesize, on the other hand, that there might be not only such external factors but also some internal factors related to cognitive abilities. We assume that it could be simulated by modeling the effect of noises on the agents’ environment. In our experimental setup, the hidden states of the LSTM-based speaker and listener were added with Gaussian noise, while the channel was subject to discrete random replacement. Our results suggest that noise on a speaker is one of the factors for ZLA or at least causes emergent languages to approach ZLA, while noise on a listener and a channel is not.},
	booktitle = {{ACL}},
	author = {Ueda, Ryojun and Washio, Koki},
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/9BFT7982/Ueda and Washio - 2021 - On the Relationship between Zipf's Law of Abbrevia.pdf:application/pdf},
}

@inproceedings{rita_role_2021,
	title = {On the role of population heterogeneity in emergent communication},
	url = {https://openreview.net/forum?id=5Qkd7-bZfI},
	abstract = {Populations have often been perceived as a structuring component for language to emerge and evolve: the larger the population, the more systematic the language. While this observation is widespread...},
	language = {en},
	urldate = {2022-03-28},
	author = {Rita, Mathieu and Strub, Florian and Grill, Jean-Bastien and Pietquin, Olivier and Dupoux, Emmanuel},
	month = sep,
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/JIQZ2NQD/Rita et al. - 2021 - On the role of population heterogeneity in emergen.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/GRNZ5SBQ/forum.html:text/html},
}

@book{skyrms_signals_2010,
	title = {Signals: {Evolution}, {Learning}, and {Information}},
	isbn = {978-0-19-958082-8},
	shorttitle = {Signals},
	abstract = {Brian Skyrms presents a fascinating exploration of how fundamental signals are to our world. He uses a variety of tools -- theories of signaling games, information, evolution, and learning -- to investigate how meaning and communication develop. He shows how signaling games themselves evolve, and introduces a new model of learning with invention. The juxtaposition of atomic signals leads to complex signals, as the natural product of gradual process. Signals operate in networks of senders and receivers at all levels of life. Information is transmitted, but it is also processed in various ways. That is how we think -- signals run around a very complicated signaling network. Signaling is a key ingredient in the evolution of teamwork, in the human but also in the animal world, even in micro-organisms. Communication and co-ordination of action are different aspects of the flow of information, and are both effected by signals.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Skyrms, Brian},
	month = apr,
	year = {2010},
	note = {Google-Books-ID: pGwVDAAAQBAJ},
	keywords = {Philosophy / Epistemology, Philosophy / Language, Philosophy / Mind \& Body, Science / Life Sciences / Evolution, Science / Philosophy \& Social Aspects},
}

@article{smith_logic_1973,
	title = {The {Logic} of {Animal} {Conflict}},
	volume = {246},
	copyright = {1973 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/246015a0},
	doi = {10.1038/246015a0},
	abstract = {Conflicts between animals of the same species usually are of “limited war” type, not causing serious injury. This is often explained as due to group or species selection for behaviour benefiting the species rather than individuals. Game theory and computer simulation analyses show, however, that a “limited war” strategy benefits individual animals as well as the species.},
	language = {en},
	number = {5427},
	urldate = {2022-03-28},
	journal = {Nature},
	author = {Smith, J. Maynard and Price, G. R.},
	month = nov,
	year = {1973},
	note = {Number: 5427
Publisher: Nature Publishing Group},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {15--18},
}

@article{rita_lazimpa_2020,
	title = {"{LazImpa}": {Lazy} and {Impatient} neural agents learn to communicate efficiently},
	shorttitle = {"{LazImpa}"},
	url = {http://arxiv.org/abs/2010.01878},
	abstract = {Previous work has shown that artificial neural agents naturally develop surprisingly non-efficient codes. This is illustrated by the fact that in a referential game involving a speaker and a listener neural networks optimizing accurate transmission over a discrete channel, the emergent messages fail to achieve an optimal length. Furthermore, frequent messages tend to be longer than infrequent ones, a pattern contrary to the Zipf Law of Abbreviation (ZLA) observed in all natural languages. Here, we show that near-optimal and ZLA-compatible messages can emerge, but only if both the speaker and the listener are modified. We hence introduce a new communication system, "LazImpa", where the speaker is made increasingly lazy, i.e. avoids long messages, and the listener impatient, i.e.,{\textasciitilde}seeks to guess the intended content as soon as possible.},
	urldate = {2022-03-28},
	journal = {arXiv:2010.01878 [cs]},
	author = {Rita, Mathieu and Chaabouni, Rahma and Dupoux, Emmanuel},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.01878},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multiagent Systems, I.2},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/EF7GNZ2K/Rita et al. - 2020 - LazImpa Lazy and Impatient neural agents learn .pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/2LCK2Q8K/2010.html:text/html},
}

@inproceedings{pimentel_how_2021,
	address = {Online},
	title = {How ({Non}-){Optimal} is the {Lexicon}?},
	url = {https://aclanthology.org/2021.naacl-main.350},
	doi = {10.18653/v1/2021.naacl-main.350},
	abstract = {The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf's law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world's languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon's optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes—as measured by code length.},
	urldate = {2022-04-04},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Pimentel, Tiago and Nikkarinen, Irene and Mahowald, Kyle and Cotterell, Ryan and Blasi, Damián},
	month = jun,
	year = {2021},
	pages = {4426--4438},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/IB2YZMB7/Pimentel et al. - 2021 - How (Non-)Optimal is the Lexicon.pdf:application/pdf},
}

@article{dankers_paradox_2022,
	title = {The paradox of the compositionality of natural language: a neural machine translation case study},
	shorttitle = {The paradox of the compositionality of natural language},
	url = {http://arxiv.org/abs/2108.05885},
	abstract = {Obtaining human-like performance in NLP is often argued to require compositional generalisation. Whether neural networks exhibit this ability is usually studied by training models on highly compositional synthetic data. However, compositionality in natural language is much more complex than the rigid, arithmetic-like version such data adheres to, and artificial compositionality tests thus do not allow us to determine how neural models deal with more realistic forms of compositionality. In this work, we re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT). Our results highlight that: i) unfavourably, models trained on more data are more compositional; ii) models are sometimes less compositional than expected, but sometimes more, exemplifying that different levels of compositionality are required, and models are not always able to modulate between them correctly; iii) some of the non-compositional behaviours are mistakes, whereas others reflect the natural variation in data. Apart from an empirical study, our work is a call to action: we should rethink the evaluation of compositionality in neural networks and develop benchmarks using real data to evaluate compositionality on natural language, where composing meaning is not as straightforward as doing the math.},
	urldate = {2022-04-04},
	journal = {arXiv:2108.05885 [cs]},
	author = {Dankers, Verna and Bruni, Elia and Hupkes, Dieuwke},
	month = mar,
	year = {2022},
	note = {arXiv: 2108.05885},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/RXV2IS58/Dankers et al. - 2022 - The paradox of the compositionality of natural lan.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/WNKJHHK8/2108.html:text/html},
}

@article{wang_surface_2018,
	title = {Surface {Statistics} of an {Unknown} {Language} {Indicate} {How} to {Parse} {It}},
	volume = {6},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00248},
	doi = {10.1162/tacl_a_00248},
	abstract = {We introduce a novel framework for delexicalized dependency parsing in a new language. We show that useful features of the target language can be extracted automatically from an unparsed corpus, which consists only of gold part-of-speech (POS) sequences. Providing these features to our neural parser enables it to parse sequences like those in the corpus. Strikingly, our system has no supervision in the target language. Rather, it is a multilingual system that is trained end-to-end on a variety of other languages, so it learns a feature extractor that works well. We show experimentally across multiple languages: (1) Features computed from the unparsed corpus improve parsing accuracy. (2) Including thousands of synthetic languages in the training yields further improvement. (3) Despite being computed from unparsed corpora, our learned task-specific features beat previous work’s interpretable typological features that require parsed corpora or expert categorization of the language. Our best method improved attachment scores on held-out test languages by an average of 5.6 percentage points over past work that does not inspect the unparsed data (McDonald et al., 2011), and by 20.7 points over past “grammar induction” work that does not use training languages (Naseem et al., 2010).},
	urldate = {2022-04-04},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Wang, Dingquan and Eisner, Jason},
	month = dec,
	year = {2018},
	pages = {667--685},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/TXVKTSXR/Wang and Eisner - 2018 - Surface Statistics of an Unknown Language Indicate.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/ZF8U8J8S/Surface-Statistics-of-an-Unknown-Language-Indicate.html:text/html},
}

@inproceedings{bailly_emergence_2020,
	address = {Online},
	title = {Emergence of {Syntax} {Needs} {Minimal} {Supervision}},
	url = {https://aclanthology.org/2020.acl-main.46},
	doi = {10.18653/v1/2020.acl-main.46},
	abstract = {This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance. Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information. We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.},
	urldate = {2022-04-04},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bailly, Raphaël and Gábor, Kata},
	month = jul,
	year = {2020},
	pages = {477--487},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/E64B6YDC/Bailly and Gábor - 2020 - Emergence of Syntax Needs Minimal Supervision.pdf:application/pdf},
}

@misc{kokel_human-guided_2021,
	title = {Human-guided {Collaborative} {Problem} {Solving}: {A} {Natural} {Language} based {Framework}},
	shorttitle = {Human-guided {Collaborative} {Problem} {Solving}},
	url = {https://www.semanticscholar.org/paper/Human-guided-Collaborative-Problem-Solving%3A-A-based-Kokel-Das/616e8d0ad6bbb37f90bbef949ad3143e99af1038},
	abstract = {The ability of this framework to address the key challenges of collaborative problem solving is illustrated by demonstrating it on a collaborative building task in a Minecraft-based blocksworld domain. We consider the problem of human-machine collaborative problem solving as a planning task coupled with natural language communication. Our framework consists of three components – a natural language engine that parses the language utterances to a formal representation and vice-versa, a concept learner that induces generalized concepts for plans based on limited interactions with the user and an HTN planner that solves the task based on human interaction. We illustrate the ability of this framework to address the key challenges of collaborative problem solving by demonstrating it on a collaborative building task in a Minecraft-based blocksworld domain. The accompanied demo video is available at https://youtu.be/q1pWe4aahF0.},
	language = {en},
	urldate = {2022-04-07},
	author = {Kokel, Harsha and Das, M. and Islam, Rakibul and Bonn, Julia and Cai, Jon and Dan, Soham and Narayan-Chen, Anjali and Jayannavar, Prashant and Doppa, Janardhan Rao and Hockenmaier, J. and Natarajan, Sriraam and Palmer, M. and Roth, D.},
	year = {2021},
	file = {Kokel et al. - 2021 - Human-guided Collaborative Problem Solving A Natu.pdf:/home/brendon/academic/misc/Zotero/storage/3F89PMZL/Kokel et al. - 2021 - Human-guided Collaborative Problem Solving A Natu.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/257FZY86/616e8d0ad6bbb37f90bbef949ad3143e99af1038.html:text/html},
}

@inproceedings{narayan-chen_collaborative_2019,
	address = {Florence, Italy},
	title = {Collaborative {Dialogue} in {Minecraft}},
	url = {https://aclanthology.org/P19-1537},
	doi = {10.18653/v1/P19-1537},
	abstract = {We wish to develop interactive agents that can communicate with humans to collaboratively solve tasks in grounded scenarios. Since computer games allow us to simulate such tasks without the need for physical robots, we define a Minecraft-based collaborative building task in which one player (A, the Architect) is shown a target structure and needs to instruct the other player (B, the Builder) to build this structure. Both players interact via a chat interface. A can observe B but cannot place blocks. We present the Minecraft Dialogue Corpus, a collection of 509 conversations and game logs. As a first step towards our goal of developing fully interactive agents for this task, we consider the subtask of Architect utterance generation, and show how challenging it is.},
	urldate = {2022-04-07},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Narayan-Chen, Anjali and Jayannavar, Prashant and Hockenmaier, Julia},
	month = jul,
	year = {2019},
	pages = {5405--5415},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/DADEHUB8/Narayan-Chen et al. - 2019 - Collaborative Dialogue in Minecraft.pdf:application/pdf},
}

@article{yao_linking_2022,
	title = {Linking {Emergent} and {Natural} {Languages} via {Corpus} {Transfer}},
	doi = {10.48550/arXiv.2203.13344},
	abstract = {This work proposes a novel way to establish a link by corpus transfer, i.e. pretraining on a corpus of emergent language for downstream natural language tasks, which is in contrast to prior work that directly transfers speaker and listener parameters. The study of language emergence aims to understand how human languages are shaped by perceptual grounding and communicative intent. Computational approaches to emergent communication (EC) predominantly consider referential games in limited domains and analyze the learned protocol within the game framework. As a result, it remains unclear how the emergent languages1 from these settings connect to natural languages or provide benefits in real-world language processing tasks, where statistical models trained on large text corpora dominate. In this work, we propose a novel way to establish such a link by corpus transfer, i.e. pretraining on a corpus of emergent language for downstream natural language tasks, which is in contrast to prior work that directly transfers speaker and listener parameters. Our approach showcases non-trivial transfer benefits for two different tasks – language modeling and image captioning. For example, in a low-resource setup (modeling 2 million natural language tokens), pre-training on an emergent language corpus with just 2 million tokens reduces model perplexity by 24.6\% on average across ten natural languages. We also introduce a novel metric to predict the transferability of an emergent language by translating emergent messages to natural language captions grounded on the same images. We find that our translation-based metric highly correlates with the downstream performance on modeling natural languages (for instance ρ = 0.83 on Hebrew), while topographic similarity, a popular metric in previous work, shows surprisingly low correlation (ρ = 0.003), hinting that simple properties like attribute disentanglement from synthetic domains might not capture the full complexities of natural language. Our findings also indicate potential benefits of moving language emergence forward with natural language resources and models2.},
	journal = {ArXiv},
	author = {Yao, Shunyu and Yu, Mo and Zhang, Yang and Narasimhan, Karthik and Tenenbaum, J. and Gan, Chuang},
	year = {2022},
	keywords = {important},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/LIKCPF2I/Yao et al. - 2022 - Linking Emergent and Natural Languages via Corpus .pdf:application/pdf},
}

@inproceedings{andreas_translating_2017,
	address = {Vancouver, Canada},
	title = {Translating {Neuralese}},
	url = {https://aclanthology.org/P17-1022},
	doi = {10.18653/v1/P17-1022},
	abstract = {Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.},
	urldate = {2022-04-12},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Andreas, Jacob and Dragan, Anca and Klein, Dan},
	month = jul,
	year = {2017},
	pages = {232--242},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/LS3QEB6G/Andreas et al. - 2017 - Translating Neuralese.pdf:application/pdf},
}

@inproceedings{papadimitriou_learning_2020,
	address = {Online},
	title = {Learning {Music} {Helps} {You} {Read}: {Using} {Transfer} to {Study} {Linguistic} {Structure} in {Language} {Models}},
	shorttitle = {Learning {Music} {Helps} {You} {Read}},
	url = {https://aclanthology.org/2020.emnlp-main.554},
	doi = {10.18653/v1/2020.emnlp-main.554},
	abstract = {We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models. We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language. We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary. To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion. Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language. Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties. Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.},
	urldate = {2022-04-12},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Papadimitriou, Isabel and Jurafsky, Dan},
	month = nov,
	year = {2020},
	pages = {6829--6839},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/AXIPXIJ7/Papadimitriou and Jurafsky - 2020 - Learning Music Helps You Read Using Transfer to S.pdf:application/pdf},
}

@incollection{dhrymes_criteria_1972,
	title = {Criteria for {Evaluation} of {Econometric} {Models}},
	url = {https://www.nber.org/books-and-chapters/annals-economic-and-social-measurement-volume-1-number-3/criteria-evaluation-econometric-models},
	urldate = {2022-04-22},
	booktitle = {Annals of {Economic} and {Social} {Measurement}, {Volume} 1, number 3},
	publisher = {NBER},
	author = {Dhrymes, Phoebus J. and Howrey, E. Philip and Hymans, Saul H. and Kmenta, Jan and Leamer, Edward E. and Quandt, Richard E. and Ramsey, James B. and Shapiro, Harold T. and Zarnowitz, Victor},
	month = jul,
	year = {1972},
	pages = {291--324},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/MU6IXFDW/Dhrymes et al. - 1972 - Criteria for Evaluation of Econometric Models.pdf:application/pdf},
}

@inproceedings{van_der_wal_grammar_2020,
	address = {Online},
	title = {The {Grammar} of {Emergent} {Languages}},
	url = {https://aclanthology.org/2020.emnlp-main.270},
	doi = {10.18653/v1/2020.emnlp-main.270},
	abstract = {In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language. We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use. Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language. We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.},
	urldate = {2022-04-29},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {van der Wal, Oskar and de Boer, Silvan and Bruni, Elia and Hupkes, Dieuwke},
	month = nov,
	year = {2020},
	keywords = {ur},
	pages = {3339--3359},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/HHETCWUE/van der Wal et al. - 2020 - The Grammar of Emergent Languages.pdf:application/pdf},
}

@inproceedings{lewis_convention_1970,
	title = {Convention: {A} {Philosophical} {Study}},
	shorttitle = {Convention},
	doi = {10.2307/2218418},
	abstract = {Acknowledgements. Foreword by W.V. Quine. Introduction. I. Coordination and Convention. Sample Coordination Problems. Analysis of Coordination Problems. Solving Coordination Problems. Convention. Sample Conventions. II. Convention Refined. Common Knowledge. Knowledge of Conventions. Alternatives to Convention. Degrees of Convention. Consequences of Conventions. III. Convention Contrasted. Agreement. Social Contracts. Norms. Rules. Conformative Behavior. Imitation. Meaning of Signals. IV. Convention and Communication. Sample Signals. Analysis of Signaling. Verbal Signaling. Conventional Meaning of Signals. V. Conventions of Language. Possible Languages. Grammars. Semantics in a Possible Language. Conventions of Truthfulness. Semantics in a Population. Conclusion. Index.},
	author = {Lewis, David},
	year = {1970},
}

@techreport{slowik_structural_2020,
	title = {Structural {Inductive} {Biases} in {Emergent} {Communication}},
	url = {http://arxiv.org/abs/2002.01335},
	abstract = {In order to communicate, humans flatten a complex representation of ideas and their attributes into a single word or a sentence. We investigate the impact of representation learning in artificial agents by developing graph referential games. We empirically show that agents parametrized by graph neural networks develop a more compositional language compared to bag-of-words and sequence models, which allows them to systematically generalize to new combinations of familiar features.},
	number = {arXiv:2002.01335},
	urldate = {2022-05-17},
	institution = {arXiv},
	author = {Słowik, Agnieszka and Gupta, Abhinav and Hamilton, William L. and Jamnik, Mateja and Holden, Sean B. and Pal, Christopher},
	month = feb,
	year = {2020},
	doi = {10.48550/arXiv.2002.01335},
	note = {arXiv:2002.01335 [cs, stat]
version: 1
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/home/brendon/academic/misc/Zotero/storage/9IFGATMY/Słowik et al. - 2020 - Structural Inductive Biases in Emergent Communicat.pdf:application/pdf;arXiv.org Snapshot:/home/brendon/academic/misc/Zotero/storage/RCKL65IT/2002.html:text/html},
}

@inproceedings{guo_expressivity_2021-1,
	title = {Expressivity of {Emergent} {Languages} is a {Trade}-off between {Contextual} {Complexity} and {Unpredictability}},
	url = {https://openreview.net/forum?id=WxuE_JWxjkW},
	abstract = {Researchers are using deep learning models to explore the emergence of language in various language games, where agents interact and develop an emergent language to solve tasks. We focus on the...},
	language = {en},
	urldate = {2022-05-17},
	author = {Guo, Shangmin and Ren, Yi and Kirby, Simon and Smith, Kenny and Mathewson, Kory Wallace and Albrecht, Stefano V.},
	month = sep,
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/7IGZYD4J/Guo et al. - 2021 - Expressivity of Emergent Languages is a Trade-off .pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/ILMDPIRN/forum.html:text/html},
}

@article{kirby_compression_2015,
	title = {Compression and communication in the cultural evolution of linguistic structure},
	volume = {141},
	issn = {0010-0277},
	url = {https://www.sciencedirect.com/science/article/pii/S0010027715000815},
	doi = {https://doi.org/10.1016/j.cognition.2015.03.016},
	abstract = {Language exhibits striking systematic structure. Words are composed of combinations of reusable sounds, and those words in turn are combined to form complex sentences. These properties make language unique among natural communication systems and enable our species to convey an open-ended set of messages. We provide a cultural evolutionary account of the origins of this structure. We show, using simulations of rational learners and laboratory experiments, that structure arises from a trade-off between pressures for compressibility (imposed during learning) and expressivity (imposed during communication). We further demonstrate that the relative strength of these two pressures can be varied in different social contexts, leading to novel predictions about the emergence of structured behaviour in the wild.},
	journal = {Cognition},
	author = {Kirby, Simon and Tamariz, Monica and Cornish, Hannah and Smith, Kenny},
	year = {2015},
	keywords = {Language evolution, Cultural transmission, Iterated learning},
	pages = {87--102},
}

@inproceedings{rita_role_2021-1,
	title = {On the role of population heterogeneity in emergent communication},
	url = {https://openreview.net/forum?id=5Qkd7-bZfI},
	abstract = {Populations have often been perceived as a structuring component for language to emerge and evolve: the larger the population, the more systematic the language. While this observation is widespread...},
	language = {en},
	urldate = {2022-05-18},
	author = {Rita, Mathieu and Strub, Florian and Grill, Jean-Bastien and Pietquin, Olivier and Dupoux, Emmanuel},
	month = sep,
	year = {2021},
	file = {Full Text PDF:/home/brendon/academic/misc/Zotero/storage/R7VBWNQ2/Rita et al. - 2021 - On the role of population heterogeneity in emergen.pdf:application/pdf;Snapshot:/home/brendon/academic/misc/Zotero/storage/UYS28FCA/forum.html:text/html},
}
